{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "137b3b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://arxiv.org/abs/1909.02027\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "import tensorflow_hub as hub\n",
    "from tqdm import tqdm\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1bb2f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"../data/clinic/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d5b6efb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/clinic/data_imbalanced/oos_train_with_use_emb.csv',\n",
       " '../data/clinic/data_imbalanced/test_with_use_emb.csv',\n",
       " '../data/clinic/data_imbalanced/val_with_use_emb.csv',\n",
       " '../data/clinic/data_imbalanced/val.csv',\n",
       " '../data/clinic/data_imbalanced/oos_test.csv',\n",
       " '../data/clinic/data_imbalanced/test.csv',\n",
       " '../data/clinic/data_imbalanced/oos_train.csv',\n",
       " '../data/clinic/data_imbalanced/train_with_use_emb.csv',\n",
       " '../data/clinic/data_imbalanced/oos_test_with_use_emb.csv',\n",
       " '../data/clinic/data_imbalanced/oos_val.csv',\n",
       " '../data/clinic/data_imbalanced/train.csv',\n",
       " '../data/clinic/data_imbalanced/oos_val_with_use_emb.csv',\n",
       " '../data/clinic/data_small/oos_train_with_use_emb.csv',\n",
       " '../data/clinic/data_small/test_with_use_emb.csv',\n",
       " '../data/clinic/data_small/val_with_use_emb.csv',\n",
       " '../data/clinic/data_small/val.csv',\n",
       " '../data/clinic/data_small/oos_test.csv',\n",
       " '../data/clinic/data_small/test.csv',\n",
       " '../data/clinic/data_small/oos_train.csv',\n",
       " '../data/clinic/data_small/train_with_use_emb.csv',\n",
       " '../data/clinic/data_small/oos_test_with_use_emb.csv',\n",
       " '../data/clinic/data_small/oos_val.csv',\n",
       " '../data/clinic/data_small/train.csv',\n",
       " '../data/clinic/data_small/oos_val_with_use_emb.csv',\n",
       " '../data/clinic/data_full/oos_train_with_use_emb.csv',\n",
       " '../data/clinic/data_full/test_with_use_emb.csv',\n",
       " '../data/clinic/data_full/train_with_use_emb.csv.zip',\n",
       " '../data/clinic/data_full/val_with_use_emb.csv',\n",
       " '../data/clinic/data_full/val.csv',\n",
       " '../data/clinic/data_full/oos_test.csv',\n",
       " '../data/clinic/data_full/test.csv',\n",
       " '../data/clinic/data_full/oos_train.csv',\n",
       " '../data/clinic/data_full/train_with_use_emb_no_text.csv',\n",
       " '../data/clinic/data_full/train_with_use_emb.csv',\n",
       " '../data/clinic/data_full/train_with_use_emb_no_text_no_str.csv',\n",
       " '../data/clinic/data_full/oos_test_with_use_emb.csv',\n",
       " '../data/clinic/data_full/oos_val.csv',\n",
       " '../data/clinic/data_full/train.csv',\n",
       " '../data/clinic/data_full/oos_val_with_use_emb.csv',\n",
       " '../data/clinic/data_oos_plus/oos_train_with_use_emb.csv',\n",
       " '../data/clinic/data_oos_plus/test_with_use_emb.csv',\n",
       " '../data/clinic/data_oos_plus/val_with_use_emb.csv',\n",
       " '../data/clinic/data_oos_plus/val.csv',\n",
       " '../data/clinic/data_oos_plus/oos_test.csv',\n",
       " '../data/clinic/data_oos_plus/test.csv',\n",
       " '../data/clinic/data_oos_plus/oos_train.csv',\n",
       " '../data/clinic/data_oos_plus/train_with_use_emb.csv',\n",
       " '../data/clinic/data_oos_plus/oos_test_with_use_emb.csv',\n",
       " '../data/clinic/data_oos_plus/oos_val.csv',\n",
       " '../data/clinic/data_oos_plus/train.csv',\n",
       " '../data/clinic/data_oos_plus/oos_val_with_use_emb.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = glob(DATA_FOLDER+ '*/*.csv*')\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "527e4c0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['change_accent', 'who_do_you_work_for', 'bill_balance',\n",
       "       'next_song', 'calories', 'change_user_name', 'confirm_reservation',\n",
       "       'jump_start', 'card_declined', 'cook_time', 'nutrition_info',\n",
       "       'greeting', 'calendar', 'schedule_maintenance', 'balance',\n",
       "       'tire_pressure', 'shopping_list', 'ingredients_list',\n",
       "       'whisper_mode', 'meal_suggestion', 'travel_alert', 'lost_luggage',\n",
       "       'weather', 'pin_change', 'pto_request', 'change_speed', 'no',\n",
       "       'user_name', 'taxes', 'book_flight', 'yes', 'timezone', 'fun_fact',\n",
       "       'order', 'traffic', 'pay_bill', 'report_fraud', 'vaccines',\n",
       "       'recipe', 'report_lost_card', 'transfer', 'redeem_rewards',\n",
       "       'exchange_rate', 'expiration_date', 'order_status',\n",
       "       'reset_settings', 'cancel_reservation', 'goodbye',\n",
       "       'restaurant_reviews', 'tell_joke', 'current_location', 'pto_used',\n",
       "       'international_visa', 'restaurant_suggestion', 'pto_balance',\n",
       "       'payday', 'flight_status', 'distance', 'routing', 'translate',\n",
       "       'text', 'carry_on', 'interest_rate', 'min_payment', 'roll_dice',\n",
       "       'measurement_conversion', 'book_hotel', 'travel_suggestion',\n",
       "       'cancel', 'credit_limit_change', 'apr', 'time', 'direct_deposit',\n",
       "       'repeat', 'how_busy', 'rollover_401k', 'travel_notification',\n",
       "       'calendar_update', 'international_fees', 'account_blocked',\n",
       "       'improve_credit_score', 'uber', 'tire_change', 'gas_type',\n",
       "       'do_you_have_pets', 'application_status',\n",
       "       'replacement_card_duration', 'play_music', 'where_are_you_from',\n",
       "       'credit_limit', 'date', 'share_location', 'who_made_you',\n",
       "       'spelling', 'maybe', 'accept_reservations', 'spending_history',\n",
       "       'meaning_of_life', 'gas', 'todo_list_update', 'plug_type',\n",
       "       'update_playlist', 'ingredient_substitution', 'reminder_update',\n",
       "       'what_is_your_name', 'todo_list', 'income', 'transactions',\n",
       "       'shopping_list_update', 'what_are_your_hobbies', 'make_call',\n",
       "       'definition', 'change_ai_name', 'change_language',\n",
       "       'oil_change_how', 'what_song', 'freeze_account', 'thank_you',\n",
       "       'mpg', 'rewards_balance', 'find_phone', 'flip_coin', 'car_rental',\n",
       "       'food_last', 'insurance_change', 'credit_score',\n",
       "       'pto_request_status', 'reminder', 'what_can_i_ask_you',\n",
       "       'next_holiday', 'order_checks', 'how_old_are_you', 'calculator',\n",
       "       'directions', 'damaged_card', 'new_card', 'are_you_a_bot',\n",
       "       'insurance', 'bill_due', 'smart_home', 'timer', 'sync_device',\n",
       "       'w2', 'schedule_meeting', 'oil_change_when', 'alarm',\n",
       "       'change_volume', 'restaurant_reservation', 'meeting_schedule',\n",
       "       'last_maintenance'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('../data/clinic/data_full/train.csv')\n",
    "df_train['intent'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7978a708",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {k:i for i,k in enumerate(set(df_train['intent'].tolist()))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c3bf520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'meaning_of_life': 0,\n",
       " 'who_do_you_work_for': 1,\n",
       " 'pto_request': 2,\n",
       " 'restaurant_reservation': 3,\n",
       " 'gas_type': 4,\n",
       " 'update_playlist': 5,\n",
       " 'restaurant_suggestion': 6,\n",
       " 'book_flight': 7,\n",
       " 'date': 8,\n",
       " 'mpg': 9,\n",
       " 'play_music': 10,\n",
       " 'new_card': 11,\n",
       " 'timezone': 12,\n",
       " 'jump_start': 13,\n",
       " 'schedule_meeting': 14,\n",
       " 'flight_status': 15,\n",
       " 'weather': 16,\n",
       " 'transfer': 17,\n",
       " 'recipe': 18,\n",
       " 'reset_settings': 19,\n",
       " 'schedule_maintenance': 20,\n",
       " 'international_fees': 21,\n",
       " 'income': 22,\n",
       " 'cook_time': 23,\n",
       " 'redeem_rewards': 24,\n",
       " 'calories': 25,\n",
       " 'shopping_list': 26,\n",
       " 'change_language': 27,\n",
       " 'alarm': 28,\n",
       " 'flip_coin': 29,\n",
       " 'no': 30,\n",
       " 'whisper_mode': 31,\n",
       " 'insurance_change': 32,\n",
       " 'payday': 33,\n",
       " 'what_are_your_hobbies': 34,\n",
       " 'damaged_card': 35,\n",
       " 'definition': 36,\n",
       " 'insurance': 37,\n",
       " 'share_location': 38,\n",
       " 'next_song': 39,\n",
       " 'report_fraud': 40,\n",
       " 'vaccines': 41,\n",
       " 'next_holiday': 42,\n",
       " 'carry_on': 43,\n",
       " 'cancel_reservation': 44,\n",
       " 'what_can_i_ask_you': 45,\n",
       " 'how_old_are_you': 46,\n",
       " 'what_song': 47,\n",
       " 'where_are_you_from': 48,\n",
       " 'directions': 49,\n",
       " 'user_name': 50,\n",
       " 'do_you_have_pets': 51,\n",
       " 'find_phone': 52,\n",
       " 'car_rental': 53,\n",
       " 'roll_dice': 54,\n",
       " 'order_status': 55,\n",
       " 'who_made_you': 56,\n",
       " 'greeting': 57,\n",
       " 'calculator': 58,\n",
       " 'distance': 59,\n",
       " 'pay_bill': 60,\n",
       " 'make_call': 61,\n",
       " 'report_lost_card': 62,\n",
       " 'improve_credit_score': 63,\n",
       " 'sync_device': 64,\n",
       " 'calendar': 65,\n",
       " 'expiration_date': 66,\n",
       " 'change_volume': 67,\n",
       " 'order_checks': 68,\n",
       " 'spending_history': 69,\n",
       " 'change_ai_name': 70,\n",
       " 'lost_luggage': 71,\n",
       " 'replacement_card_duration': 72,\n",
       " 'credit_limit': 73,\n",
       " 'min_payment': 74,\n",
       " 'cancel': 75,\n",
       " 'calendar_update': 76,\n",
       " 'tire_change': 77,\n",
       " 'goodbye': 78,\n",
       " 'application_status': 79,\n",
       " 'repeat': 80,\n",
       " 'text': 81,\n",
       " 'direct_deposit': 82,\n",
       " 'meal_suggestion': 83,\n",
       " 'measurement_conversion': 84,\n",
       " 'what_is_your_name': 85,\n",
       " 'restaurant_reviews': 86,\n",
       " 'taxes': 87,\n",
       " 'interest_rate': 88,\n",
       " 'travel_suggestion': 89,\n",
       " 'plug_type': 90,\n",
       " 'yes': 91,\n",
       " 'book_hotel': 92,\n",
       " 'smart_home': 93,\n",
       " 'ingredient_substitution': 94,\n",
       " 'pto_balance': 95,\n",
       " 'gas': 96,\n",
       " 'todo_list_update': 97,\n",
       " 'last_maintenance': 98,\n",
       " 'apr': 99,\n",
       " 'shopping_list_update': 100,\n",
       " 'oil_change_when': 101,\n",
       " 'thank_you': 102,\n",
       " 'w2': 103,\n",
       " 'balance': 104,\n",
       " 'credit_limit_change': 105,\n",
       " 'current_location': 106,\n",
       " 'uber': 107,\n",
       " 'travel_notification': 108,\n",
       " 'confirm_reservation': 109,\n",
       " 'card_declined': 110,\n",
       " 'bill_due': 111,\n",
       " 'traffic': 112,\n",
       " 'maybe': 113,\n",
       " 'bill_balance': 114,\n",
       " 'change_accent': 115,\n",
       " 'food_last': 116,\n",
       " 'account_blocked': 117,\n",
       " 'timer': 118,\n",
       " 'reminder_update': 119,\n",
       " 'reminder': 120,\n",
       " 'rollover_401k': 121,\n",
       " 'pin_change': 122,\n",
       " 'todo_list': 123,\n",
       " 'fun_fact': 124,\n",
       " 'meeting_schedule': 125,\n",
       " 'rewards_balance': 126,\n",
       " 'time': 127,\n",
       " 'travel_alert': 128,\n",
       " 'pto_request_status': 129,\n",
       " 'change_speed': 130,\n",
       " 'accept_reservations': 131,\n",
       " 'tell_joke': 132,\n",
       " 'translate': 133,\n",
       " 'tire_pressure': 134,\n",
       " 'oil_change_how': 135,\n",
       " 'how_busy': 136,\n",
       " 'spelling': 137,\n",
       " 'transactions': 138,\n",
       " 'routing': 139,\n",
       " 'order': 140,\n",
       " 'change_user_name': 141,\n",
       " 'freeze_account': 142,\n",
       " 'credit_score': 143,\n",
       " 'are_you_a_bot': 144,\n",
       " 'exchange_rate': 145,\n",
       " 'pto_used': 146,\n",
       " 'international_visa': 147,\n",
       " 'nutrition_info': 148,\n",
       " 'ingredients_list': 149}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a927a6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels['oos']=150"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a62284f",
   "metadata": {},
   "source": [
    "## ADD USE EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ba02016",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-large/5\")\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2521835",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_guse_embedings_with_batch(sentences,batch_size=64):\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0,len(sentences),batch_size)):\n",
    "      embeddings_batch = embed(sentences[i:i+batch_size])\n",
    "      embeddings.extend(embeddings_batch)\n",
    "    return embeddings\n",
    "\n",
    "def add_embeddings(dt, column= 'text'):\n",
    "    embeddings = get_guse_embedings_with_batch(dt[column])\n",
    "    embs = np.array(embeddings).tolist()\n",
    "    df = pd.DataFrame([pd.Series(x) for x in embs])\n",
    "    df.columns = ['emb_{}'.format(x+1) for x in df.columns]\n",
    "    dt = pd.concat([dt,df], axis=1).reindex(dt.index)\n",
    "    return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d29c34dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-76c8748ddfca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'intent'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dt' is not defined"
     ]
    }
   ],
   "source": [
    "dt['label'] = dt['intent'].apply(lambda x: labels.get(x,150))\n",
    "dt = add_embeddings(dt)\n",
    "dt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8c927394",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:07<00:00,  6.41it/s]\n",
      "100%|██████████| 16/16 [00:02<00:00,  6.06it/s]\n",
      "100%|██████████| 71/71 [00:12<00:00,  5.85it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  8.81it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  8.63it/s]\n",
      "100%|██████████| 165/165 [00:24<00:00,  6.79it/s]\n",
      "100%|██████████| 47/47 [00:07<00:00,  6.57it/s]\n",
      "100%|██████████| 16/16 [00:02<00:00,  6.07it/s]\n",
      "100%|██████████| 71/71 [00:11<00:00,  6.07it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  8.89it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.64it/s]\n",
      "100%|██████████| 118/118 [00:18<00:00,  6.54it/s]\n",
      "100%|██████████| 47/47 [00:06<00:00,  7.01it/s]\n",
      "100%|██████████| 16/16 [00:02<00:00,  6.69it/s]\n",
      "100%|██████████| 71/71 [00:10<00:00,  6.69it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  6.17it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  8.28it/s]\n",
      "100%|██████████| 235/235 [00:32<00:00,  7.24it/s]\n",
      "100%|██████████| 47/47 [00:06<00:00,  7.00it/s]\n",
      "100%|██████████| 16/16 [00:02<00:00,  6.86it/s]\n",
      "100%|██████████| 71/71 [00:10<00:00,  6.95it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  7.55it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  8.85it/s]\n",
      "100%|██████████| 235/235 [00:34<00:00,  6.80it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2703.8341739177704"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time()\n",
    "for file_name in files:\n",
    "    dt = pd.read_csv(file_name)\n",
    "    dt['label'] = dt['intent'].apply(lambda x: labels.get(x,150))\n",
    "    add_embeddings(dt).to_csv(file_name.replace('.csv','_with_use_emb.csv'))\n",
    "time()-start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6125aef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:00<00:00, 51.96it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 66.22it/s]\n",
      "100%|██████████| 71/71 [00:00<00:00, 86.84it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 14.21it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 38.25it/s]\n",
      "100%|██████████| 165/165 [00:02<00:00, 65.82it/s]\n",
      "100%|██████████| 47/47 [00:00<00:00, 64.50it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 65.59it/s]\n",
      "100%|██████████| 71/71 [00:00<00:00, 71.11it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 68.00it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 119.85it/s]\n",
      "100%|██████████| 118/118 [00:01<00:00, 82.21it/s]\n",
      "100%|██████████| 47/47 [00:00<00:00, 86.25it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 75.70it/s]\n",
      "100%|██████████| 71/71 [00:00<00:00, 92.11it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 88.70it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 101.01it/s]\n",
      "100%|██████████| 235/235 [00:02<00:00, 87.07it/s]\n",
      "100%|██████████| 47/47 [00:00<00:00, 93.29it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 56.27it/s]\n",
      "100%|██████████| 71/71 [00:00<00:00, 72.83it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 40.71it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 102.50it/s]\n",
      "100%|██████████| 235/235 [00:02<00:00, 83.33it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2839.24116396904"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time()\n",
    "for file_name in files:\n",
    "    if file_name.find('with_use_emb')>-1:\n",
    "        continue\n",
    "    dt = pd.read_csv(file_name)\n",
    "    dt['label'] = dt['intent'].apply(lambda x: labels.get(x,150))\n",
    "    add_embeddings(dt).to_csv(file_name.replace('.csv','_with_use_emb_not_large.csv'))\n",
    "time()-start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b49e7a05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/clinic/data_imbalanced/oos_test_with_use_emb_not_large.csv',\n",
       " '../data/clinic/data_imbalanced/train_with_use_emb_not_large.csv',\n",
       " '../data/clinic/data_imbalanced/oos_val_with_use_emb_not_large.csv',\n",
       " '../data/clinic/data_imbalanced/val_with_use_emb_not_large.csv',\n",
       " '../data/clinic/data_imbalanced/test_with_use_emb_not_large.csv',\n",
       " '../data/clinic/data_imbalanced/oos_train_with_use_emb_not_large.csv',\n",
       " '../data/clinic/data_small/oos_test_with_use_emb_not_large.csv',\n",
       " '../data/clinic/data_small/train_with_use_emb_not_large.csv',\n",
       " '../data/clinic/data_small/oos_val_with_use_emb_not_large.csv',\n",
       " '../data/clinic/data_small/val_with_use_emb_not_large.csv',\n",
       " '../data/clinic/data_small/test_with_use_emb_not_large.csv',\n",
       " '../data/clinic/data_small/oos_train_with_use_emb_not_large.csv',\n",
       " '../data/clinic/data_full/oos_test_with_use_emb_not_large.csv',\n",
       " '../data/clinic/data_full/train_with_use_emb_not_large.csv',\n",
       " '../data/clinic/data_full/oos_val_with_use_emb_not_large.csv',\n",
       " '../data/clinic/data_full/val_with_use_emb_not_large.csv',\n",
       " '../data/clinic/data_full/test_with_use_emb_not_large.csv',\n",
       " '../data/clinic/data_full/oos_train_with_use_emb_not_large.csv',\n",
       " '../data/clinic/data_oos_plus/oos_test_with_use_emb_not_large.csv',\n",
       " '../data/clinic/data_oos_plus/train_with_use_emb_not_large.csv',\n",
       " '../data/clinic/data_oos_plus/oos_val_with_use_emb_not_large.csv',\n",
       " '../data/clinic/data_oos_plus/val_with_use_emb_not_large.csv',\n",
       " '../data/clinic/data_oos_plus/test_with_use_emb_not_large.csv',\n",
       " '../data/clinic/data_oos_plus/oos_train_with_use_emb_not_large.csv']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = glob(DATA_FOLDER+ '*/*_with_use_emb_not_large.csv')\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3b837d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "try:\n",
    "    import zlib\n",
    "    compression = zipfile.ZIP_DEFLATED\n",
    "except:\n",
    "    compression = zipfile.ZIP_STORED\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "32a56a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "\n",
    "for file_name in files:\n",
    "    with ZipFile(file_name.replace('.csv','.zip'),'w') as zip:\n",
    "           zip.write(file_name, compress_type=compression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ec8616",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ee5e190",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dropout, Dense\n",
    "from tensorflow import keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916f08d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_full = MLPClassifier(hidden_layer_sizes=(400), \n",
    "                         max_iter=300,\n",
    "                         activation = 'relu',\n",
    "                         solver='adam',\n",
    "                         random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a26a4f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_MLP_Model(num_labels=2, \n",
    "                         dense_dropout=0.5, \n",
    "                         input_size = 512, \n",
    "                         hidden_size = 512,\n",
    "                        hidden_activation = 'tanh',\n",
    "                         output_activation = 'softmax',\n",
    "                         num_layers = 2):\n",
    "    features = Input(shape=(input_size,), name=\"first\")\n",
    "    hidden = Dropout(dense_dropout)(features)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        name = 'dense{}'.format(i) if i != num_layers - 1 else 'hidden'\n",
    "        if dense_dropout>0:\n",
    "            hidden = Dense(units=hidden_size, activation=\"relu\", name=name)(hidden)\n",
    "            hidden = Dropout(dense_dropout)(hidden)\n",
    "        else:\n",
    "            hidden = Dense(units=hidden_size, activation=\"relu\", name=name)(features)\n",
    "\n",
    "    logits = hidden\n",
    "    outputs = Dense(units=num_labels, activation=output_activation, name=\"output_1\")(logits)\n",
    "    model = keras.Model(inputs=features, outputs=outputs)\n",
    "    model.compile(optimizer='adam',  \n",
    "                  loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                  #optimizer='sgd',  loss='mse',\n",
    "                  #loss='binary_crossentropy',\n",
    "                  #metrics=['accuracy'],\n",
    "                  metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")],\n",
    "                  #metrics=[keras.metrics.PrecisionAtRecall(recall=0.8)],\n",
    "                  run_eagerly = False)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f03c070c",
   "metadata": {},
   "source": [
    "# Parameters:\n",
    "#https://github.com/clinc/oos-eval/blob/master/hyperparameters.csv\n",
    "\n",
    "Full\tclassifier\tf_hidden_activation\ts_hidden_activation\thidden_dim\tvectorizer\tbatch_size\tdropout\t\t\t\n",
    "           mlp\t       tanh\t                  softmax\t       400\t       use\t        1\t      0\t\n",
    "           \n",
    "Small\tclassifier\tf_hidden_activation\ts_hidden_activation\thidden_dim\tvectorizer\tbatch_size\tdropout\t\t\t\n",
    "           mlp\t       tanh\t                  softmax\t       200\t       use\t        1\t      0.1\n",
    "           \n",
    "Imbalanced\tclassifier\tf_hidden_activation\ts_hidden_activation\thidden_dim\tvectorizer\tbatch_size\tdropout\t\t\t\n",
    "           mlp\t       tanh\t                  softmax\t       200\t       use\t        64      \t0\n",
    "           \n",
    "OOS+\tclassifier\tf_hidden_activation\ts_hidden_activation\thidden_dim\tvectorizer\tbatch_size\tdropout\t\t\t\n",
    "           mlp\t       tanh\t                  softmax\t       200\t       use\t        16      \t0.1\t\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61663457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "first (InputLayer)           [(None, 512)]             0         \n",
      "_________________________________________________________________\n",
      "hidden (Dense)               (None, 400)               205200    \n",
      "_________________________________________________________________\n",
      "output_1 (Dense)             (None, 151)               60551     \n",
      "=================================================================\n",
      "Total params: 265,751\n",
      "Trainable params: 265,751\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mlp_full = generate_MLP_Model(num_labels= 151, \n",
    "                     num_layers=1, \n",
    "                     hidden_size = 400, \n",
    "                     hidden_activation = 'tanh',\n",
    "                     output_activation = 'softmax',\n",
    "                     dense_dropout=0)\n",
    "mlp_full.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c3a1783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "first (InputLayer)           [(None, 512)]             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "hidden (Dense)               (None, 200)               102600    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "output_1 (Dense)             (None, 151)               30351     \n",
      "=================================================================\n",
      "Total params: 132,951\n",
      "Trainable params: 132,951\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mlp_small = generate_MLP_Model(num_labels= 151, \n",
    "                     num_layers=1, \n",
    "                     hidden_size = 200, \n",
    "                     hidden_activation = 'tanh',\n",
    "                     output_activation = 'softmax',\n",
    "                     dense_dropout=0.1)\n",
    "mlp_small.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "992c77c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "first (InputLayer)           [(None, 512)]             0         \n",
      "_________________________________________________________________\n",
      "hidden (Dense)               (None, 200)               102600    \n",
      "_________________________________________________________________\n",
      "output_1 (Dense)             (None, 151)               30351     \n",
      "=================================================================\n",
      "Total params: 132,951\n",
      "Trainable params: 132,951\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mlp_imbalanced = generate_MLP_Model(num_labels= 151, \n",
    "                     num_layers=1, \n",
    "                     hidden_size = 200, \n",
    "                     hidden_activation = 'tanh',\n",
    "                     output_activation = 'softmax',\n",
    "                     dense_dropout=0)\n",
    "mlp_imbalanced.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96fdd952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "first (InputLayer)           [(None, 512)]             0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "hidden (Dense)               (None, 200)               102600    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "output_1 (Dense)             (None, 151)               30351     \n",
      "=================================================================\n",
      "Total params: 132,951\n",
      "Trainable params: 132,951\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mlp_oss_plus = generate_MLP_Model(num_labels= 151, \n",
    "                     num_layers=1, \n",
    "                     hidden_size = 200, \n",
    "                     hidden_activation = 'tanh',\n",
    "                     output_activation = 'softmax',\n",
    "                     dense_dropout=0.1)\n",
    "mlp_oss_plus.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807acc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Small Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60928810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/clinic/data_imbalanced/',\n",
       " '../data/clinic/data_small/',\n",
       " '../data/clinic/data_full/',\n",
       " '../data/clinic/data_oos_plus/']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirs = glob(DATA_FOLDER +\"*/\")\n",
    "dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ecea9b12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/clinic/data_small/oos_test_with_use_emb_not_large.csv',\n",
       " '../data/clinic/data_small/train_with_use_emb_not_large.csv',\n",
       " '../data/clinic/data_small/oos_val_with_use_emb_not_large.csv',\n",
       " '../data/clinic/data_small/val_with_use_emb_not_large.csv',\n",
       " '../data/clinic/data_small/test_with_use_emb_not_large.csv',\n",
       " '../data/clinic/data_small/oos_train_with_use_emb_not_large.csv']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = glob(DATA_FOLDER+ 'data_small/*_with_use_emb_not_large.csv')\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f96bf953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>intent</th>\n",
       "      <th>label</th>\n",
       "      <th>emb_1</th>\n",
       "      <th>emb_2</th>\n",
       "      <th>emb_3</th>\n",
       "      <th>emb_4</th>\n",
       "      <th>emb_5</th>\n",
       "      <th>...</th>\n",
       "      <th>emb_503</th>\n",
       "      <th>emb_504</th>\n",
       "      <th>emb_505</th>\n",
       "      <th>emb_506</th>\n",
       "      <th>emb_507</th>\n",
       "      <th>emb_508</th>\n",
       "      <th>emb_509</th>\n",
       "      <th>emb_510</th>\n",
       "      <th>emb_511</th>\n",
       "      <th>emb_512</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>tell me the expiration date for my current cre...</td>\n",
       "      <td>expiration_date</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.040155</td>\n",
       "      <td>-0.100772</td>\n",
       "      <td>-0.013970</td>\n",
       "      <td>-0.033594</td>\n",
       "      <td>-0.074708</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.071578</td>\n",
       "      <td>0.071027</td>\n",
       "      <td>0.051464</td>\n",
       "      <td>-0.040543</td>\n",
       "      <td>0.089377</td>\n",
       "      <td>0.053426</td>\n",
       "      <td>0.006784</td>\n",
       "      <td>0.016784</td>\n",
       "      <td>0.013674</td>\n",
       "      <td>0.018969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>would you disconnect from my phone</td>\n",
       "      <td>sync_device</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.027262</td>\n",
       "      <td>0.059226</td>\n",
       "      <td>0.034830</td>\n",
       "      <td>-0.007459</td>\n",
       "      <td>0.073154</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024827</td>\n",
       "      <td>0.034608</td>\n",
       "      <td>-0.022895</td>\n",
       "      <td>0.058841</td>\n",
       "      <td>0.006152</td>\n",
       "      <td>-0.021668</td>\n",
       "      <td>0.052534</td>\n",
       "      <td>-0.002309</td>\n",
       "      <td>-0.016219</td>\n",
       "      <td>-0.020532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>could you please track my package</td>\n",
       "      <td>order_status</td>\n",
       "      <td>26</td>\n",
       "      <td>-0.055063</td>\n",
       "      <td>0.014510</td>\n",
       "      <td>-0.011288</td>\n",
       "      <td>0.006235</td>\n",
       "      <td>0.055658</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.048539</td>\n",
       "      <td>0.055937</td>\n",
       "      <td>0.013898</td>\n",
       "      <td>0.021369</td>\n",
       "      <td>-0.053738</td>\n",
       "      <td>-0.047681</td>\n",
       "      <td>0.013829</td>\n",
       "      <td>0.021530</td>\n",
       "      <td>-0.051963</td>\n",
       "      <td>-0.048460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>any travel alerts for canada</td>\n",
       "      <td>travel_alert</td>\n",
       "      <td>28</td>\n",
       "      <td>0.022354</td>\n",
       "      <td>0.026428</td>\n",
       "      <td>0.007004</td>\n",
       "      <td>-0.046039</td>\n",
       "      <td>0.045855</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057745</td>\n",
       "      <td>0.057128</td>\n",
       "      <td>0.019820</td>\n",
       "      <td>-0.001276</td>\n",
       "      <td>0.050386</td>\n",
       "      <td>0.043875</td>\n",
       "      <td>0.031540</td>\n",
       "      <td>0.051629</td>\n",
       "      <td>-0.000464</td>\n",
       "      <td>-0.010409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>i want to report fraudulent activity on my ame...</td>\n",
       "      <td>report_fraud</td>\n",
       "      <td>105</td>\n",
       "      <td>-0.070291</td>\n",
       "      <td>-0.051348</td>\n",
       "      <td>0.029456</td>\n",
       "      <td>-0.044244</td>\n",
       "      <td>0.049450</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.037034</td>\n",
       "      <td>0.071476</td>\n",
       "      <td>-0.002037</td>\n",
       "      <td>-0.055908</td>\n",
       "      <td>0.028856</td>\n",
       "      <td>0.038823</td>\n",
       "      <td>-0.027568</td>\n",
       "      <td>0.010344</td>\n",
       "      <td>-0.081946</td>\n",
       "      <td>-0.004740</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 517 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  Unnamed: 0                                               text  \\\n",
       "0      0           0  tell me the expiration date for my current cre...   \n",
       "1      1           1                 would you disconnect from my phone   \n",
       "2      2           2                  could you please track my package   \n",
       "3      3           3                       any travel alerts for canada   \n",
       "4      4           4  i want to report fraudulent activity on my ame...   \n",
       "\n",
       "            intent  label     emb_1     emb_2     emb_3     emb_4     emb_5  \\\n",
       "0  expiration_date     10 -0.040155 -0.100772 -0.013970 -0.033594 -0.074708   \n",
       "1      sync_device      7 -0.027262  0.059226  0.034830 -0.007459  0.073154   \n",
       "2     order_status     26 -0.055063  0.014510 -0.011288  0.006235  0.055658   \n",
       "3     travel_alert     28  0.022354  0.026428  0.007004 -0.046039  0.045855   \n",
       "4     report_fraud    105 -0.070291 -0.051348  0.029456 -0.044244  0.049450   \n",
       "\n",
       "   ...   emb_503   emb_504   emb_505   emb_506   emb_507   emb_508   emb_509  \\\n",
       "0  ... -0.071578  0.071027  0.051464 -0.040543  0.089377  0.053426  0.006784   \n",
       "1  ... -0.024827  0.034608 -0.022895  0.058841  0.006152 -0.021668  0.052534   \n",
       "2  ... -0.048539  0.055937  0.013898  0.021369 -0.053738 -0.047681  0.013829   \n",
       "3  ...  0.057745  0.057128  0.019820 -0.001276  0.050386  0.043875  0.031540   \n",
       "4  ... -0.037034  0.071476 -0.002037 -0.055908  0.028856  0.038823 -0.027568   \n",
       "\n",
       "    emb_510   emb_511   emb_512  \n",
       "0  0.016784  0.013674  0.018969  \n",
       "1 -0.002309 -0.016219 -0.020532  \n",
       "2  0.021530 -0.051963 -0.048460  \n",
       "3  0.051629 -0.000464 -0.010409  \n",
       "4  0.010344 -0.081946 -0.004740  \n",
       "\n",
       "[5 rows x 517 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.concat([ pd.read_csv('../data/clinic/data_small/train_with_use_emb_not_large.csv'),\n",
    "                        pd.read_csv('../data/clinic/data_small/oos_train_with_use_emb_not_large.csv')]\n",
    "                    ).reset_index()\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5771fd68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7600"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e580e4eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>intent</th>\n",
       "      <th>label</th>\n",
       "      <th>emb_1</th>\n",
       "      <th>emb_2</th>\n",
       "      <th>emb_3</th>\n",
       "      <th>emb_4</th>\n",
       "      <th>emb_5</th>\n",
       "      <th>...</th>\n",
       "      <th>emb_503</th>\n",
       "      <th>emb_504</th>\n",
       "      <th>emb_505</th>\n",
       "      <th>emb_506</th>\n",
       "      <th>emb_507</th>\n",
       "      <th>emb_508</th>\n",
       "      <th>emb_509</th>\n",
       "      <th>emb_510</th>\n",
       "      <th>emb_511</th>\n",
       "      <th>emb_512</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>hey</td>\n",
       "      <td>greeting</td>\n",
       "      <td>18</td>\n",
       "      <td>-0.028977</td>\n",
       "      <td>-0.069464</td>\n",
       "      <td>0.052299</td>\n",
       "      <td>0.033163</td>\n",
       "      <td>0.057072</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.047304</td>\n",
       "      <td>-0.094889</td>\n",
       "      <td>-0.015521</td>\n",
       "      <td>0.052980</td>\n",
       "      <td>0.014295</td>\n",
       "      <td>-0.019171</td>\n",
       "      <td>-0.049758</td>\n",
       "      <td>-0.064394</td>\n",
       "      <td>-0.047327</td>\n",
       "      <td>0.009936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>put laundry on my chore list</td>\n",
       "      <td>todo_list_update</td>\n",
       "      <td>109</td>\n",
       "      <td>0.054710</td>\n",
       "      <td>-0.050362</td>\n",
       "      <td>-0.007406</td>\n",
       "      <td>-0.056255</td>\n",
       "      <td>-0.008085</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014306</td>\n",
       "      <td>0.037495</td>\n",
       "      <td>0.059220</td>\n",
       "      <td>0.070494</td>\n",
       "      <td>-0.047755</td>\n",
       "      <td>-0.054949</td>\n",
       "      <td>0.013341</td>\n",
       "      <td>0.027322</td>\n",
       "      <td>0.028261</td>\n",
       "      <td>0.033261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>go into whisper mode</td>\n",
       "      <td>whisper_mode</td>\n",
       "      <td>129</td>\n",
       "      <td>-0.014857</td>\n",
       "      <td>0.051608</td>\n",
       "      <td>0.013153</td>\n",
       "      <td>-0.019881</td>\n",
       "      <td>0.013295</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015017</td>\n",
       "      <td>-0.042217</td>\n",
       "      <td>0.073386</td>\n",
       "      <td>-0.002565</td>\n",
       "      <td>0.051920</td>\n",
       "      <td>0.003217</td>\n",
       "      <td>-0.042009</td>\n",
       "      <td>0.070887</td>\n",
       "      <td>0.075490</td>\n",
       "      <td>0.002634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>when do i need to change my motor oil again</td>\n",
       "      <td>oil_change_when</td>\n",
       "      <td>46</td>\n",
       "      <td>-0.004057</td>\n",
       "      <td>-0.065462</td>\n",
       "      <td>-0.057030</td>\n",
       "      <td>0.046128</td>\n",
       "      <td>-0.051639</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026361</td>\n",
       "      <td>0.075311</td>\n",
       "      <td>0.016529</td>\n",
       "      <td>-0.046038</td>\n",
       "      <td>0.042125</td>\n",
       "      <td>-0.034304</td>\n",
       "      <td>-0.023360</td>\n",
       "      <td>-0.013787</td>\n",
       "      <td>-0.036103</td>\n",
       "      <td>0.021343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>what is the insurance plan i am enrolled in</td>\n",
       "      <td>insurance</td>\n",
       "      <td>23</td>\n",
       "      <td>-0.087778</td>\n",
       "      <td>-0.073400</td>\n",
       "      <td>-0.011642</td>\n",
       "      <td>-0.036968</td>\n",
       "      <td>-0.045147</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034078</td>\n",
       "      <td>0.072586</td>\n",
       "      <td>0.021591</td>\n",
       "      <td>-0.108329</td>\n",
       "      <td>-0.008622</td>\n",
       "      <td>-0.021959</td>\n",
       "      <td>-0.002704</td>\n",
       "      <td>0.079256</td>\n",
       "      <td>0.074996</td>\n",
       "      <td>-0.014043</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 517 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  Unnamed: 0                                         text  \\\n",
       "0      0           0                                          hey   \n",
       "1      1           1                 put laundry on my chore list   \n",
       "2      2           2                         go into whisper mode   \n",
       "3      3           3  when do i need to change my motor oil again   \n",
       "4      4           4  what is the insurance plan i am enrolled in   \n",
       "\n",
       "             intent  label     emb_1     emb_2     emb_3     emb_4     emb_5  \\\n",
       "0          greeting     18 -0.028977 -0.069464  0.052299  0.033163  0.057072   \n",
       "1  todo_list_update    109  0.054710 -0.050362 -0.007406 -0.056255 -0.008085   \n",
       "2      whisper_mode    129 -0.014857  0.051608  0.013153 -0.019881  0.013295   \n",
       "3   oil_change_when     46 -0.004057 -0.065462 -0.057030  0.046128 -0.051639   \n",
       "4         insurance     23 -0.087778 -0.073400 -0.011642 -0.036968 -0.045147   \n",
       "\n",
       "   ...   emb_503   emb_504   emb_505   emb_506   emb_507   emb_508   emb_509  \\\n",
       "0  ... -0.047304 -0.094889 -0.015521  0.052980  0.014295 -0.019171 -0.049758   \n",
       "1  ...  0.014306  0.037495  0.059220  0.070494 -0.047755 -0.054949  0.013341   \n",
       "2  ...  0.015017 -0.042217  0.073386 -0.002565  0.051920  0.003217 -0.042009   \n",
       "3  ...  0.026361  0.075311  0.016529 -0.046038  0.042125 -0.034304 -0.023360   \n",
       "4  ...  0.034078  0.072586  0.021591 -0.108329 -0.008622 -0.021959 -0.002704   \n",
       "\n",
       "    emb_510   emb_511   emb_512  \n",
       "0 -0.064394 -0.047327  0.009936  \n",
       "1  0.027322  0.028261  0.033261  \n",
       "2  0.070887  0.075490  0.002634  \n",
       "3 -0.013787 -0.036103  0.021343  \n",
       "4  0.079256  0.074996 -0.014043  \n",
       "\n",
       "[5 rows x 517 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid = pd.concat([ pd.read_csv('../data/clinic/data_small/val_with_use_emb_not_large.csv'),\n",
    "                        pd.read_csv('../data/clinic/data_small/oos_val_with_use_emb_not_large.csv')]\n",
    "                    ).reset_index()\n",
    "df_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fe23001b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>intent</th>\n",
       "      <th>label</th>\n",
       "      <th>emb_1</th>\n",
       "      <th>emb_2</th>\n",
       "      <th>emb_3</th>\n",
       "      <th>emb_4</th>\n",
       "      <th>emb_5</th>\n",
       "      <th>emb_6</th>\n",
       "      <th>...</th>\n",
       "      <th>emb_503</th>\n",
       "      <th>emb_504</th>\n",
       "      <th>emb_505</th>\n",
       "      <th>emb_506</th>\n",
       "      <th>emb_507</th>\n",
       "      <th>emb_508</th>\n",
       "      <th>emb_509</th>\n",
       "      <th>emb_510</th>\n",
       "      <th>emb_511</th>\n",
       "      <th>emb_512</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>thanks so much ai</td>\n",
       "      <td>thank_you</td>\n",
       "      <td>36</td>\n",
       "      <td>-0.029997</td>\n",
       "      <td>-0.014980</td>\n",
       "      <td>0.061295</td>\n",
       "      <td>-0.030674</td>\n",
       "      <td>0.021879</td>\n",
       "      <td>-0.024469</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013312</td>\n",
       "      <td>-0.001770</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.063153</td>\n",
       "      <td>0.062007</td>\n",
       "      <td>0.022718</td>\n",
       "      <td>-0.013320</td>\n",
       "      <td>-0.007570</td>\n",
       "      <td>0.006375</td>\n",
       "      <td>-0.001329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>i will be traveling to lima alert my bank</td>\n",
       "      <td>travel_notification</td>\n",
       "      <td>6</td>\n",
       "      <td>0.036371</td>\n",
       "      <td>-0.010319</td>\n",
       "      <td>0.032173</td>\n",
       "      <td>-0.059230</td>\n",
       "      <td>0.027334</td>\n",
       "      <td>0.054314</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019954</td>\n",
       "      <td>0.076818</td>\n",
       "      <td>0.082983</td>\n",
       "      <td>0.003128</td>\n",
       "      <td>0.034301</td>\n",
       "      <td>0.099494</td>\n",
       "      <td>-0.024311</td>\n",
       "      <td>0.003610</td>\n",
       "      <td>-0.049011</td>\n",
       "      <td>-0.095653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>say again please</td>\n",
       "      <td>repeat</td>\n",
       "      <td>135</td>\n",
       "      <td>-0.038975</td>\n",
       "      <td>-0.006627</td>\n",
       "      <td>-0.043051</td>\n",
       "      <td>0.070269</td>\n",
       "      <td>0.036932</td>\n",
       "      <td>0.040348</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047787</td>\n",
       "      <td>-0.092466</td>\n",
       "      <td>-0.040176</td>\n",
       "      <td>-0.006622</td>\n",
       "      <td>0.040359</td>\n",
       "      <td>-0.041509</td>\n",
       "      <td>0.006387</td>\n",
       "      <td>-0.057521</td>\n",
       "      <td>-0.028157</td>\n",
       "      <td>-0.038858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>what is needed to cook lasagna</td>\n",
       "      <td>ingredients_list</td>\n",
       "      <td>60</td>\n",
       "      <td>-0.053814</td>\n",
       "      <td>-0.061073</td>\n",
       "      <td>0.036123</td>\n",
       "      <td>0.043716</td>\n",
       "      <td>0.050313</td>\n",
       "      <td>0.019721</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000789</td>\n",
       "      <td>0.070016</td>\n",
       "      <td>0.030398</td>\n",
       "      <td>0.051650</td>\n",
       "      <td>-0.026917</td>\n",
       "      <td>0.029005</td>\n",
       "      <td>-0.003356</td>\n",
       "      <td>-0.033767</td>\n",
       "      <td>0.047321</td>\n",
       "      <td>-0.090501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>give me the pressure for the tires on my car</td>\n",
       "      <td>tire_pressure</td>\n",
       "      <td>38</td>\n",
       "      <td>0.012925</td>\n",
       "      <td>-0.014824</td>\n",
       "      <td>0.076381</td>\n",
       "      <td>0.021814</td>\n",
       "      <td>0.054738</td>\n",
       "      <td>-0.006144</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056830</td>\n",
       "      <td>0.071343</td>\n",
       "      <td>0.029660</td>\n",
       "      <td>-0.095485</td>\n",
       "      <td>-0.011102</td>\n",
       "      <td>0.044503</td>\n",
       "      <td>-0.001196</td>\n",
       "      <td>-0.004610</td>\n",
       "      <td>0.017682</td>\n",
       "      <td>0.019733</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 516 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                          text  \\\n",
       "0           0                             thanks so much ai   \n",
       "1           1     i will be traveling to lima alert my bank   \n",
       "2           2                              say again please   \n",
       "3           3                what is needed to cook lasagna   \n",
       "4           4  give me the pressure for the tires on my car   \n",
       "\n",
       "                intent  label     emb_1     emb_2     emb_3     emb_4  \\\n",
       "0            thank_you     36 -0.029997 -0.014980  0.061295 -0.030674   \n",
       "1  travel_notification      6  0.036371 -0.010319  0.032173 -0.059230   \n",
       "2               repeat    135 -0.038975 -0.006627 -0.043051  0.070269   \n",
       "3     ingredients_list     60 -0.053814 -0.061073  0.036123  0.043716   \n",
       "4        tire_pressure     38  0.012925 -0.014824  0.076381  0.021814   \n",
       "\n",
       "      emb_5     emb_6  ...   emb_503   emb_504   emb_505   emb_506   emb_507  \\\n",
       "0  0.021879 -0.024469  ...  0.013312 -0.001770  0.033333  0.063153  0.062007   \n",
       "1  0.027334  0.054314  ...  0.019954  0.076818  0.082983  0.003128  0.034301   \n",
       "2  0.036932  0.040348  ...  0.047787 -0.092466 -0.040176 -0.006622  0.040359   \n",
       "3  0.050313  0.019721  ...  0.000789  0.070016  0.030398  0.051650 -0.026917   \n",
       "4  0.054738 -0.006144  ...  0.056830  0.071343  0.029660 -0.095485 -0.011102   \n",
       "\n",
       "    emb_508   emb_509   emb_510   emb_511   emb_512  \n",
       "0  0.022718 -0.013320 -0.007570  0.006375 -0.001329  \n",
       "1  0.099494 -0.024311  0.003610 -0.049011 -0.095653  \n",
       "2 -0.041509  0.006387 -0.057521 -0.028157 -0.038858  \n",
       "3  0.029005 -0.003356 -0.033767  0.047321 -0.090501  \n",
       "4  0.044503 -0.001196 -0.004610  0.017682  0.019733  \n",
       "\n",
       "[5 rows x 516 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_inscope = pd.read_csv('../data/clinic/data_small/test_with_use_emb_not_large.csv')\n",
    "df_test_inscope.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0e84a084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>intent</th>\n",
       "      <th>label</th>\n",
       "      <th>emb_1</th>\n",
       "      <th>emb_2</th>\n",
       "      <th>emb_3</th>\n",
       "      <th>emb_4</th>\n",
       "      <th>emb_5</th>\n",
       "      <th>emb_6</th>\n",
       "      <th>...</th>\n",
       "      <th>emb_503</th>\n",
       "      <th>emb_504</th>\n",
       "      <th>emb_505</th>\n",
       "      <th>emb_506</th>\n",
       "      <th>emb_507</th>\n",
       "      <th>emb_508</th>\n",
       "      <th>emb_509</th>\n",
       "      <th>emb_510</th>\n",
       "      <th>emb_511</th>\n",
       "      <th>emb_512</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>can i get a sear's appliance repairman</td>\n",
       "      <td>oos</td>\n",
       "      <td>150</td>\n",
       "      <td>0.001829</td>\n",
       "      <td>-0.016941</td>\n",
       "      <td>-0.054595</td>\n",
       "      <td>0.017081</td>\n",
       "      <td>-0.029765</td>\n",
       "      <td>0.070834</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.043084</td>\n",
       "      <td>0.029035</td>\n",
       "      <td>0.030350</td>\n",
       "      <td>0.045751</td>\n",
       "      <td>0.057523</td>\n",
       "      <td>-0.031591</td>\n",
       "      <td>0.023559</td>\n",
       "      <td>-0.036538</td>\n",
       "      <td>0.042396</td>\n",
       "      <td>-0.006072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>what do you do if you can't stop vomiting</td>\n",
       "      <td>oos</td>\n",
       "      <td>150</td>\n",
       "      <td>0.013368</td>\n",
       "      <td>0.040282</td>\n",
       "      <td>0.002469</td>\n",
       "      <td>-0.001204</td>\n",
       "      <td>0.033581</td>\n",
       "      <td>-0.010957</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006601</td>\n",
       "      <td>0.080698</td>\n",
       "      <td>-0.067142</td>\n",
       "      <td>-0.101796</td>\n",
       "      <td>0.047814</td>\n",
       "      <td>-0.013325</td>\n",
       "      <td>-0.005319</td>\n",
       "      <td>-0.066210</td>\n",
       "      <td>-0.003475</td>\n",
       "      <td>-0.039296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>how many ppm of particulate is in my local water</td>\n",
       "      <td>oos</td>\n",
       "      <td>150</td>\n",
       "      <td>-0.007001</td>\n",
       "      <td>-0.025082</td>\n",
       "      <td>-0.052956</td>\n",
       "      <td>-0.001758</td>\n",
       "      <td>0.020452</td>\n",
       "      <td>-0.012768</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054614</td>\n",
       "      <td>0.085859</td>\n",
       "      <td>0.026262</td>\n",
       "      <td>-0.018662</td>\n",
       "      <td>0.003845</td>\n",
       "      <td>-0.009895</td>\n",
       "      <td>-0.027959</td>\n",
       "      <td>0.063628</td>\n",
       "      <td>-0.027185</td>\n",
       "      <td>-0.058524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>get me a list of divorce attorneys in the new ...</td>\n",
       "      <td>oos</td>\n",
       "      <td>150</td>\n",
       "      <td>0.023987</td>\n",
       "      <td>-0.019148</td>\n",
       "      <td>0.013654</td>\n",
       "      <td>-0.011939</td>\n",
       "      <td>-0.000627</td>\n",
       "      <td>0.023051</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042193</td>\n",
       "      <td>0.062463</td>\n",
       "      <td>0.063075</td>\n",
       "      <td>-0.016516</td>\n",
       "      <td>0.061391</td>\n",
       "      <td>0.053259</td>\n",
       "      <td>0.023495</td>\n",
       "      <td>0.031551</td>\n",
       "      <td>0.062632</td>\n",
       "      <td>0.025739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>clear my search history</td>\n",
       "      <td>oos</td>\n",
       "      <td>150</td>\n",
       "      <td>0.036190</td>\n",
       "      <td>0.066142</td>\n",
       "      <td>-0.017360</td>\n",
       "      <td>0.037186</td>\n",
       "      <td>-0.020080</td>\n",
       "      <td>-0.054559</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081030</td>\n",
       "      <td>-0.034575</td>\n",
       "      <td>0.050740</td>\n",
       "      <td>0.011291</td>\n",
       "      <td>0.019406</td>\n",
       "      <td>0.001155</td>\n",
       "      <td>-0.024586</td>\n",
       "      <td>0.029265</td>\n",
       "      <td>-0.041097</td>\n",
       "      <td>0.017834</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 516 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text intent  \\\n",
       "0           0             can i get a sear's appliance repairman    oos   \n",
       "1           1          what do you do if you can't stop vomiting    oos   \n",
       "2           2   how many ppm of particulate is in my local water    oos   \n",
       "3           3  get me a list of divorce attorneys in the new ...    oos   \n",
       "4           4                            clear my search history    oos   \n",
       "\n",
       "   label     emb_1     emb_2     emb_3     emb_4     emb_5     emb_6  ...  \\\n",
       "0    150  0.001829 -0.016941 -0.054595  0.017081 -0.029765  0.070834  ...   \n",
       "1    150  0.013368  0.040282  0.002469 -0.001204  0.033581 -0.010957  ...   \n",
       "2    150 -0.007001 -0.025082 -0.052956 -0.001758  0.020452 -0.012768  ...   \n",
       "3    150  0.023987 -0.019148  0.013654 -0.011939 -0.000627  0.023051  ...   \n",
       "4    150  0.036190  0.066142 -0.017360  0.037186 -0.020080 -0.054559  ...   \n",
       "\n",
       "    emb_503   emb_504   emb_505   emb_506   emb_507   emb_508   emb_509  \\\n",
       "0 -0.043084  0.029035  0.030350  0.045751  0.057523 -0.031591  0.023559   \n",
       "1 -0.006601  0.080698 -0.067142 -0.101796  0.047814 -0.013325 -0.005319   \n",
       "2  0.054614  0.085859  0.026262 -0.018662  0.003845 -0.009895 -0.027959   \n",
       "3  0.042193  0.062463  0.063075 -0.016516  0.061391  0.053259  0.023495   \n",
       "4  0.081030 -0.034575  0.050740  0.011291  0.019406  0.001155 -0.024586   \n",
       "\n",
       "    emb_510   emb_511   emb_512  \n",
       "0 -0.036538  0.042396 -0.006072  \n",
       "1 -0.066210 -0.003475 -0.039296  \n",
       "2  0.063628 -0.027185 -0.058524  \n",
       "3  0.031551  0.062632  0.025739  \n",
       "4  0.029265 -0.041097  0.017834  \n",
       "\n",
       "[5 rows x 516 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_oos = pd.read_csv('../data/clinic/data_small/oos_test_with_use_emb_not_large.csv')\n",
    "df_test_oos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "247bc0d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emb_1',\n",
       " 'emb_2',\n",
       " 'emb_3',\n",
       " 'emb_4',\n",
       " 'emb_5',\n",
       " 'emb_6',\n",
       " 'emb_7',\n",
       " 'emb_8',\n",
       " 'emb_9',\n",
       " 'emb_10',\n",
       " 'emb_11',\n",
       " 'emb_12',\n",
       " 'emb_13',\n",
       " 'emb_14',\n",
       " 'emb_15',\n",
       " 'emb_16',\n",
       " 'emb_17',\n",
       " 'emb_18',\n",
       " 'emb_19',\n",
       " 'emb_20',\n",
       " 'emb_21',\n",
       " 'emb_22',\n",
       " 'emb_23',\n",
       " 'emb_24',\n",
       " 'emb_25',\n",
       " 'emb_26',\n",
       " 'emb_27',\n",
       " 'emb_28',\n",
       " 'emb_29',\n",
       " 'emb_30',\n",
       " 'emb_31',\n",
       " 'emb_32',\n",
       " 'emb_33',\n",
       " 'emb_34',\n",
       " 'emb_35',\n",
       " 'emb_36',\n",
       " 'emb_37',\n",
       " 'emb_38',\n",
       " 'emb_39',\n",
       " 'emb_40',\n",
       " 'emb_41',\n",
       " 'emb_42',\n",
       " 'emb_43',\n",
       " 'emb_44',\n",
       " 'emb_45',\n",
       " 'emb_46',\n",
       " 'emb_47',\n",
       " 'emb_48',\n",
       " 'emb_49',\n",
       " 'emb_50',\n",
       " 'emb_51',\n",
       " 'emb_52',\n",
       " 'emb_53',\n",
       " 'emb_54',\n",
       " 'emb_55',\n",
       " 'emb_56',\n",
       " 'emb_57',\n",
       " 'emb_58',\n",
       " 'emb_59',\n",
       " 'emb_60',\n",
       " 'emb_61',\n",
       " 'emb_62',\n",
       " 'emb_63',\n",
       " 'emb_64',\n",
       " 'emb_65',\n",
       " 'emb_66',\n",
       " 'emb_67',\n",
       " 'emb_68',\n",
       " 'emb_69',\n",
       " 'emb_70',\n",
       " 'emb_71',\n",
       " 'emb_72',\n",
       " 'emb_73',\n",
       " 'emb_74',\n",
       " 'emb_75',\n",
       " 'emb_76',\n",
       " 'emb_77',\n",
       " 'emb_78',\n",
       " 'emb_79',\n",
       " 'emb_80',\n",
       " 'emb_81',\n",
       " 'emb_82',\n",
       " 'emb_83',\n",
       " 'emb_84',\n",
       " 'emb_85',\n",
       " 'emb_86',\n",
       " 'emb_87',\n",
       " 'emb_88',\n",
       " 'emb_89',\n",
       " 'emb_90',\n",
       " 'emb_91',\n",
       " 'emb_92',\n",
       " 'emb_93',\n",
       " 'emb_94',\n",
       " 'emb_95',\n",
       " 'emb_96',\n",
       " 'emb_97',\n",
       " 'emb_98',\n",
       " 'emb_99',\n",
       " 'emb_100',\n",
       " 'emb_101',\n",
       " 'emb_102',\n",
       " 'emb_103',\n",
       " 'emb_104',\n",
       " 'emb_105',\n",
       " 'emb_106',\n",
       " 'emb_107',\n",
       " 'emb_108',\n",
       " 'emb_109',\n",
       " 'emb_110',\n",
       " 'emb_111',\n",
       " 'emb_112',\n",
       " 'emb_113',\n",
       " 'emb_114',\n",
       " 'emb_115',\n",
       " 'emb_116',\n",
       " 'emb_117',\n",
       " 'emb_118',\n",
       " 'emb_119',\n",
       " 'emb_120',\n",
       " 'emb_121',\n",
       " 'emb_122',\n",
       " 'emb_123',\n",
       " 'emb_124',\n",
       " 'emb_125',\n",
       " 'emb_126',\n",
       " 'emb_127',\n",
       " 'emb_128',\n",
       " 'emb_129',\n",
       " 'emb_130',\n",
       " 'emb_131',\n",
       " 'emb_132',\n",
       " 'emb_133',\n",
       " 'emb_134',\n",
       " 'emb_135',\n",
       " 'emb_136',\n",
       " 'emb_137',\n",
       " 'emb_138',\n",
       " 'emb_139',\n",
       " 'emb_140',\n",
       " 'emb_141',\n",
       " 'emb_142',\n",
       " 'emb_143',\n",
       " 'emb_144',\n",
       " 'emb_145',\n",
       " 'emb_146',\n",
       " 'emb_147',\n",
       " 'emb_148',\n",
       " 'emb_149',\n",
       " 'emb_150',\n",
       " 'emb_151',\n",
       " 'emb_152',\n",
       " 'emb_153',\n",
       " 'emb_154',\n",
       " 'emb_155',\n",
       " 'emb_156',\n",
       " 'emb_157',\n",
       " 'emb_158',\n",
       " 'emb_159',\n",
       " 'emb_160',\n",
       " 'emb_161',\n",
       " 'emb_162',\n",
       " 'emb_163',\n",
       " 'emb_164',\n",
       " 'emb_165',\n",
       " 'emb_166',\n",
       " 'emb_167',\n",
       " 'emb_168',\n",
       " 'emb_169',\n",
       " 'emb_170',\n",
       " 'emb_171',\n",
       " 'emb_172',\n",
       " 'emb_173',\n",
       " 'emb_174',\n",
       " 'emb_175',\n",
       " 'emb_176',\n",
       " 'emb_177',\n",
       " 'emb_178',\n",
       " 'emb_179',\n",
       " 'emb_180',\n",
       " 'emb_181',\n",
       " 'emb_182',\n",
       " 'emb_183',\n",
       " 'emb_184',\n",
       " 'emb_185',\n",
       " 'emb_186',\n",
       " 'emb_187',\n",
       " 'emb_188',\n",
       " 'emb_189',\n",
       " 'emb_190',\n",
       " 'emb_191',\n",
       " 'emb_192',\n",
       " 'emb_193',\n",
       " 'emb_194',\n",
       " 'emb_195',\n",
       " 'emb_196',\n",
       " 'emb_197',\n",
       " 'emb_198',\n",
       " 'emb_199',\n",
       " 'emb_200',\n",
       " 'emb_201',\n",
       " 'emb_202',\n",
       " 'emb_203',\n",
       " 'emb_204',\n",
       " 'emb_205',\n",
       " 'emb_206',\n",
       " 'emb_207',\n",
       " 'emb_208',\n",
       " 'emb_209',\n",
       " 'emb_210',\n",
       " 'emb_211',\n",
       " 'emb_212',\n",
       " 'emb_213',\n",
       " 'emb_214',\n",
       " 'emb_215',\n",
       " 'emb_216',\n",
       " 'emb_217',\n",
       " 'emb_218',\n",
       " 'emb_219',\n",
       " 'emb_220',\n",
       " 'emb_221',\n",
       " 'emb_222',\n",
       " 'emb_223',\n",
       " 'emb_224',\n",
       " 'emb_225',\n",
       " 'emb_226',\n",
       " 'emb_227',\n",
       " 'emb_228',\n",
       " 'emb_229',\n",
       " 'emb_230',\n",
       " 'emb_231',\n",
       " 'emb_232',\n",
       " 'emb_233',\n",
       " 'emb_234',\n",
       " 'emb_235',\n",
       " 'emb_236',\n",
       " 'emb_237',\n",
       " 'emb_238',\n",
       " 'emb_239',\n",
       " 'emb_240',\n",
       " 'emb_241',\n",
       " 'emb_242',\n",
       " 'emb_243',\n",
       " 'emb_244',\n",
       " 'emb_245',\n",
       " 'emb_246',\n",
       " 'emb_247',\n",
       " 'emb_248',\n",
       " 'emb_249',\n",
       " 'emb_250',\n",
       " 'emb_251',\n",
       " 'emb_252',\n",
       " 'emb_253',\n",
       " 'emb_254',\n",
       " 'emb_255',\n",
       " 'emb_256',\n",
       " 'emb_257',\n",
       " 'emb_258',\n",
       " 'emb_259',\n",
       " 'emb_260',\n",
       " 'emb_261',\n",
       " 'emb_262',\n",
       " 'emb_263',\n",
       " 'emb_264',\n",
       " 'emb_265',\n",
       " 'emb_266',\n",
       " 'emb_267',\n",
       " 'emb_268',\n",
       " 'emb_269',\n",
       " 'emb_270',\n",
       " 'emb_271',\n",
       " 'emb_272',\n",
       " 'emb_273',\n",
       " 'emb_274',\n",
       " 'emb_275',\n",
       " 'emb_276',\n",
       " 'emb_277',\n",
       " 'emb_278',\n",
       " 'emb_279',\n",
       " 'emb_280',\n",
       " 'emb_281',\n",
       " 'emb_282',\n",
       " 'emb_283',\n",
       " 'emb_284',\n",
       " 'emb_285',\n",
       " 'emb_286',\n",
       " 'emb_287',\n",
       " 'emb_288',\n",
       " 'emb_289',\n",
       " 'emb_290',\n",
       " 'emb_291',\n",
       " 'emb_292',\n",
       " 'emb_293',\n",
       " 'emb_294',\n",
       " 'emb_295',\n",
       " 'emb_296',\n",
       " 'emb_297',\n",
       " 'emb_298',\n",
       " 'emb_299',\n",
       " 'emb_300',\n",
       " 'emb_301',\n",
       " 'emb_302',\n",
       " 'emb_303',\n",
       " 'emb_304',\n",
       " 'emb_305',\n",
       " 'emb_306',\n",
       " 'emb_307',\n",
       " 'emb_308',\n",
       " 'emb_309',\n",
       " 'emb_310',\n",
       " 'emb_311',\n",
       " 'emb_312',\n",
       " 'emb_313',\n",
       " 'emb_314',\n",
       " 'emb_315',\n",
       " 'emb_316',\n",
       " 'emb_317',\n",
       " 'emb_318',\n",
       " 'emb_319',\n",
       " 'emb_320',\n",
       " 'emb_321',\n",
       " 'emb_322',\n",
       " 'emb_323',\n",
       " 'emb_324',\n",
       " 'emb_325',\n",
       " 'emb_326',\n",
       " 'emb_327',\n",
       " 'emb_328',\n",
       " 'emb_329',\n",
       " 'emb_330',\n",
       " 'emb_331',\n",
       " 'emb_332',\n",
       " 'emb_333',\n",
       " 'emb_334',\n",
       " 'emb_335',\n",
       " 'emb_336',\n",
       " 'emb_337',\n",
       " 'emb_338',\n",
       " 'emb_339',\n",
       " 'emb_340',\n",
       " 'emb_341',\n",
       " 'emb_342',\n",
       " 'emb_343',\n",
       " 'emb_344',\n",
       " 'emb_345',\n",
       " 'emb_346',\n",
       " 'emb_347',\n",
       " 'emb_348',\n",
       " 'emb_349',\n",
       " 'emb_350',\n",
       " 'emb_351',\n",
       " 'emb_352',\n",
       " 'emb_353',\n",
       " 'emb_354',\n",
       " 'emb_355',\n",
       " 'emb_356',\n",
       " 'emb_357',\n",
       " 'emb_358',\n",
       " 'emb_359',\n",
       " 'emb_360',\n",
       " 'emb_361',\n",
       " 'emb_362',\n",
       " 'emb_363',\n",
       " 'emb_364',\n",
       " 'emb_365',\n",
       " 'emb_366',\n",
       " 'emb_367',\n",
       " 'emb_368',\n",
       " 'emb_369',\n",
       " 'emb_370',\n",
       " 'emb_371',\n",
       " 'emb_372',\n",
       " 'emb_373',\n",
       " 'emb_374',\n",
       " 'emb_375',\n",
       " 'emb_376',\n",
       " 'emb_377',\n",
       " 'emb_378',\n",
       " 'emb_379',\n",
       " 'emb_380',\n",
       " 'emb_381',\n",
       " 'emb_382',\n",
       " 'emb_383',\n",
       " 'emb_384',\n",
       " 'emb_385',\n",
       " 'emb_386',\n",
       " 'emb_387',\n",
       " 'emb_388',\n",
       " 'emb_389',\n",
       " 'emb_390',\n",
       " 'emb_391',\n",
       " 'emb_392',\n",
       " 'emb_393',\n",
       " 'emb_394',\n",
       " 'emb_395',\n",
       " 'emb_396',\n",
       " 'emb_397',\n",
       " 'emb_398',\n",
       " 'emb_399',\n",
       " 'emb_400',\n",
       " 'emb_401',\n",
       " 'emb_402',\n",
       " 'emb_403',\n",
       " 'emb_404',\n",
       " 'emb_405',\n",
       " 'emb_406',\n",
       " 'emb_407',\n",
       " 'emb_408',\n",
       " 'emb_409',\n",
       " 'emb_410',\n",
       " 'emb_411',\n",
       " 'emb_412',\n",
       " 'emb_413',\n",
       " 'emb_414',\n",
       " 'emb_415',\n",
       " 'emb_416',\n",
       " 'emb_417',\n",
       " 'emb_418',\n",
       " 'emb_419',\n",
       " 'emb_420',\n",
       " 'emb_421',\n",
       " 'emb_422',\n",
       " 'emb_423',\n",
       " 'emb_424',\n",
       " 'emb_425',\n",
       " 'emb_426',\n",
       " 'emb_427',\n",
       " 'emb_428',\n",
       " 'emb_429',\n",
       " 'emb_430',\n",
       " 'emb_431',\n",
       " 'emb_432',\n",
       " 'emb_433',\n",
       " 'emb_434',\n",
       " 'emb_435',\n",
       " 'emb_436',\n",
       " 'emb_437',\n",
       " 'emb_438',\n",
       " 'emb_439',\n",
       " 'emb_440',\n",
       " 'emb_441',\n",
       " 'emb_442',\n",
       " 'emb_443',\n",
       " 'emb_444',\n",
       " 'emb_445',\n",
       " 'emb_446',\n",
       " 'emb_447',\n",
       " 'emb_448',\n",
       " 'emb_449',\n",
       " 'emb_450',\n",
       " 'emb_451',\n",
       " 'emb_452',\n",
       " 'emb_453',\n",
       " 'emb_454',\n",
       " 'emb_455',\n",
       " 'emb_456',\n",
       " 'emb_457',\n",
       " 'emb_458',\n",
       " 'emb_459',\n",
       " 'emb_460',\n",
       " 'emb_461',\n",
       " 'emb_462',\n",
       " 'emb_463',\n",
       " 'emb_464',\n",
       " 'emb_465',\n",
       " 'emb_466',\n",
       " 'emb_467',\n",
       " 'emb_468',\n",
       " 'emb_469',\n",
       " 'emb_470',\n",
       " 'emb_471',\n",
       " 'emb_472',\n",
       " 'emb_473',\n",
       " 'emb_474',\n",
       " 'emb_475',\n",
       " 'emb_476',\n",
       " 'emb_477',\n",
       " 'emb_478',\n",
       " 'emb_479',\n",
       " 'emb_480',\n",
       " 'emb_481',\n",
       " 'emb_482',\n",
       " 'emb_483',\n",
       " 'emb_484',\n",
       " 'emb_485',\n",
       " 'emb_486',\n",
       " 'emb_487',\n",
       " 'emb_488',\n",
       " 'emb_489',\n",
       " 'emb_490',\n",
       " 'emb_491',\n",
       " 'emb_492',\n",
       " 'emb_493',\n",
       " 'emb_494',\n",
       " 'emb_495',\n",
       " 'emb_496',\n",
       " 'emb_497',\n",
       " 'emb_498',\n",
       " 'emb_499',\n",
       " 'emb_500',\n",
       " 'emb_501',\n",
       " 'emb_502',\n",
       " 'emb_503',\n",
       " 'emb_504',\n",
       " 'emb_505',\n",
       " 'emb_506',\n",
       " 'emb_507',\n",
       " 'emb_508',\n",
       " 'emb_509',\n",
       " 'emb_510',\n",
       " 'emb_511',\n",
       " 'emb_512']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_cols = ['emb'+'_'+str(i+1) for i in range(512)]\n",
    "emb_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e9e3bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import callbacks\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "earlystopping = callbacks.EarlyStopping(monitor =\"val_loss\", \n",
    "                                            mode =\"min\", patience = 20, \n",
    "                                            restore_best_weights = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f21e8350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7600 samples, validate on 3100 samples\n",
      "Epoch 1/100\n",
      "7600/7600 [==============================] - 1s 85us/sample - loss: 4.7482 - acc: 0.2655 - val_loss: 4.2660 - val_acc: 0.3400\n",
      "Epoch 2/100\n",
      "7600/7600 [==============================] - 0s 31us/sample - loss: 3.4773 - acc: 0.6183 - val_loss: 2.6681 - val_acc: 0.7961\n",
      "Epoch 3/100\n",
      "7600/7600 [==============================] - 0s 31us/sample - loss: 1.9228 - acc: 0.8317 - val_loss: 1.4772 - val_acc: 0.8526\n",
      "Epoch 4/100\n",
      "7600/7600 [==============================] - 0s 32us/sample - loss: 1.0639 - acc: 0.8895 - val_loss: 0.9567 - val_acc: 0.8861\n",
      "Epoch 5/100\n",
      "7600/7600 [==============================] - 0s 32us/sample - loss: 0.6958 - acc: 0.9155 - val_loss: 0.7142 - val_acc: 0.9023\n",
      "Epoch 6/100\n",
      "7600/7600 [==============================] - 0s 36us/sample - loss: 0.5164 - acc: 0.9291 - val_loss: 0.5839 - val_acc: 0.9084\n",
      "Epoch 7/100\n",
      "7600/7600 [==============================] - 0s 39us/sample - loss: 0.4105 - acc: 0.9362 - val_loss: 0.5035 - val_acc: 0.9135\n",
      "Epoch 8/100\n",
      "7600/7600 [==============================] - 0s 33us/sample - loss: 0.3344 - acc: 0.9480 - val_loss: 0.4492 - val_acc: 0.9206\n",
      "Epoch 9/100\n",
      "7600/7600 [==============================] - 0s 45us/sample - loss: 0.2904 - acc: 0.9511 - val_loss: 0.4134 - val_acc: 0.9197\n",
      "Epoch 10/100\n",
      "7600/7600 [==============================] - 0s 36us/sample - loss: 0.2525 - acc: 0.9568 - val_loss: 0.3808 - val_acc: 0.9281\n",
      "Epoch 11/100\n",
      "7600/7600 [==============================] - 0s 32us/sample - loss: 0.2166 - acc: 0.9655 - val_loss: 0.3618 - val_acc: 0.9290\n",
      "Epoch 12/100\n",
      "7600/7600 [==============================] - 0s 32us/sample - loss: 0.1984 - acc: 0.9662 - val_loss: 0.3441 - val_acc: 0.9255\n",
      "Epoch 13/100\n",
      "7600/7600 [==============================] - 0s 36us/sample - loss: 0.1763 - acc: 0.9703 - val_loss: 0.3292 - val_acc: 0.9277\n",
      "Epoch 14/100\n",
      "7600/7600 [==============================] - 0s 36us/sample - loss: 0.1599 - acc: 0.9720 - val_loss: 0.3193 - val_acc: 0.9297\n",
      "Epoch 15/100\n",
      "7600/7600 [==============================] - 0s 35us/sample - loss: 0.1441 - acc: 0.9763 - val_loss: 0.3064 - val_acc: 0.9332\n",
      "Epoch 16/100\n",
      "7600/7600 [==============================] - 0s 37us/sample - loss: 0.1340 - acc: 0.9778 - val_loss: 0.3023 - val_acc: 0.9323\n",
      "Epoch 17/100\n",
      "7600/7600 [==============================] - 0s 35us/sample - loss: 0.1220 - acc: 0.9809 - val_loss: 0.2914 - val_acc: 0.9345\n",
      "Epoch 18/100\n",
      "7600/7600 [==============================] - 0s 31us/sample - loss: 0.1115 - acc: 0.9813 - val_loss: 0.2871 - val_acc: 0.9374\n",
      "Epoch 19/100\n",
      "7600/7600 [==============================] - 0s 36us/sample - loss: 0.1058 - acc: 0.9800 - val_loss: 0.2798 - val_acc: 0.9361\n",
      "Epoch 20/100\n",
      "7600/7600 [==============================] - 0s 32us/sample - loss: 0.0987 - acc: 0.9855 - val_loss: 0.2796 - val_acc: 0.9345\n",
      "Epoch 21/100\n",
      "7600/7600 [==============================] - 0s 31us/sample - loss: 0.0897 - acc: 0.9859 - val_loss: 0.2706 - val_acc: 0.9361\n",
      "Epoch 22/100\n",
      "7600/7600 [==============================] - 0s 32us/sample - loss: 0.0829 - acc: 0.9887 - val_loss: 0.2693 - val_acc: 0.9384\n",
      "Epoch 23/100\n",
      "7600/7600 [==============================] - 0s 33us/sample - loss: 0.0774 - acc: 0.9889 - val_loss: 0.2659 - val_acc: 0.9374\n",
      "Epoch 24/100\n",
      "7600/7600 [==============================] - 0s 40us/sample - loss: 0.0729 - acc: 0.9897 - val_loss: 0.2682 - val_acc: 0.9332\n",
      "Epoch 25/100\n",
      "7600/7600 [==============================] - 0s 36us/sample - loss: 0.0683 - acc: 0.9895 - val_loss: 0.2627 - val_acc: 0.9387\n",
      "Epoch 26/100\n",
      "7600/7600 [==============================] - 0s 33us/sample - loss: 0.0646 - acc: 0.9908 - val_loss: 0.2648 - val_acc: 0.9329\n",
      "Epoch 27/100\n",
      "7600/7600 [==============================] - 0s 37us/sample - loss: 0.0598 - acc: 0.9925 - val_loss: 0.2612 - val_acc: 0.9352\n",
      "Epoch 28/100\n",
      "7600/7600 [==============================] - 0s 34us/sample - loss: 0.0548 - acc: 0.9929 - val_loss: 0.2629 - val_acc: 0.9365\n",
      "Epoch 29/100\n",
      "7600/7600 [==============================] - 0s 31us/sample - loss: 0.0517 - acc: 0.9950 - val_loss: 0.2607 - val_acc: 0.9377\n",
      "Epoch 30/100\n",
      "7600/7600 [==============================] - 0s 33us/sample - loss: 0.0487 - acc: 0.9947 - val_loss: 0.2590 - val_acc: 0.9371\n",
      "Epoch 31/100\n",
      "7600/7600 [==============================] - 0s 33us/sample - loss: 0.0453 - acc: 0.9947 - val_loss: 0.2576 - val_acc: 0.9365\n",
      "Epoch 32/100\n",
      "7600/7600 [==============================] - 0s 31us/sample - loss: 0.0440 - acc: 0.9959 - val_loss: 0.2551 - val_acc: 0.9387\n",
      "Epoch 33/100\n",
      "7600/7600 [==============================] - 0s 32us/sample - loss: 0.0416 - acc: 0.9950 - val_loss: 0.2514 - val_acc: 0.9397\n",
      "Epoch 34/100\n",
      "7600/7600 [==============================] - 0s 32us/sample - loss: 0.0389 - acc: 0.9958 - val_loss: 0.2508 - val_acc: 0.9390\n",
      "Epoch 35/100\n",
      "7600/7600 [==============================] - 0s 32us/sample - loss: 0.0374 - acc: 0.9963 - val_loss: 0.2539 - val_acc: 0.9377\n",
      "Epoch 36/100\n",
      "7600/7600 [==============================] - 0s 34us/sample - loss: 0.0364 - acc: 0.9950 - val_loss: 0.2495 - val_acc: 0.9419\n",
      "Epoch 37/100\n",
      "7600/7600 [==============================] - 0s 33us/sample - loss: 0.0339 - acc: 0.9962 - val_loss: 0.2506 - val_acc: 0.9387\n",
      "Epoch 38/100\n",
      "7600/7600 [==============================] - 0s 30us/sample - loss: 0.0327 - acc: 0.9964 - val_loss: 0.2558 - val_acc: 0.9387\n",
      "Epoch 39/100\n",
      "7600/7600 [==============================] - 0s 31us/sample - loss: 0.0297 - acc: 0.9971 - val_loss: 0.2563 - val_acc: 0.9374\n",
      "Epoch 40/100\n",
      "7600/7600 [==============================] - 0s 31us/sample - loss: 0.0282 - acc: 0.9975 - val_loss: 0.2540 - val_acc: 0.9381\n",
      "Epoch 41/100\n",
      "7600/7600 [==============================] - 0s 32us/sample - loss: 0.0279 - acc: 0.9972 - val_loss: 0.2527 - val_acc: 0.9374\n",
      "Epoch 42/100\n",
      "7600/7600 [==============================] - 0s 30us/sample - loss: 0.0272 - acc: 0.9971 - val_loss: 0.2506 - val_acc: 0.9400\n",
      "Epoch 43/100\n",
      "7600/7600 [==============================] - 0s 30us/sample - loss: 0.0253 - acc: 0.9983 - val_loss: 0.2544 - val_acc: 0.9374\n",
      "Epoch 44/100\n",
      "7600/7600 [==============================] - 0s 33us/sample - loss: 0.0258 - acc: 0.9971 - val_loss: 0.2519 - val_acc: 0.9390\n",
      "Epoch 45/100\n",
      "7600/7600 [==============================] - 0s 29us/sample - loss: 0.0224 - acc: 0.9978 - val_loss: 0.2524 - val_acc: 0.9394\n",
      "Epoch 46/100\n",
      "7600/7600 [==============================] - 0s 31us/sample - loss: 0.0215 - acc: 0.9986 - val_loss: 0.2554 - val_acc: 0.9374\n",
      "Epoch 47/100\n",
      "7600/7600 [==============================] - 0s 31us/sample - loss: 0.0222 - acc: 0.9976 - val_loss: 0.2522 - val_acc: 0.9416\n",
      "Epoch 48/100\n",
      "7600/7600 [==============================] - 0s 32us/sample - loss: 0.0223 - acc: 0.9980 - val_loss: 0.2618 - val_acc: 0.9381\n",
      "Epoch 49/100\n",
      "7600/7600 [==============================] - 0s 34us/sample - loss: 0.0198 - acc: 0.9991 - val_loss: 0.2574 - val_acc: 0.9394\n",
      "Epoch 50/100\n",
      "7600/7600 [==============================] - 0s 36us/sample - loss: 0.0183 - acc: 0.9986 - val_loss: 0.2557 - val_acc: 0.9368\n",
      "Epoch 51/100\n",
      "7600/7600 [==============================] - 0s 32us/sample - loss: 0.0181 - acc: 0.9983 - val_loss: 0.2551 - val_acc: 0.9374\n",
      "Epoch 52/100\n",
      "7600/7600 [==============================] - 0s 34us/sample - loss: 0.0174 - acc: 0.9983 - val_loss: 0.2541 - val_acc: 0.9361\n",
      "Epoch 53/100\n",
      "7600/7600 [==============================] - 0s 31us/sample - loss: 0.0163 - acc: 0.9989 - val_loss: 0.2576 - val_acc: 0.9371\n",
      "Epoch 54/100\n",
      "7600/7600 [==============================] - 0s 31us/sample - loss: 0.0158 - acc: 0.9989 - val_loss: 0.2576 - val_acc: 0.9390\n",
      "Epoch 55/100\n",
      "7600/7600 [==============================] - 0s 31us/sample - loss: 0.0164 - acc: 0.9980 - val_loss: 0.2576 - val_acc: 0.9384\n",
      "Epoch 56/100\n",
      "7600/7600 [==============================] - 0s 33us/sample - loss: 0.0152 - acc: 0.9988 - val_loss: 0.2585 - val_acc: 0.9403\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ffa60d4f358>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_small.fit(df_train[emb_cols].values, \n",
    "              df_train['label'].values,\n",
    "              batch_size = 100, \n",
    "              epochs=100, \n",
    "              validation_data=(df_valid[emb_cols].values, df_valid['label'].values),\n",
    "              callbacks =[earlystopping],            \n",
    "              verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "509066ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7600 samples, validate on 3100 samples\n",
      "Epoch 1/100\n",
      "7600/7600 [==============================] - 9s 1ms/sample - loss: 0.0282 - acc: 0.9942 - val_loss: 0.4650 - val_acc: 0.9413\n",
      "Epoch 2/100\n",
      "7600/7600 [==============================] - 10s 1ms/sample - loss: 0.0025 - acc: 0.9993 - val_loss: 0.4545 - val_acc: 0.9484\n",
      "Epoch 3/100\n",
      "7600/7600 [==============================] - 10s 1ms/sample - loss: 0.0024 - acc: 0.9992 - val_loss: 0.4422 - val_acc: 0.9510\n",
      "Epoch 4/100\n",
      "7600/7600 [==============================] - 9s 1ms/sample - loss: 0.0028 - acc: 0.9989 - val_loss: 0.4682 - val_acc: 0.9471\n",
      "Epoch 5/100\n",
      "7600/7600 [==============================] - 9s 1ms/sample - loss: 8.1679e-04 - acc: 0.9997 - val_loss: 0.4791 - val_acc: 0.9435\n",
      "Epoch 6/100\n",
      "7600/7600 [==============================] - 10s 1ms/sample - loss: 0.0017 - acc: 0.9993 - val_loss: 0.4511 - val_acc: 0.9519\n",
      "Epoch 7/100\n",
      "7600/7600 [==============================] - 11s 1ms/sample - loss: 0.0057 - acc: 0.9986 - val_loss: 0.4569 - val_acc: 0.9513\n",
      "Epoch 8/100\n",
      "7600/7600 [==============================] - 10s 1ms/sample - loss: 0.0029 - acc: 0.9995 - val_loss: 0.4633 - val_acc: 0.9500\n",
      "Epoch 9/100\n",
      "7600/7600 [==============================] - 10s 1ms/sample - loss: 0.0032 - acc: 0.9993 - val_loss: 0.4548 - val_acc: 0.9529\n",
      "Epoch 10/100\n",
      "7600/7600 [==============================] - 9s 1ms/sample - loss: 0.0031 - acc: 0.9991 - val_loss: 0.4606 - val_acc: 0.9490\n",
      "Epoch 11/100\n",
      "7600/7600 [==============================] - 9s 1ms/sample - loss: 4.7250e-04 - acc: 1.0000 - val_loss: 0.4697 - val_acc: 0.9481\n",
      "Epoch 12/100\n",
      "7600/7600 [==============================] - 10s 1ms/sample - loss: 0.0025 - acc: 0.9993 - val_loss: 0.4653 - val_acc: 0.9468\n",
      "Epoch 13/100\n",
      "7600/7600 [==============================] - 10s 1ms/sample - loss: 7.2102e-04 - acc: 0.9997 - val_loss: 0.4777 - val_acc: 0.9503\n",
      "Epoch 14/100\n",
      "7600/7600 [==============================] - 10s 1ms/sample - loss: 9.6117e-04 - acc: 0.9997 - val_loss: 0.4699 - val_acc: 0.9494\n",
      "Epoch 15/100\n",
      "7600/7600 [==============================] - 10s 1ms/sample - loss: 0.0014 - acc: 0.9993 - val_loss: 0.5143 - val_acc: 0.9452\n",
      "Epoch 16/100\n",
      "7600/7600 [==============================] - 11s 1ms/sample - loss: 0.0024 - acc: 0.9993 - val_loss: 0.5097 - val_acc: 0.9484\n",
      "Epoch 17/100\n",
      "7600/7600 [==============================] - 10s 1ms/sample - loss: 0.0018 - acc: 0.9997 - val_loss: 0.5074 - val_acc: 0.9455\n",
      "Epoch 18/100\n",
      "7600/7600 [==============================] - 10s 1ms/sample - loss: 0.0031 - acc: 0.9995 - val_loss: 0.5257 - val_acc: 0.9490\n",
      "Epoch 19/100\n",
      "7600/7600 [==============================] - 11s 1ms/sample - loss: 0.0012 - acc: 0.9997 - val_loss: 0.5152 - val_acc: 0.9494\n",
      "Epoch 20/100\n",
      "7600/7600 [==============================] - 10s 1ms/sample - loss: 0.0017 - acc: 0.9996 - val_loss: 0.5146 - val_acc: 0.9519\n",
      "Epoch 21/100\n",
      "7600/7600 [==============================] - 10s 1ms/sample - loss: 8.5856e-04 - acc: 0.9996 - val_loss: 0.5060 - val_acc: 0.9529\n",
      "Epoch 22/100\n",
      "7600/7600 [==============================] - 10s 1ms/sample - loss: 0.0028 - acc: 0.9992 - val_loss: 0.5140 - val_acc: 0.9506\n",
      "Epoch 23/100\n",
      "7600/7600 [==============================] - 12s 2ms/sample - loss: 0.0012 - acc: 0.9997 - val_loss: 0.5120 - val_acc: 0.9516\n",
      "Epoch 24/100\n",
      "7600/7600 [==============================] - 12s 2ms/sample - loss: 0.0013 - acc: 0.9997 - val_loss: 0.5115 - val_acc: 0.9490\n",
      "Epoch 25/100\n",
      "7600/7600 [==============================] - 10s 1ms/sample - loss: 0.0012 - acc: 0.9996 - val_loss: 0.5416 - val_acc: 0.9439\n",
      "Epoch 26/100\n",
      "7600/7600 [==============================] - 9s 1ms/sample - loss: 0.0017 - acc: 0.9997 - val_loss: 0.5368 - val_acc: 0.9516\n",
      "Epoch 27/100\n",
      "7600/7600 [==============================] - 9s 1ms/sample - loss: 0.0038 - acc: 0.9989 - val_loss: 0.5223 - val_acc: 0.9510\n",
      "Epoch 28/100\n",
      "7600/7600 [==============================] - 9s 1ms/sample - loss: 0.0024 - acc: 0.9995 - val_loss: 0.5357 - val_acc: 0.9500\n",
      "Epoch 29/100\n",
      "7600/7600 [==============================] - 9s 1ms/sample - loss: 5.1944e-04 - acc: 0.9997 - val_loss: 0.5176 - val_acc: 0.9532\n",
      "Epoch 30/100\n",
      "7600/7600 [==============================] - 9s 1ms/sample - loss: 0.0055 - acc: 0.9995 - val_loss: 0.5359 - val_acc: 0.9487\n",
      "Epoch 31/100\n",
      "7600/7600 [==============================] - 9s 1ms/sample - loss: 0.0012 - acc: 0.9996 - val_loss: 0.5481 - val_acc: 0.9494\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb5d43a2668>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_small.fit(df_train[emb_cols].values, \n",
    "              df_train['label'].values,\n",
    "              batch_size = 1, \n",
    "              epochs=100, \n",
    "              validation_data=(df_valid[emb_cols].values, df_valid['label'].values),\n",
    "              callbacks =[earlystopping],            \n",
    "              verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76c86bf",
   "metadata": {},
   "source": [
    "## Inscope Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f8c6169b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8214    0.7667    0.7931        30\n",
      "           1     0.8571    1.0000    0.9231        30\n",
      "           2     0.8929    0.8333    0.8621        30\n",
      "           3     1.0000    1.0000    1.0000        30\n",
      "           4     1.0000    0.9667    0.9831        30\n",
      "           5     1.0000    0.9000    0.9474        30\n",
      "           6     1.0000    1.0000    1.0000        30\n",
      "           7     1.0000    0.9667    0.9831        30\n",
      "           8     1.0000    1.0000    1.0000        30\n",
      "           9     0.8571    1.0000    0.9231        30\n",
      "          10     0.9355    0.9667    0.9508        30\n",
      "          11     0.8966    0.8667    0.8814        30\n",
      "          12     0.9667    0.9667    0.9667        30\n",
      "          13     1.0000    1.0000    1.0000        30\n",
      "          14     0.8529    0.9667    0.9062        30\n",
      "          15     0.8519    0.7667    0.8070        30\n",
      "          16     0.9310    0.9000    0.9153        30\n",
      "          17     1.0000    0.9667    0.9831        30\n",
      "          18     0.9062    0.9667    0.9355        30\n",
      "          19     0.9062    0.9667    0.9355        30\n",
      "          20     0.9667    0.9667    0.9667        30\n",
      "          21     1.0000    1.0000    1.0000        30\n",
      "          22     0.9677    1.0000    0.9836        30\n",
      "          23     1.0000    1.0000    1.0000        30\n",
      "          24     0.9333    0.9333    0.9333        30\n",
      "          25     1.0000    0.9000    0.9474        30\n",
      "          26     0.9062    0.9667    0.9355        30\n",
      "          27     0.9375    1.0000    0.9677        30\n",
      "          28     1.0000    1.0000    1.0000        30\n",
      "          29     1.0000    1.0000    1.0000        30\n",
      "          30     1.0000    0.8333    0.9091        30\n",
      "          31     0.9667    0.9667    0.9667        30\n",
      "          32     0.9583    0.7667    0.8519        30\n",
      "          33     0.8750    0.9333    0.9032        30\n",
      "          34     0.9583    0.7667    0.8519        30\n",
      "          35     1.0000    0.9000    0.9474        30\n",
      "          36     0.8529    0.9667    0.9062        30\n",
      "          37     0.7931    0.7667    0.7797        30\n",
      "          38     0.9091    1.0000    0.9524        30\n",
      "          39     1.0000    0.9667    0.9831        30\n",
      "          40     1.0000    0.8667    0.9286        30\n",
      "          41     1.0000    1.0000    1.0000        30\n",
      "          42     0.9375    1.0000    0.9677        30\n",
      "          43     1.0000    0.9667    0.9831        30\n",
      "          44     1.0000    1.0000    1.0000        30\n",
      "          45     1.0000    0.9667    0.9831        30\n",
      "          46     0.9643    0.9000    0.9310        30\n",
      "          47     0.9000    0.9000    0.9000        30\n",
      "          48     0.9677    1.0000    0.9836        30\n",
      "          49     0.9677    1.0000    0.9836        30\n",
      "          50     0.8182    0.9000    0.8571        30\n",
      "          51     1.0000    0.9333    0.9655        30\n",
      "          52     1.0000    0.9000    0.9474        30\n",
      "          53     0.8621    0.8333    0.8475        30\n",
      "          54     0.9310    0.9000    0.9153        30\n",
      "          55     0.9677    1.0000    0.9836        30\n",
      "          56     0.9333    0.9333    0.9333        30\n",
      "          57     1.0000    0.9667    0.9831        30\n",
      "          58     0.8824    1.0000    0.9375        30\n",
      "          59     1.0000    0.9000    0.9474        30\n",
      "          60     0.9583    0.7667    0.8519        30\n",
      "          61     0.9643    0.9000    0.9310        30\n",
      "          62     1.0000    1.0000    1.0000        30\n",
      "          63     1.0000    1.0000    1.0000        30\n",
      "          64     0.9355    0.9667    0.9508        30\n",
      "          65     1.0000    0.9667    0.9831        30\n",
      "          66     1.0000    0.9667    0.9831        30\n",
      "          67     0.8750    0.9333    0.9032        30\n",
      "          68     0.7647    0.8667    0.8125        30\n",
      "          69     0.9667    0.9667    0.9667        30\n",
      "          70     0.9333    0.9333    0.9333        30\n",
      "          71     0.9375    1.0000    0.9677        30\n",
      "          72     0.7714    0.9000    0.8308        30\n",
      "          73     0.9000    0.9000    0.9000        30\n",
      "          74     0.9375    1.0000    0.9677        30\n",
      "          75     1.0000    1.0000    1.0000        30\n",
      "          76     0.9677    1.0000    0.9836        30\n",
      "          77     0.9615    0.8333    0.8929        30\n",
      "          78     1.0000    1.0000    1.0000        30\n",
      "          79     1.0000    0.9667    0.9831        30\n",
      "          80     1.0000    1.0000    1.0000        30\n",
      "          81     1.0000    0.8667    0.9286        30\n",
      "          82     1.0000    1.0000    1.0000        30\n",
      "          83     0.9655    0.9333    0.9492        30\n",
      "          84     0.9355    0.9667    0.9508        30\n",
      "          85     0.9677    1.0000    0.9836        30\n",
      "          86     0.9643    0.9000    0.9310        30\n",
      "          87     1.0000    1.0000    1.0000        30\n",
      "          88     1.0000    1.0000    1.0000        30\n",
      "          89     0.9355    0.9667    0.9508        30\n",
      "          90     1.0000    1.0000    1.0000        30\n",
      "          91     1.0000    0.8000    0.8889        30\n",
      "          92     1.0000    0.9667    0.9831        30\n",
      "          93     1.0000    1.0000    1.0000        30\n",
      "          94     0.9091    1.0000    0.9524        30\n",
      "          95     0.9667    0.9667    0.9667        30\n",
      "          96     1.0000    1.0000    1.0000        30\n",
      "          97     0.9333    0.9333    0.9333        30\n",
      "          98     0.8108    1.0000    0.8955        30\n",
      "          99     1.0000    0.7333    0.8462        30\n",
      "         100     1.0000    0.9333    0.9655        30\n",
      "         101     1.0000    1.0000    1.0000        30\n",
      "         102     0.9259    0.8333    0.8772        30\n",
      "         103     0.8286    0.9667    0.8923        30\n",
      "         104     0.9259    0.8333    0.8772        30\n",
      "         105     0.7222    0.8667    0.7879        30\n",
      "         106     0.8286    0.9667    0.8923        30\n",
      "         107     1.0000    0.9667    0.9831        30\n",
      "         108     0.9200    0.7667    0.8364        30\n",
      "         109     0.9231    0.8000    0.8571        30\n",
      "         110     0.9677    1.0000    0.9836        30\n",
      "         111     0.8929    0.8333    0.8621        30\n",
      "         112     1.0000    0.7333    0.8462        30\n",
      "         113     0.9615    0.8333    0.8929        30\n",
      "         114     1.0000    1.0000    1.0000        30\n",
      "         115     0.8235    0.9333    0.8750        30\n",
      "         116     0.8667    0.8667    0.8667        30\n",
      "         117     0.9677    1.0000    0.9836        30\n",
      "         118     1.0000    0.9667    0.9831        30\n",
      "         119     0.8824    1.0000    0.9375        30\n",
      "         120     0.8182    0.9000    0.8571        30\n",
      "         121     0.9375    1.0000    0.9677        30\n",
      "         122     1.0000    0.9667    0.9831        30\n",
      "         123     0.7692    1.0000    0.8696        30\n",
      "         124     0.9000    0.9000    0.9000        30\n",
      "         125     0.9375    1.0000    0.9677        30\n",
      "         126     1.0000    0.9667    0.9831        30\n",
      "         127     0.8750    0.9333    0.9032        30\n",
      "         128     1.0000    0.9667    0.9831        30\n",
      "         129     1.0000    0.9667    0.9831        30\n",
      "         130     0.9655    0.9333    0.9492        30\n",
      "         131     1.0000    1.0000    1.0000        30\n",
      "         132     0.9630    0.8667    0.9123        30\n",
      "         133     0.9032    0.9333    0.9180        30\n",
      "         134     0.8824    1.0000    0.9375        30\n",
      "         135     0.9062    0.9667    0.9355        30\n",
      "         136     0.9355    0.9667    0.9508        30\n",
      "         137     0.9259    0.8333    0.8772        30\n",
      "         138     0.8485    0.9333    0.8889        30\n",
      "         139     0.9375    1.0000    0.9677        30\n",
      "         140     1.0000    1.0000    1.0000        30\n",
      "         141     0.8485    0.9333    0.8889        30\n",
      "         142     1.0000    0.9000    0.9474        30\n",
      "         143     0.9355    0.9667    0.9508        30\n",
      "         144     0.9630    0.8667    0.9123        30\n",
      "         145     0.9310    0.9000    0.9153        30\n",
      "         146     0.9375    1.0000    0.9677        30\n",
      "         147     1.0000    1.0000    1.0000        30\n",
      "         148     1.0000    0.9667    0.9831        30\n",
      "         149     0.9259    0.8333    0.8772        30\n",
      "         150     0.0000    0.0000    0.0000         0\n",
      "\n",
      "    accuracy                         0.9378      4500\n",
      "   macro avg     0.9376    0.9316    0.9327      4500\n",
      "weighted avg     0.9439    0.9378    0.9389      4500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tayfun.tuna/opt/anaconda3/envs/astra/lib/python3.6/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "X,y = df_test_inscope[emb_cols].values, df_test_inscope['label'].values\n",
    "pred_probs = mlp_small.predict(X)\n",
    "preds = [np. argmax(p) for p in pred_probs]\n",
    "print(classification_report(y, preds,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7820421e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8214    0.7667    0.7931        30\n",
      "           1     0.7692    1.0000    0.8696        30\n",
      "           2     0.6098    0.8333    0.7042        30\n",
      "           3     0.9677    1.0000    0.9836        30\n",
      "           4     0.9355    0.9667    0.9508        30\n",
      "           5     1.0000    0.9000    0.9474        30\n",
      "           6     0.9375    1.0000    0.9677        30\n",
      "           7     0.7632    0.9667    0.8529        30\n",
      "           8     1.0000    1.0000    1.0000        30\n",
      "           9     0.7317    1.0000    0.8451        30\n",
      "          10     0.9062    0.9667    0.9355        30\n",
      "          11     0.8966    0.8667    0.8814        30\n",
      "          12     0.9667    0.9667    0.9667        30\n",
      "          13     0.8333    1.0000    0.9091        30\n",
      "          14     0.7838    0.9667    0.8657        30\n",
      "          15     0.8519    0.7667    0.8070        30\n",
      "          16     0.8438    0.9000    0.8710        30\n",
      "          17     0.8788    0.9667    0.9206        30\n",
      "          18     0.8286    0.9667    0.8923        30\n",
      "          19     0.8529    0.9667    0.9062        30\n",
      "          20     0.8529    0.9667    0.9062        30\n",
      "          21     0.7500    1.0000    0.8571        30\n",
      "          22     0.9677    1.0000    0.9836        30\n",
      "          23     1.0000    1.0000    1.0000        30\n",
      "          24     0.9032    0.9333    0.9180        30\n",
      "          25     0.9310    0.9000    0.9153        30\n",
      "          26     0.8056    0.9667    0.8788        30\n",
      "          27     0.8571    1.0000    0.9231        30\n",
      "          28     0.9677    1.0000    0.9836        30\n",
      "          29     0.9375    1.0000    0.9677        30\n",
      "          30     0.7576    0.8333    0.7937        30\n",
      "          31     0.9062    0.9667    0.9355        30\n",
      "          32     0.7419    0.7667    0.7541        30\n",
      "          33     0.8235    0.9333    0.8750        30\n",
      "          34     0.9200    0.7667    0.8364        30\n",
      "          35     1.0000    0.9000    0.9474        30\n",
      "          36     0.8529    0.9667    0.9062        30\n",
      "          37     0.4792    0.7667    0.5897        30\n",
      "          38     0.8824    1.0000    0.9375        30\n",
      "          39     0.9667    0.9667    0.9667        30\n",
      "          40     0.7429    0.8667    0.8000        30\n",
      "          41     0.9091    1.0000    0.9524        30\n",
      "          42     0.8108    1.0000    0.8955        30\n",
      "          43     0.8788    0.9667    0.9206        30\n",
      "          44     0.9375    1.0000    0.9677        30\n",
      "          45     0.9667    0.9667    0.9667        30\n",
      "          46     0.9310    0.9000    0.9153        30\n",
      "          47     0.6923    0.9000    0.7826        30\n",
      "          48     0.9375    1.0000    0.9677        30\n",
      "          49     0.7692    1.0000    0.8696        30\n",
      "          50     0.5510    0.9000    0.6835        30\n",
      "          51     0.9655    0.9333    0.9492        30\n",
      "          52     0.7714    0.9000    0.8308        30\n",
      "          53     0.8065    0.8333    0.8197        30\n",
      "          54     0.8710    0.9000    0.8852        30\n",
      "          55     0.8824    1.0000    0.9375        30\n",
      "          56     0.6667    0.9333    0.7778        30\n",
      "          57     1.0000    0.9667    0.9831        30\n",
      "          58     0.7500    1.0000    0.8571        30\n",
      "          59     0.6923    0.9000    0.7826        30\n",
      "          60     0.7667    0.7667    0.7667        30\n",
      "          61     0.9643    0.9000    0.9310        30\n",
      "          62     0.8824    1.0000    0.9375        30\n",
      "          63     0.9677    1.0000    0.9836        30\n",
      "          64     0.9355    0.9667    0.9508        30\n",
      "          65     0.8056    0.9667    0.8788        30\n",
      "          66     1.0000    0.9667    0.9831        30\n",
      "          67     0.8000    0.9333    0.8615        30\n",
      "          68     0.7429    0.8667    0.8000        30\n",
      "          69     0.4754    0.9667    0.6374        30\n",
      "          70     0.9032    0.9333    0.9180        30\n",
      "          71     0.8824    1.0000    0.9375        30\n",
      "          72     0.7500    0.9000    0.8182        30\n",
      "          73     0.8438    0.9000    0.8710        30\n",
      "          74     0.8824    1.0000    0.9375        30\n",
      "          75     0.9375    1.0000    0.9677        30\n",
      "          76     0.9375    1.0000    0.9677        30\n",
      "          77     0.9259    0.8333    0.8772        30\n",
      "          78     0.9091    1.0000    0.9524        30\n",
      "          79     0.9062    0.9667    0.9355        30\n",
      "          80     1.0000    1.0000    1.0000        30\n",
      "          81     0.9286    0.8667    0.8966        30\n",
      "          82     0.9091    1.0000    0.9524        30\n",
      "          83     0.9032    0.9333    0.9180        30\n",
      "          84     0.8529    0.9667    0.9062        30\n",
      "          85     0.9677    1.0000    0.9836        30\n",
      "          86     0.9310    0.9000    0.9153        30\n",
      "          87     0.8824    1.0000    0.9375        30\n",
      "          88     0.9677    1.0000    0.9836        30\n",
      "          89     0.9355    0.9667    0.9508        30\n",
      "          90     0.9375    1.0000    0.9677        30\n",
      "          91     0.7273    0.8000    0.7619        30\n",
      "          92     0.9667    0.9667    0.9667        30\n",
      "          93     0.8824    1.0000    0.9375        30\n",
      "          94     0.7895    1.0000    0.8824        30\n",
      "          95     0.8056    0.9667    0.8788        30\n",
      "          96     0.9091    1.0000    0.9524        30\n",
      "          97     0.7368    0.9333    0.8235        30\n",
      "          98     0.7143    1.0000    0.8333        30\n",
      "          99     0.8800    0.7333    0.8000        30\n",
      "         100     0.9655    0.9333    0.9492        30\n",
      "         101     1.0000    1.0000    1.0000        30\n",
      "         102     0.8621    0.8333    0.8475        30\n",
      "         103     0.5800    0.9667    0.7250        30\n",
      "         104     0.9259    0.8333    0.8772        30\n",
      "         105     0.7027    0.8667    0.7761        30\n",
      "         106     0.6905    0.9667    0.8056        30\n",
      "         107     0.9667    0.9667    0.9667        30\n",
      "         108     0.8846    0.7667    0.8214        30\n",
      "         109     0.8000    0.8000    0.8000        30\n",
      "         110     0.9375    1.0000    0.9677        30\n",
      "         111     0.8621    0.8333    0.8475        30\n",
      "         112     0.9565    0.7333    0.8302        30\n",
      "         113     0.9615    0.8333    0.8929        30\n",
      "         114     1.0000    1.0000    1.0000        30\n",
      "         115     0.8000    0.9333    0.8615        30\n",
      "         116     0.7027    0.8667    0.7761        30\n",
      "         117     0.9091    1.0000    0.9524        30\n",
      "         118     0.9667    0.9667    0.9667        30\n",
      "         119     0.7895    1.0000    0.8824        30\n",
      "         120     0.8182    0.9000    0.8571        30\n",
      "         121     0.8824    1.0000    0.9375        30\n",
      "         122     0.9062    0.9667    0.9355        30\n",
      "         123     0.7692    1.0000    0.8696        30\n",
      "         124     0.8182    0.9000    0.8571        30\n",
      "         125     0.7317    1.0000    0.8451        30\n",
      "         126     0.9667    0.9667    0.9667        30\n",
      "         127     0.8000    0.9333    0.8615        30\n",
      "         128     0.9355    0.9667    0.9508        30\n",
      "         129     0.9667    0.9667    0.9667        30\n",
      "         130     0.9333    0.9333    0.9333        30\n",
      "         131     1.0000    1.0000    1.0000        30\n",
      "         132     0.9286    0.8667    0.8966        30\n",
      "         133     0.6512    0.9333    0.7671        30\n",
      "         134     0.5000    1.0000    0.6667        30\n",
      "         135     0.8529    0.9667    0.9062        30\n",
      "         136     0.8788    0.9667    0.9206        30\n",
      "         137     0.8333    0.8333    0.8333        30\n",
      "         138     0.5957    0.9333    0.7273        30\n",
      "         139     0.8333    1.0000    0.9091        30\n",
      "         140     0.7143    1.0000    0.8333        30\n",
      "         141     0.8235    0.9333    0.8750        30\n",
      "         142     0.8182    0.9000    0.8571        30\n",
      "         143     0.8788    0.9667    0.9206        30\n",
      "         144     0.9630    0.8667    0.9123        30\n",
      "         145     0.9310    0.9000    0.9153        30\n",
      "         146     0.8824    1.0000    0.9375        30\n",
      "         147     0.9091    1.0000    0.9524        30\n",
      "         148     0.9667    0.9667    0.9667        30\n",
      "         149     0.8065    0.8333    0.8197        30\n",
      "         150     0.9728    0.4650    0.6292      1000\n",
      "\n",
      "    accuracy                         0.8518      5500\n",
      "   macro avg     0.8569    0.9346    0.8889      5500\n",
      "weighted avg     0.8773    0.8518    0.8431      5500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X,y = pd.concat([df_test_inscope,df_test_oos])[emb_cols].values, pd.concat([df_test_inscope,df_test_oos])['label'].values\n",
    "pred_probs = mlp_small.predict(X)\n",
    "preds = [np. argmax(p) for p in pred_probs]\n",
    "print(classification_report(y, preds,digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c0034f",
   "metadata": {},
   "source": [
    "## OutofScope Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b123dde5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.0000    0.0000    0.0000         0\n",
      "           2     0.0000    0.0000    0.0000         0\n",
      "           3     0.0000    0.0000    0.0000         0\n",
      "           5     0.0000    0.0000    0.0000         0\n",
      "           6     0.0000    0.0000    0.0000         0\n",
      "           7     0.0000    0.0000    0.0000         0\n",
      "           9     0.0000    0.0000    0.0000         0\n",
      "          10     0.0000    0.0000    0.0000         0\n",
      "          11     0.0000    0.0000    0.0000         0\n",
      "          14     0.0000    0.0000    0.0000         0\n",
      "          16     0.0000    0.0000    0.0000         0\n",
      "          17     0.0000    0.0000    0.0000         0\n",
      "          18     0.0000    0.0000    0.0000         0\n",
      "          20     0.0000    0.0000    0.0000         0\n",
      "          22     0.0000    0.0000    0.0000         0\n",
      "          23     0.0000    0.0000    0.0000         0\n",
      "          25     0.0000    0.0000    0.0000         0\n",
      "          26     0.0000    0.0000    0.0000         0\n",
      "          27     0.0000    0.0000    0.0000         0\n",
      "          28     0.0000    0.0000    0.0000         0\n",
      "          32     0.0000    0.0000    0.0000         0\n",
      "          33     0.0000    0.0000    0.0000         0\n",
      "          34     0.0000    0.0000    0.0000         0\n",
      "          35     0.0000    0.0000    0.0000         0\n",
      "          36     0.0000    0.0000    0.0000         0\n",
      "          38     0.0000    0.0000    0.0000         0\n",
      "          39     0.0000    0.0000    0.0000         0\n",
      "          40     0.0000    0.0000    0.0000         0\n",
      "          41     0.0000    0.0000    0.0000         0\n",
      "          42     0.0000    0.0000    0.0000         0\n",
      "          43     0.0000    0.0000    0.0000         0\n",
      "          44     0.0000    0.0000    0.0000         0\n",
      "          45     0.0000    0.0000    0.0000         0\n",
      "          46     0.0000    0.0000    0.0000         0\n",
      "          47     0.0000    0.0000    0.0000         0\n",
      "          49     0.0000    0.0000    0.0000         0\n",
      "          50     0.0000    0.0000    0.0000         0\n",
      "          51     0.0000    0.0000    0.0000         0\n",
      "          52     0.0000    0.0000    0.0000         0\n",
      "          53     0.0000    0.0000    0.0000         0\n",
      "          55     0.0000    0.0000    0.0000         0\n",
      "          56     0.0000    0.0000    0.0000         0\n",
      "          57     0.0000    0.0000    0.0000         0\n",
      "          58     0.0000    0.0000    0.0000         0\n",
      "          59     0.0000    0.0000    0.0000         0\n",
      "          60     0.0000    0.0000    0.0000         0\n",
      "          61     0.0000    0.0000    0.0000         0\n",
      "          62     0.0000    0.0000    0.0000         0\n",
      "          63     0.0000    0.0000    0.0000         0\n",
      "          64     0.0000    0.0000    0.0000         0\n",
      "          65     0.0000    0.0000    0.0000         0\n",
      "          67     0.0000    0.0000    0.0000         0\n",
      "          69     0.0000    0.0000    0.0000         0\n",
      "          70     0.0000    0.0000    0.0000         0\n",
      "          73     0.0000    0.0000    0.0000         0\n",
      "          75     0.0000    0.0000    0.0000         0\n",
      "          76     0.0000    0.0000    0.0000         0\n",
      "          77     0.0000    0.0000    0.0000         0\n",
      "          79     0.0000    0.0000    0.0000         0\n",
      "          80     0.0000    0.0000    0.0000         0\n",
      "          81     0.0000    0.0000    0.0000         0\n",
      "          82     0.0000    0.0000    0.0000         0\n",
      "          83     0.0000    0.0000    0.0000         0\n",
      "          84     0.0000    0.0000    0.0000         0\n",
      "          86     0.0000    0.0000    0.0000         0\n",
      "          87     0.0000    0.0000    0.0000         0\n",
      "          88     0.0000    0.0000    0.0000         0\n",
      "          89     0.0000    0.0000    0.0000         0\n",
      "          90     0.0000    0.0000    0.0000         0\n",
      "          91     0.0000    0.0000    0.0000         0\n",
      "          92     0.0000    0.0000    0.0000         0\n",
      "          93     0.0000    0.0000    0.0000         0\n",
      "          94     0.0000    0.0000    0.0000         0\n",
      "          96     0.0000    0.0000    0.0000         0\n",
      "          97     0.0000    0.0000    0.0000         0\n",
      "          98     0.0000    0.0000    0.0000         0\n",
      "          99     0.0000    0.0000    0.0000         0\n",
      "         100     0.0000    0.0000    0.0000         0\n",
      "         103     0.0000    0.0000    0.0000         0\n",
      "         104     0.0000    0.0000    0.0000         0\n",
      "         106     0.0000    0.0000    0.0000         0\n",
      "         107     0.0000    0.0000    0.0000         0\n",
      "         111     0.0000    0.0000    0.0000         0\n",
      "         114     0.0000    0.0000    0.0000         0\n",
      "         116     0.0000    0.0000    0.0000         0\n",
      "         119     0.0000    0.0000    0.0000         0\n",
      "         120     0.0000    0.0000    0.0000         0\n",
      "         121     0.0000    0.0000    0.0000         0\n",
      "         122     0.0000    0.0000    0.0000         0\n",
      "         123     0.0000    0.0000    0.0000         0\n",
      "         124     0.0000    0.0000    0.0000         0\n",
      "         125     0.0000    0.0000    0.0000         0\n",
      "         126     0.0000    0.0000    0.0000         0\n",
      "         127     0.0000    0.0000    0.0000         0\n",
      "         128     0.0000    0.0000    0.0000         0\n",
      "         129     0.0000    0.0000    0.0000         0\n",
      "         132     0.0000    0.0000    0.0000         0\n",
      "         133     0.0000    0.0000    0.0000         0\n",
      "         135     0.0000    0.0000    0.0000         0\n",
      "         136     0.0000    0.0000    0.0000         0\n",
      "         137     0.0000    0.0000    0.0000         0\n",
      "         138     0.0000    0.0000    0.0000         0\n",
      "         140     0.0000    0.0000    0.0000         0\n",
      "         141     0.0000    0.0000    0.0000         0\n",
      "         142     0.0000    0.0000    0.0000         0\n",
      "         143     0.0000    0.0000    0.0000         0\n",
      "         145     0.0000    0.0000    0.0000         0\n",
      "         146     0.0000    0.0000    0.0000         0\n",
      "         148     0.0000    0.0000    0.0000         0\n",
      "         149     0.0000    0.0000    0.0000         0\n",
      "         150     1.0000    0.6410    0.7812      1000\n",
      "\n",
      "    accuracy                         0.6410      1000\n",
      "   macro avg     0.0090    0.0058    0.0070      1000\n",
      "weighted avg     1.0000    0.6410    0.7812      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X,y = df_test_oos[emb_cols].values, df_test_oos['label'].values\n",
    "pred_probs = mlp_small.predict(X)\n",
    "preds = [np. argmax(p) for p in pred_probs]\n",
    "print(classification_report(y, preds,digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e914d25e",
   "metadata": {},
   "source": [
    "# Automate All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "e3e4c8d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/clinic/data_full/',\n",
       " '../data/clinic/data_imbalanced/',\n",
       " '../data/clinic/data_oos_plus/',\n",
       " '../data/clinic/data_small/']"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirs = sorted(glob(DATA_FOLDER +\"*/\"))\n",
    "dirs\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2775b07b",
   "metadata": {},
   "source": [
    "# Parameters:\n",
    "#https://github.com/clinc/oos-eval/blob/master/hyperparameters.csv\n",
    "\n",
    "Full\tclassifier\tf_hidden_activation\ts_hidden_activation\thidden_dim\tvectorizer\tbatch_size\tdropout\t\t\t\n",
    "           mlp\t       tanh\t                  softmax\t       400\t       use\t        1\t      0\t\n",
    "\n",
    "Imbalanced\tclassifier\tf_hidden_activation\ts_hidden_activation\thidden_dim\tvectorizer\tbatch_size\tdropout\t\t\t\n",
    "           mlp\t       tanh\t                  softmax\t       200\t       use\t        64      \t0\n",
    "            \n",
    "OOS+\tclassifier\tf_hidden_activation\ts_hidden_activation\thidden_dim\tvectorizer\tbatch_size\tdropout\t\t\t\n",
    "           mlp\t       tanh\t                  softmax\t       200\t       use\t        16      \t0.1\t\n",
    "\n",
    "Small\tclassifier\tf_hidden_activation\ts_hidden_activation\thidden_dim\tvectorizer\tbatch_size\tdropout\t\t\t\n",
    "           mlp\t       tanh\t                  softmax\t       200\t       use\t        1\t      0.1\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "59908529",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [mlp_full, mlp_imbalanced, mlp_oss_plus, mlp_small]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "842deed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes =[1, 64,16,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "37dced4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_clf_report(model, dt):\n",
    "    X,y = dt[emb_cols].values, dt['label'].values\n",
    "    pred_probs = model.predict(X)\n",
    "    preds = [np. argmax(p) for p in pred_probs]\n",
    "    print(classification_report(y, preds,digits=4))\n",
    "\n",
    "def train_evaulate_model(directory, model, batch_size=1): \n",
    "    print(directory)\n",
    "    df_train = pd.concat([ pd.read_csv(directory + 'train_with_use_emb.csv'),\n",
    "                           pd.read_csv(directory + 'oos_train_with_use_emb.csv')]\n",
    "                    ).reset_index()\n",
    "    \n",
    "    df_valid = pd.concat([ pd.read_csv(directory + 'val_with_use_emb.csv'),\n",
    "                           pd.read_csv(directory + 'oos_val_with_use_emb.csv')]\n",
    "                    ).reset_index()\n",
    "    df_test_inscope = pd.read_csv(directory + 'test_with_use_emb.csv')\n",
    "    df_test_oos = pd.read_csv(directory + 'oos_test_with_use_emb.csv')\n",
    "    \n",
    "    model.fit(df_train[emb_cols].values, \n",
    "              df_train['label'].values,\n",
    "              batch_size = batch_size, \n",
    "              epochs=100, \n",
    "              validation_data=(df_valid[emb_cols].values, df_valid['label'].values),\n",
    "              callbacks =[earlystopping],            \n",
    "              verbose=0)\n",
    "   \n",
    "    print('Inscope')\n",
    "    print_clf_report(model, df_test_inscope)\n",
    "    \n",
    "    print('Out of Scope')\n",
    "    print_clf_report(model, df_test_oos)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c89b40f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/clinic/data_full/\n",
      "Inscope\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9667    0.9831        30\n",
      "           1     1.0000    0.9000    0.9474        30\n",
      "           2     1.0000    0.9000    0.9474        30\n",
      "           3     0.9667    0.9667    0.9667        30\n",
      "           4     1.0000    1.0000    1.0000        30\n",
      "           5     0.9667    0.9667    0.9667        30\n",
      "           6     1.0000    0.9333    0.9655        30\n",
      "           7     1.0000    1.0000    1.0000        30\n",
      "           8     1.0000    0.9000    0.9474        30\n",
      "           9     0.9091    1.0000    0.9524        30\n",
      "          10     0.9000    0.9000    0.9000        30\n",
      "          11     0.8788    0.9667    0.9206        30\n",
      "          12     1.0000    0.9667    0.9831        30\n",
      "          13     1.0000    1.0000    1.0000        30\n",
      "          14     0.8333    1.0000    0.9091        30\n",
      "          15     1.0000    1.0000    1.0000        30\n",
      "          16     0.9677    1.0000    0.9836        30\n",
      "          17     1.0000    0.9000    0.9474        30\n",
      "          18     0.8438    0.9000    0.8710        30\n",
      "          19     1.0000    1.0000    1.0000        30\n",
      "          20     0.9643    0.9000    0.9310        30\n",
      "          21     1.0000    1.0000    1.0000        30\n",
      "          22     0.9000    0.9000    0.9000        30\n",
      "          23     1.0000    0.9667    0.9831        30\n",
      "          24     0.8056    0.9667    0.8788        30\n",
      "          25     0.9677    1.0000    0.9836        30\n",
      "          26     0.9286    0.8667    0.8966        30\n",
      "          27     1.0000    1.0000    1.0000        30\n",
      "          28     0.9677    1.0000    0.9836        30\n",
      "          29     1.0000    1.0000    1.0000        30\n",
      "          30     0.9375    1.0000    0.9677        30\n",
      "          31     0.9677    1.0000    0.9836        30\n",
      "          32     1.0000    0.9667    0.9831        30\n",
      "          33     1.0000    1.0000    1.0000        30\n",
      "          34     1.0000    1.0000    1.0000        30\n",
      "          35     0.9333    0.9333    0.9333        30\n",
      "          36     1.0000    0.9667    0.9831        30\n",
      "          37     1.0000    1.0000    1.0000        30\n",
      "          38     0.9000    0.9000    0.9000        30\n",
      "          39     1.0000    1.0000    1.0000        30\n",
      "          40     0.9286    0.8667    0.8966        30\n",
      "          41     1.0000    1.0000    1.0000        30\n",
      "          42     0.9375    1.0000    0.9677        30\n",
      "          43     1.0000    1.0000    1.0000        30\n",
      "          44     0.9677    1.0000    0.9836        30\n",
      "          45     1.0000    0.9000    0.9474        30\n",
      "          46     1.0000    1.0000    1.0000        30\n",
      "          47     1.0000    0.9333    0.9655        30\n",
      "          48     1.0000    1.0000    1.0000        30\n",
      "          49     0.9091    1.0000    0.9524        30\n",
      "          50     0.8485    0.9333    0.8889        30\n",
      "          51     1.0000    1.0000    1.0000        30\n",
      "          52     0.9677    1.0000    0.9836        30\n",
      "          53     1.0000    0.9667    0.9831        30\n",
      "          54     0.9677    1.0000    0.9836        30\n",
      "          55     0.9355    0.9667    0.9508        30\n",
      "          56     0.9375    1.0000    0.9677        30\n",
      "          57     0.9355    0.9667    0.9508        30\n",
      "          58     0.9667    0.9667    0.9667        30\n",
      "          59     0.9630    0.8667    0.9123        30\n",
      "          60     1.0000    0.8667    0.9286        30\n",
      "          61     0.8571    1.0000    0.9231        30\n",
      "          62     0.9643    0.9000    0.9310        30\n",
      "          63     0.9375    1.0000    0.9677        30\n",
      "          64     1.0000    1.0000    1.0000        30\n",
      "          65     0.8519    0.7667    0.8070        30\n",
      "          66     0.9655    0.9333    0.9492        30\n",
      "          67     0.9286    0.8667    0.8966        30\n",
      "          68     1.0000    0.9667    0.9831        30\n",
      "          69     0.9032    0.9333    0.9180        30\n",
      "          70     0.8214    0.7667    0.7931        30\n",
      "          71     1.0000    1.0000    1.0000        30\n",
      "          72     0.9259    0.8333    0.8772        30\n",
      "          73     0.9333    0.9333    0.9333        30\n",
      "          74     0.9091    1.0000    0.9524        30\n",
      "          75     0.9032    0.9333    0.9180        30\n",
      "          76     0.8529    0.9667    0.9062        30\n",
      "          77     1.0000    1.0000    1.0000        30\n",
      "          78     0.9667    0.9667    0.9667        30\n",
      "          79     1.0000    0.9667    0.9831        30\n",
      "          80     0.9677    1.0000    0.9836        30\n",
      "          81     1.0000    0.8667    0.9286        30\n",
      "          82     1.0000    1.0000    1.0000        30\n",
      "          83     0.9091    1.0000    0.9524        30\n",
      "          84     1.0000    1.0000    1.0000        30\n",
      "          85     0.8333    0.8333    0.8333        30\n",
      "          86     0.9643    0.9000    0.9310        30\n",
      "          87     1.0000    0.9667    0.9831        30\n",
      "          88     0.9062    0.9667    0.9355        30\n",
      "          89     0.9375    1.0000    0.9677        30\n",
      "          90     1.0000    1.0000    1.0000        30\n",
      "          91     0.9286    0.8667    0.8966        30\n",
      "          92     0.9677    1.0000    0.9836        30\n",
      "          93     1.0000    0.9667    0.9831        30\n",
      "          94     1.0000    1.0000    1.0000        30\n",
      "          95     0.9062    0.9667    0.9355        30\n",
      "          96     1.0000    0.8667    0.9286        30\n",
      "          97     0.8485    0.9333    0.8889        30\n",
      "          98     0.8824    1.0000    0.9375        30\n",
      "          99     0.9310    0.9000    0.9153        30\n",
      "         100     0.8529    0.9667    0.9062        30\n",
      "         101     0.8929    0.8333    0.8621        30\n",
      "         102     0.9667    0.9667    0.9667        30\n",
      "         103     1.0000    1.0000    1.0000        30\n",
      "         104     0.9643    0.9000    0.9310        30\n",
      "         105     0.9667    0.9667    0.9667        30\n",
      "         106     0.9062    0.9667    0.9355        30\n",
      "         107     1.0000    1.0000    1.0000        30\n",
      "         108     1.0000    1.0000    1.0000        30\n",
      "         109     0.9667    0.9667    0.9667        30\n",
      "         110     1.0000    0.9667    0.9831        30\n",
      "         111     0.8235    0.9333    0.8750        30\n",
      "         112     0.9667    0.9667    0.9667        30\n",
      "         113     1.0000    0.9667    0.9831        30\n",
      "         114     0.9333    0.9333    0.9333        30\n",
      "         115     0.9677    1.0000    0.9836        30\n",
      "         116     1.0000    0.9667    0.9831        30\n",
      "         117     1.0000    0.9667    0.9831        30\n",
      "         118     0.9667    0.9667    0.9667        30\n",
      "         119     1.0000    0.8000    0.8889        30\n",
      "         120     0.9333    0.9333    0.9333        30\n",
      "         121     1.0000    1.0000    1.0000        30\n",
      "         122     1.0000    0.8333    0.9091        30\n",
      "         123     0.9091    1.0000    0.9524        30\n",
      "         124     1.0000    1.0000    1.0000        30\n",
      "         125     0.9600    0.8000    0.8727        30\n",
      "         126     0.9600    0.8000    0.8727        30\n",
      "         127     0.9655    0.9333    0.9492        30\n",
      "         128     1.0000    1.0000    1.0000        30\n",
      "         129     0.9286    0.8667    0.8966        30\n",
      "         130     0.9667    0.9667    0.9667        30\n",
      "         131     0.9375    1.0000    0.9677        30\n",
      "         132     1.0000    1.0000    1.0000        30\n",
      "         133     1.0000    1.0000    1.0000        30\n",
      "         134     1.0000    1.0000    1.0000        30\n",
      "         135     0.9375    1.0000    0.9677        30\n",
      "         136     1.0000    0.9333    0.9655        30\n",
      "         137     1.0000    0.9667    0.9831        30\n",
      "         138     0.7714    0.9000    0.8308        30\n",
      "         139     1.0000    1.0000    1.0000        30\n",
      "         140     0.8387    0.8667    0.8525        30\n",
      "         141     0.7297    0.9000    0.8060        30\n",
      "         142     0.9677    1.0000    0.9836        30\n",
      "         143     1.0000    0.9667    0.9831        30\n",
      "         144     1.0000    1.0000    1.0000        30\n",
      "         145     0.9677    1.0000    0.9836        30\n",
      "         146     1.0000    0.9333    0.9655        30\n",
      "         147     1.0000    1.0000    1.0000        30\n",
      "         148     1.0000    1.0000    1.0000        30\n",
      "         149     0.9231    0.8000    0.8571        30\n",
      "         150     0.0000    0.0000    0.0000         0\n",
      "\n",
      "    accuracy                         0.9531      4500\n",
      "   macro avg     0.9495    0.9468    0.9470      4500\n",
      "weighted avg     0.9559    0.9531    0.9533      4500\n",
      "\n",
      "Out of Scope\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000         0\n",
      "           1     0.0000    0.0000    0.0000         0\n",
      "           2     0.0000    0.0000    0.0000         0\n",
      "           3     0.0000    0.0000    0.0000         0\n",
      "           5     0.0000    0.0000    0.0000         0\n",
      "           6     0.0000    0.0000    0.0000         0\n",
      "           7     0.0000    0.0000    0.0000         0\n",
      "           9     0.0000    0.0000    0.0000         0\n",
      "          10     0.0000    0.0000    0.0000         0\n",
      "          11     0.0000    0.0000    0.0000         0\n",
      "          13     0.0000    0.0000    0.0000         0\n",
      "          14     0.0000    0.0000    0.0000         0\n",
      "          16     0.0000    0.0000    0.0000         0\n",
      "          17     0.0000    0.0000    0.0000         0\n",
      "          18     0.0000    0.0000    0.0000         0\n",
      "          19     0.0000    0.0000    0.0000         0\n",
      "          20     0.0000    0.0000    0.0000         0\n",
      "          22     0.0000    0.0000    0.0000         0\n",
      "          23     0.0000    0.0000    0.0000         0\n",
      "          24     0.0000    0.0000    0.0000         0\n",
      "          25     0.0000    0.0000    0.0000         0\n",
      "          26     0.0000    0.0000    0.0000         0\n",
      "          27     0.0000    0.0000    0.0000         0\n",
      "          28     0.0000    0.0000    0.0000         0\n",
      "          30     0.0000    0.0000    0.0000         0\n",
      "          31     0.0000    0.0000    0.0000         0\n",
      "          32     0.0000    0.0000    0.0000         0\n",
      "          34     0.0000    0.0000    0.0000         0\n",
      "          35     0.0000    0.0000    0.0000         0\n",
      "          36     0.0000    0.0000    0.0000         0\n",
      "          37     0.0000    0.0000    0.0000         0\n",
      "          38     0.0000    0.0000    0.0000         0\n",
      "          39     0.0000    0.0000    0.0000         0\n",
      "          40     0.0000    0.0000    0.0000         0\n",
      "          41     0.0000    0.0000    0.0000         0\n",
      "          42     0.0000    0.0000    0.0000         0\n",
      "          43     0.0000    0.0000    0.0000         0\n",
      "          46     0.0000    0.0000    0.0000         0\n",
      "          47     0.0000    0.0000    0.0000         0\n",
      "          48     0.0000    0.0000    0.0000         0\n",
      "          49     0.0000    0.0000    0.0000         0\n",
      "          50     0.0000    0.0000    0.0000         0\n",
      "          51     0.0000    0.0000    0.0000         0\n",
      "          52     0.0000    0.0000    0.0000         0\n",
      "          53     0.0000    0.0000    0.0000         0\n",
      "          54     0.0000    0.0000    0.0000         0\n",
      "          55     0.0000    0.0000    0.0000         0\n",
      "          56     0.0000    0.0000    0.0000         0\n",
      "          57     0.0000    0.0000    0.0000         0\n",
      "          58     0.0000    0.0000    0.0000         0\n",
      "          59     0.0000    0.0000    0.0000         0\n",
      "          61     0.0000    0.0000    0.0000         0\n",
      "          63     0.0000    0.0000    0.0000         0\n",
      "          64     0.0000    0.0000    0.0000         0\n",
      "          65     0.0000    0.0000    0.0000         0\n",
      "          67     0.0000    0.0000    0.0000         0\n",
      "          69     0.0000    0.0000    0.0000         0\n",
      "          70     0.0000    0.0000    0.0000         0\n",
      "          71     0.0000    0.0000    0.0000         0\n",
      "          72     0.0000    0.0000    0.0000         0\n",
      "          74     0.0000    0.0000    0.0000         0\n",
      "          75     0.0000    0.0000    0.0000         0\n",
      "          76     0.0000    0.0000    0.0000         0\n",
      "          77     0.0000    0.0000    0.0000         0\n",
      "          78     0.0000    0.0000    0.0000         0\n",
      "          79     0.0000    0.0000    0.0000         0\n",
      "          80     0.0000    0.0000    0.0000         0\n",
      "          81     0.0000    0.0000    0.0000         0\n",
      "          82     0.0000    0.0000    0.0000         0\n",
      "          83     0.0000    0.0000    0.0000         0\n",
      "          84     0.0000    0.0000    0.0000         0\n",
      "          85     0.0000    0.0000    0.0000         0\n",
      "          86     0.0000    0.0000    0.0000         0\n",
      "          87     0.0000    0.0000    0.0000         0\n",
      "          88     0.0000    0.0000    0.0000         0\n",
      "          89     0.0000    0.0000    0.0000         0\n",
      "          90     0.0000    0.0000    0.0000         0\n",
      "          92     0.0000    0.0000    0.0000         0\n",
      "          93     0.0000    0.0000    0.0000         0\n",
      "          94     0.0000    0.0000    0.0000         0\n",
      "          97     0.0000    0.0000    0.0000         0\n",
      "          98     0.0000    0.0000    0.0000         0\n",
      "          99     0.0000    0.0000    0.0000         0\n",
      "         104     0.0000    0.0000    0.0000         0\n",
      "         105     0.0000    0.0000    0.0000         0\n",
      "         106     0.0000    0.0000    0.0000         0\n",
      "         107     0.0000    0.0000    0.0000         0\n",
      "         109     0.0000    0.0000    0.0000         0\n",
      "         111     0.0000    0.0000    0.0000         0\n",
      "         112     0.0000    0.0000    0.0000         0\n",
      "         114     0.0000    0.0000    0.0000         0\n",
      "         116     0.0000    0.0000    0.0000         0\n",
      "         118     0.0000    0.0000    0.0000         0\n",
      "         119     0.0000    0.0000    0.0000         0\n",
      "         120     0.0000    0.0000    0.0000         0\n",
      "         122     0.0000    0.0000    0.0000         0\n",
      "         123     0.0000    0.0000    0.0000         0\n",
      "         124     0.0000    0.0000    0.0000         0\n",
      "         125     0.0000    0.0000    0.0000         0\n",
      "         127     0.0000    0.0000    0.0000         0\n",
      "         128     0.0000    0.0000    0.0000         0\n",
      "         129     0.0000    0.0000    0.0000         0\n",
      "         131     0.0000    0.0000    0.0000         0\n",
      "         132     0.0000    0.0000    0.0000         0\n",
      "         133     0.0000    0.0000    0.0000         0\n",
      "         135     0.0000    0.0000    0.0000         0\n",
      "         136     0.0000    0.0000    0.0000         0\n",
      "         137     0.0000    0.0000    0.0000         0\n",
      "         138     0.0000    0.0000    0.0000         0\n",
      "         139     0.0000    0.0000    0.0000         0\n",
      "         140     0.0000    0.0000    0.0000         0\n",
      "         141     0.0000    0.0000    0.0000         0\n",
      "         142     0.0000    0.0000    0.0000         0\n",
      "         145     0.0000    0.0000    0.0000         0\n",
      "         146     0.0000    0.0000    0.0000         0\n",
      "         148     0.0000    0.0000    0.0000         0\n",
      "         149     0.0000    0.0000    0.0000         0\n",
      "         150     1.0000    0.3340    0.5007      1000\n",
      "\n",
      "    accuracy                         0.3340      1000\n",
      "   macro avg     0.0085    0.0028    0.0042      1000\n",
      "weighted avg     1.0000    0.3340    0.5007      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tayfun.tuna/opt/anaconda3/envs/astra/lib/python3.6/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "376.9124836921692"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time()\n",
    "train_evaulate_model(dirs[0], models[0], batch_sizes[0])\n",
    "time()- start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "8898b6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/clinic/data_imbalanced/\n",
      "Inscope\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9667    0.9831        30\n",
      "           1     0.9286    0.8667    0.8966        30\n",
      "           2     1.0000    0.8667    0.9286        30\n",
      "           3     0.9062    0.9667    0.9355        30\n",
      "           4     1.0000    1.0000    1.0000        30\n",
      "           5     0.9655    0.9333    0.9492        30\n",
      "           6     1.0000    0.9000    0.9474        30\n",
      "           7     1.0000    1.0000    1.0000        30\n",
      "           8     1.0000    1.0000    1.0000        30\n",
      "           9     0.9677    1.0000    0.9836        30\n",
      "          10     0.9032    0.9333    0.9180        30\n",
      "          11     0.8485    0.9333    0.8889        30\n",
      "          12     1.0000    0.9667    0.9831        30\n",
      "          13     1.0000    1.0000    1.0000        30\n",
      "          14     0.8750    0.9333    0.9032        30\n",
      "          15     0.9375    1.0000    0.9677        30\n",
      "          16     0.9630    0.8667    0.9123        30\n",
      "          17     1.0000    0.9000    0.9474        30\n",
      "          18     0.8710    0.9000    0.8852        30\n",
      "          19     1.0000    1.0000    1.0000        30\n",
      "          20     1.0000    0.9000    0.9474        30\n",
      "          21     1.0000    1.0000    1.0000        30\n",
      "          22     0.9000    0.9000    0.9000        30\n",
      "          23     1.0000    1.0000    1.0000        30\n",
      "          24     0.9091    1.0000    0.9524        30\n",
      "          25     1.0000    0.9667    0.9831        30\n",
      "          26     0.9310    0.9000    0.9153        30\n",
      "          27     1.0000    1.0000    1.0000        30\n",
      "          28     1.0000    1.0000    1.0000        30\n",
      "          29     1.0000    1.0000    1.0000        30\n",
      "          30     0.9375    1.0000    0.9677        30\n",
      "          31     0.9677    1.0000    0.9836        30\n",
      "          32     1.0000    0.9667    0.9831        30\n",
      "          33     1.0000    0.9667    0.9831        30\n",
      "          34     1.0000    0.9667    0.9831        30\n",
      "          35     0.9000    0.9000    0.9000        30\n",
      "          36     1.0000    0.9667    0.9831        30\n",
      "          37     1.0000    1.0000    1.0000        30\n",
      "          38     0.9000    0.9000    0.9000        30\n",
      "          39     1.0000    0.9667    0.9831        30\n",
      "          40     0.9630    0.8667    0.9123        30\n",
      "          41     1.0000    1.0000    1.0000        30\n",
      "          42     0.9375    1.0000    0.9677        30\n",
      "          43     1.0000    1.0000    1.0000        30\n",
      "          44     0.9091    1.0000    0.9524        30\n",
      "          45     1.0000    1.0000    1.0000        30\n",
      "          46     1.0000    1.0000    1.0000        30\n",
      "          47     1.0000    1.0000    1.0000        30\n",
      "          48     1.0000    0.9667    0.9831        30\n",
      "          49     0.9091    1.0000    0.9524        30\n",
      "          50     0.8286    0.9667    0.8923        30\n",
      "          51     1.0000    1.0000    1.0000        30\n",
      "          52     1.0000    0.9333    0.9655        30\n",
      "          53     1.0000    0.9667    0.9831        30\n",
      "          54     1.0000    1.0000    1.0000        30\n",
      "          55     0.9355    0.9667    0.9508        30\n",
      "          56     0.9677    1.0000    0.9836        30\n",
      "          57     0.9667    0.9667    0.9667        30\n",
      "          58     0.9655    0.9333    0.9492        30\n",
      "          59     0.9630    0.8667    0.9123        30\n",
      "          60     1.0000    0.9333    0.9655        30\n",
      "          61     0.9091    1.0000    0.9524        30\n",
      "          62     0.9643    0.9000    0.9310        30\n",
      "          63     0.9091    1.0000    0.9524        30\n",
      "          64     1.0000    1.0000    1.0000        30\n",
      "          65     0.8571    0.8000    0.8276        30\n",
      "          66     1.0000    0.9667    0.9831        30\n",
      "          67     0.9630    0.8667    0.9123        30\n",
      "          68     1.0000    1.0000    1.0000        30\n",
      "          69     0.8286    0.9667    0.8923        30\n",
      "          70     0.8710    0.9000    0.8852        30\n",
      "          71     1.0000    1.0000    1.0000        30\n",
      "          72     0.9583    0.7667    0.8519        30\n",
      "          73     0.8333    1.0000    0.9091        30\n",
      "          74     1.0000    1.0000    1.0000        30\n",
      "          75     0.8788    0.9667    0.9206        30\n",
      "          76     0.9062    0.9667    0.9355        30\n",
      "          77     1.0000    1.0000    1.0000        30\n",
      "          78     0.9310    0.9000    0.9153        30\n",
      "          79     1.0000    0.9667    0.9831        30\n",
      "          80     1.0000    1.0000    1.0000        30\n",
      "          81     0.9630    0.8667    0.9123        30\n",
      "          82     1.0000    1.0000    1.0000        30\n",
      "          83     0.9355    0.9667    0.9508        30\n",
      "          84     1.0000    1.0000    1.0000        30\n",
      "          85     0.9259    0.8333    0.8772        30\n",
      "          86     0.9643    0.9000    0.9310        30\n",
      "          87     1.0000    1.0000    1.0000        30\n",
      "          88     1.0000    0.8667    0.9286        30\n",
      "          89     0.8824    1.0000    0.9375        30\n",
      "          90     1.0000    1.0000    1.0000        30\n",
      "          91     0.9655    0.9333    0.9492        30\n",
      "          92     0.9375    1.0000    0.9677        30\n",
      "          93     0.9667    0.9667    0.9667        30\n",
      "          94     1.0000    1.0000    1.0000        30\n",
      "          95     0.8710    0.9000    0.8852        30\n",
      "          96     0.9667    0.9667    0.9667        30\n",
      "          97     1.0000    0.9667    0.9831        30\n",
      "          98     0.9677    1.0000    0.9836        30\n",
      "          99     0.9655    0.9333    0.9492        30\n",
      "         100     0.9375    1.0000    0.9677        30\n",
      "         101     0.9032    0.9333    0.9180        30\n",
      "         102     0.9355    0.9667    0.9508        30\n",
      "         103     1.0000    1.0000    1.0000        30\n",
      "         104     0.8286    0.9667    0.8923        30\n",
      "         105     1.0000    0.9000    0.9474        30\n",
      "         106     0.8571    1.0000    0.9231        30\n",
      "         107     1.0000    1.0000    1.0000        30\n",
      "         108     1.0000    1.0000    1.0000        30\n",
      "         109     1.0000    0.9333    0.9655        30\n",
      "         110     0.9677    1.0000    0.9836        30\n",
      "         111     0.8571    1.0000    0.9231        30\n",
      "         112     0.9667    0.9667    0.9667        30\n",
      "         113     0.8571    1.0000    0.9231        30\n",
      "         114     1.0000    0.7667    0.8679        30\n",
      "         115     0.9677    1.0000    0.9836        30\n",
      "         116     1.0000    1.0000    1.0000        30\n",
      "         117     0.9583    0.7667    0.8519        30\n",
      "         118     0.9667    0.9667    0.9667        30\n",
      "         119     0.9667    0.9667    0.9667        30\n",
      "         120     0.9677    1.0000    0.9836        30\n",
      "         121     1.0000    1.0000    1.0000        30\n",
      "         122     0.8889    0.8000    0.8421        30\n",
      "         123     0.9677    1.0000    0.9836        30\n",
      "         124     1.0000    1.0000    1.0000        30\n",
      "         125     0.9630    0.8667    0.9123        30\n",
      "         126     1.0000    0.9333    0.9655        30\n",
      "         127     0.9655    0.9333    0.9492        30\n",
      "         128     1.0000    1.0000    1.0000        30\n",
      "         129     0.9000    0.9000    0.9000        30\n",
      "         130     0.9667    0.9667    0.9667        30\n",
      "         131     0.8824    1.0000    0.9375        30\n",
      "         132     1.0000    1.0000    1.0000        30\n",
      "         133     1.0000    0.9667    0.9831        30\n",
      "         134     1.0000    1.0000    1.0000        30\n",
      "         135     0.9375    1.0000    0.9677        30\n",
      "         136     1.0000    0.9000    0.9474        30\n",
      "         137     1.0000    0.9667    0.9831        30\n",
      "         138     0.9200    0.7667    0.8364        30\n",
      "         139     1.0000    1.0000    1.0000        30\n",
      "         140     0.7931    0.7667    0.7797        30\n",
      "         141     0.7879    0.8667    0.8254        30\n",
      "         142     0.8571    1.0000    0.9231        30\n",
      "         143     0.9667    0.9667    0.9667        30\n",
      "         144     1.0000    1.0000    1.0000        30\n",
      "         145     1.0000    1.0000    1.0000        30\n",
      "         146     0.9286    0.8667    0.8966        30\n",
      "         147     0.9677    1.0000    0.9836        30\n",
      "         148     1.0000    1.0000    1.0000        30\n",
      "         149     0.9286    0.8667    0.8966        30\n",
      "         150     0.0000    0.0000    0.0000         0\n",
      "\n",
      "    accuracy                         0.9542      4500\n",
      "   macro avg     0.9512    0.9479    0.9483      4500\n",
      "weighted avg     0.9576    0.9542    0.9547      4500\n",
      "\n",
      "Out of Scope\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.0000    0.0000    0.0000         0\n",
      "           3     0.0000    0.0000    0.0000         0\n",
      "           5     0.0000    0.0000    0.0000         0\n",
      "           6     0.0000    0.0000    0.0000         0\n",
      "           7     0.0000    0.0000    0.0000         0\n",
      "           8     0.0000    0.0000    0.0000         0\n",
      "           9     0.0000    0.0000    0.0000         0\n",
      "          10     0.0000    0.0000    0.0000         0\n",
      "          11     0.0000    0.0000    0.0000         0\n",
      "          13     0.0000    0.0000    0.0000         0\n",
      "          16     0.0000    0.0000    0.0000         0\n",
      "          17     0.0000    0.0000    0.0000         0\n",
      "          18     0.0000    0.0000    0.0000         0\n",
      "          19     0.0000    0.0000    0.0000         0\n",
      "          20     0.0000    0.0000    0.0000         0\n",
      "          22     0.0000    0.0000    0.0000         0\n",
      "          23     0.0000    0.0000    0.0000         0\n",
      "          24     0.0000    0.0000    0.0000         0\n",
      "          25     0.0000    0.0000    0.0000         0\n",
      "          26     0.0000    0.0000    0.0000         0\n",
      "          27     0.0000    0.0000    0.0000         0\n",
      "          28     0.0000    0.0000    0.0000         0\n",
      "          29     0.0000    0.0000    0.0000         0\n",
      "          30     0.0000    0.0000    0.0000         0\n",
      "          31     0.0000    0.0000    0.0000         0\n",
      "          32     0.0000    0.0000    0.0000         0\n",
      "          33     0.0000    0.0000    0.0000         0\n",
      "          34     0.0000    0.0000    0.0000         0\n",
      "          36     0.0000    0.0000    0.0000         0\n",
      "          38     0.0000    0.0000    0.0000         0\n",
      "          39     0.0000    0.0000    0.0000         0\n",
      "          41     0.0000    0.0000    0.0000         0\n",
      "          42     0.0000    0.0000    0.0000         0\n",
      "          43     0.0000    0.0000    0.0000         0\n",
      "          44     0.0000    0.0000    0.0000         0\n",
      "          45     0.0000    0.0000    0.0000         0\n",
      "          46     0.0000    0.0000    0.0000         0\n",
      "          47     0.0000    0.0000    0.0000         0\n",
      "          48     0.0000    0.0000    0.0000         0\n",
      "          49     0.0000    0.0000    0.0000         0\n",
      "          50     0.0000    0.0000    0.0000         0\n",
      "          51     0.0000    0.0000    0.0000         0\n",
      "          52     0.0000    0.0000    0.0000         0\n",
      "          53     0.0000    0.0000    0.0000         0\n",
      "          54     0.0000    0.0000    0.0000         0\n",
      "          55     0.0000    0.0000    0.0000         0\n",
      "          56     0.0000    0.0000    0.0000         0\n",
      "          57     0.0000    0.0000    0.0000         0\n",
      "          58     0.0000    0.0000    0.0000         0\n",
      "          59     0.0000    0.0000    0.0000         0\n",
      "          61     0.0000    0.0000    0.0000         0\n",
      "          63     0.0000    0.0000    0.0000         0\n",
      "          64     0.0000    0.0000    0.0000         0\n",
      "          65     0.0000    0.0000    0.0000         0\n",
      "          67     0.0000    0.0000    0.0000         0\n",
      "          69     0.0000    0.0000    0.0000         0\n",
      "          70     0.0000    0.0000    0.0000         0\n",
      "          72     0.0000    0.0000    0.0000         0\n",
      "          73     0.0000    0.0000    0.0000         0\n",
      "          74     0.0000    0.0000    0.0000         0\n",
      "          75     0.0000    0.0000    0.0000         0\n",
      "          76     0.0000    0.0000    0.0000         0\n",
      "          77     0.0000    0.0000    0.0000         0\n",
      "          78     0.0000    0.0000    0.0000         0\n",
      "          80     0.0000    0.0000    0.0000         0\n",
      "          81     0.0000    0.0000    0.0000         0\n",
      "          82     0.0000    0.0000    0.0000         0\n",
      "          84     0.0000    0.0000    0.0000         0\n",
      "          86     0.0000    0.0000    0.0000         0\n",
      "          87     0.0000    0.0000    0.0000         0\n",
      "          88     0.0000    0.0000    0.0000         0\n",
      "          89     0.0000    0.0000    0.0000         0\n",
      "          90     0.0000    0.0000    0.0000         0\n",
      "          92     0.0000    0.0000    0.0000         0\n",
      "          93     0.0000    0.0000    0.0000         0\n",
      "          96     0.0000    0.0000    0.0000         0\n",
      "          97     0.0000    0.0000    0.0000         0\n",
      "          98     0.0000    0.0000    0.0000         0\n",
      "          99     0.0000    0.0000    0.0000         0\n",
      "         104     0.0000    0.0000    0.0000         0\n",
      "         106     0.0000    0.0000    0.0000         0\n",
      "         107     0.0000    0.0000    0.0000         0\n",
      "         112     0.0000    0.0000    0.0000         0\n",
      "         113     0.0000    0.0000    0.0000         0\n",
      "         114     0.0000    0.0000    0.0000         0\n",
      "         116     0.0000    0.0000    0.0000         0\n",
      "         119     0.0000    0.0000    0.0000         0\n",
      "         120     0.0000    0.0000    0.0000         0\n",
      "         121     0.0000    0.0000    0.0000         0\n",
      "         122     0.0000    0.0000    0.0000         0\n",
      "         123     0.0000    0.0000    0.0000         0\n",
      "         124     0.0000    0.0000    0.0000         0\n",
      "         125     0.0000    0.0000    0.0000         0\n",
      "         127     0.0000    0.0000    0.0000         0\n",
      "         128     0.0000    0.0000    0.0000         0\n",
      "         130     0.0000    0.0000    0.0000         0\n",
      "         131     0.0000    0.0000    0.0000         0\n",
      "         132     0.0000    0.0000    0.0000         0\n",
      "         133     0.0000    0.0000    0.0000         0\n",
      "         134     0.0000    0.0000    0.0000         0\n",
      "         135     0.0000    0.0000    0.0000         0\n",
      "         137     0.0000    0.0000    0.0000         0\n",
      "         138     0.0000    0.0000    0.0000         0\n",
      "         139     0.0000    0.0000    0.0000         0\n",
      "         140     0.0000    0.0000    0.0000         0\n",
      "         141     0.0000    0.0000    0.0000         0\n",
      "         142     0.0000    0.0000    0.0000         0\n",
      "         145     0.0000    0.0000    0.0000         0\n",
      "         146     0.0000    0.0000    0.0000         0\n",
      "         148     0.0000    0.0000    0.0000         0\n",
      "         149     0.0000    0.0000    0.0000         0\n",
      "         150     1.0000    0.5480    0.7080      1000\n",
      "\n",
      "    accuracy                         0.5480      1000\n",
      "   macro avg     0.0089    0.0049    0.0063      1000\n",
      "weighted avg     1.0000    0.5480    0.7080      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tayfun.tuna/opt/anaconda3/envs/astra/lib/python3.6/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14.265594959259033"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time()\n",
    "train_evaulate_model(dirs[1], models[1], batch_sizes[1])\n",
    "time()- start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a5a5a309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/clinic/data_oos_plus/\n",
      "Inscope\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9667    0.9831        30\n",
      "           1     1.0000    0.9333    0.9655        30\n",
      "           2     0.9667    0.9667    0.9667        30\n",
      "           3     0.9667    0.9667    0.9667        30\n",
      "           4     1.0000    1.0000    1.0000        30\n",
      "           5     0.9655    0.9333    0.9492        30\n",
      "           6     1.0000    0.9333    0.9655        30\n",
      "           7     1.0000    1.0000    1.0000        30\n",
      "           8     1.0000    0.9667    0.9831        30\n",
      "           9     0.9677    1.0000    0.9836        30\n",
      "          10     0.8750    0.9333    0.9032        30\n",
      "          11     0.8788    0.9667    0.9206        30\n",
      "          12     1.0000    0.9667    0.9831        30\n",
      "          13     1.0000    1.0000    1.0000        30\n",
      "          14     0.8824    1.0000    0.9375        30\n",
      "          15     0.9375    1.0000    0.9677        30\n",
      "          16     0.9677    1.0000    0.9836        30\n",
      "          17     1.0000    0.9333    0.9655        30\n",
      "          18     0.8182    0.9000    0.8571        30\n",
      "          19     1.0000    1.0000    1.0000        30\n",
      "          20     1.0000    0.9000    0.9474        30\n",
      "          21     1.0000    1.0000    1.0000        30\n",
      "          22     0.9643    0.9000    0.9310        30\n",
      "          23     1.0000    1.0000    1.0000        30\n",
      "          24     1.0000    0.9333    0.9655        30\n",
      "          25     1.0000    0.9667    0.9831        30\n",
      "          26     0.9032    0.9333    0.9180        30\n",
      "          27     1.0000    1.0000    1.0000        30\n",
      "          28     0.9677    1.0000    0.9836        30\n",
      "          29     1.0000    1.0000    1.0000        30\n",
      "          30     1.0000    1.0000    1.0000        30\n",
      "          31     0.9677    1.0000    0.9836        30\n",
      "          32     1.0000    0.9667    0.9831        30\n",
      "          33     1.0000    1.0000    1.0000        30\n",
      "          34     1.0000    1.0000    1.0000        30\n",
      "          35     0.9655    0.9333    0.9492        30\n",
      "          36     1.0000    0.9667    0.9831        30\n",
      "          37     1.0000    1.0000    1.0000        30\n",
      "          38     0.9643    0.9000    0.9310        30\n",
      "          39     1.0000    1.0000    1.0000        30\n",
      "          40     0.8529    0.9667    0.9062        30\n",
      "          41     1.0000    1.0000    1.0000        30\n",
      "          42     0.9677    1.0000    0.9836        30\n",
      "          43     1.0000    1.0000    1.0000        30\n",
      "          44     0.9677    1.0000    0.9836        30\n",
      "          45     0.9677    1.0000    0.9836        30\n",
      "          46     1.0000    1.0000    1.0000        30\n",
      "          47     1.0000    0.9667    0.9831        30\n",
      "          48     1.0000    1.0000    1.0000        30\n",
      "          49     0.9375    1.0000    0.9677        30\n",
      "          50     0.9032    0.9333    0.9180        30\n",
      "          51     1.0000    1.0000    1.0000        30\n",
      "          52     1.0000    1.0000    1.0000        30\n",
      "          53     1.0000    0.9667    0.9831        30\n",
      "          54     1.0000    1.0000    1.0000        30\n",
      "          55     0.9677    1.0000    0.9836        30\n",
      "          56     1.0000    1.0000    1.0000        30\n",
      "          57     0.9667    0.9667    0.9667        30\n",
      "          58     0.9643    0.9000    0.9310        30\n",
      "          59     0.9600    0.8000    0.8727        30\n",
      "          60     0.9643    0.9000    0.9310        30\n",
      "          61     0.9667    0.9667    0.9667        30\n",
      "          62     0.9630    0.8667    0.9123        30\n",
      "          63     1.0000    1.0000    1.0000        30\n",
      "          64     1.0000    1.0000    1.0000        30\n",
      "          65     0.9231    0.8000    0.8571        30\n",
      "          66     0.9355    0.9667    0.9508        30\n",
      "          67     0.9643    0.9000    0.9310        30\n",
      "          68     0.9677    1.0000    0.9836        30\n",
      "          69     0.8056    0.9667    0.8788        30\n",
      "          70     0.9259    0.8333    0.8772        30\n",
      "          71     1.0000    1.0000    1.0000        30\n",
      "          72     0.9630    0.8667    0.9123        30\n",
      "          73     0.9375    1.0000    0.9677        30\n",
      "          74     0.9677    1.0000    0.9836        30\n",
      "          75     0.9062    0.9667    0.9355        30\n",
      "          76     0.8529    0.9667    0.9062        30\n",
      "          77     1.0000    0.9667    0.9831        30\n",
      "          78     0.9000    0.9000    0.9000        30\n",
      "          79     1.0000    0.9667    0.9831        30\n",
      "          80     0.9677    1.0000    0.9836        30\n",
      "          81     0.9310    0.9000    0.9153        30\n",
      "          82     1.0000    1.0000    1.0000        30\n",
      "          83     0.9333    0.9333    0.9333        30\n",
      "          84     1.0000    0.9667    0.9831        30\n",
      "          85     0.9310    0.9000    0.9153        30\n",
      "          86     0.9655    0.9333    0.9492        30\n",
      "          87     1.0000    1.0000    1.0000        30\n",
      "          88     1.0000    0.9000    0.9474        30\n",
      "          89     1.0000    1.0000    1.0000        30\n",
      "          90     1.0000    0.9667    0.9831        30\n",
      "          91     0.9643    0.9000    0.9310        30\n",
      "          92     0.9677    1.0000    0.9836        30\n",
      "          93     0.9355    0.9667    0.9508        30\n",
      "          94     1.0000    1.0000    1.0000        30\n",
      "          95     0.8788    0.9667    0.9206        30\n",
      "          96     0.9667    0.9667    0.9667        30\n",
      "          97     0.9667    0.9667    0.9667        30\n",
      "          98     0.9677    1.0000    0.9836        30\n",
      "          99     0.9677    1.0000    0.9836        30\n",
      "         100     0.9375    1.0000    0.9677        30\n",
      "         101     0.9032    0.9333    0.9180        30\n",
      "         102     0.9062    0.9667    0.9355        30\n",
      "         103     1.0000    1.0000    1.0000        30\n",
      "         104     0.8788    0.9667    0.9206        30\n",
      "         105     0.9375    1.0000    0.9677        30\n",
      "         106     0.9091    1.0000    0.9524        30\n",
      "         107     1.0000    1.0000    1.0000        30\n",
      "         108     1.0000    1.0000    1.0000        30\n",
      "         109     1.0000    0.9333    0.9655        30\n",
      "         110     1.0000    0.9333    0.9655        30\n",
      "         111     0.8529    0.9667    0.9062        30\n",
      "         112     0.9355    0.9667    0.9508        30\n",
      "         113     1.0000    1.0000    1.0000        30\n",
      "         114     0.9643    0.9000    0.9310        30\n",
      "         115     0.9677    1.0000    0.9836        30\n",
      "         116     1.0000    0.9667    0.9831        30\n",
      "         117     0.9667    0.9667    0.9667        30\n",
      "         118     1.0000    0.9333    0.9655        30\n",
      "         119     1.0000    0.9333    0.9655        30\n",
      "         120     0.9375    1.0000    0.9677        30\n",
      "         121     1.0000    1.0000    1.0000        30\n",
      "         122     1.0000    0.8667    0.9286        30\n",
      "         123     1.0000    0.9333    0.9655        30\n",
      "         124     1.0000    1.0000    1.0000        30\n",
      "         125     1.0000    0.8667    0.9286        30\n",
      "         126     0.9375    1.0000    0.9677        30\n",
      "         127     0.9667    0.9667    0.9667        30\n",
      "         128     1.0000    1.0000    1.0000        30\n",
      "         129     0.9333    0.9333    0.9333        30\n",
      "         130     0.9667    0.9667    0.9667        30\n",
      "         131     0.8824    1.0000    0.9375        30\n",
      "         132     1.0000    1.0000    1.0000        30\n",
      "         133     0.9667    0.9667    0.9667        30\n",
      "         134     0.9677    1.0000    0.9836        30\n",
      "         135     0.9375    1.0000    0.9677        30\n",
      "         136     0.9643    0.9000    0.9310        30\n",
      "         137     1.0000    0.9667    0.9831        30\n",
      "         138     0.8889    0.8000    0.8421        30\n",
      "         139     1.0000    1.0000    1.0000        30\n",
      "         140     0.9200    0.7667    0.8364        30\n",
      "         141     0.8000    0.9333    0.8615        30\n",
      "         142     0.9667    0.9667    0.9667        30\n",
      "         143     1.0000    1.0000    1.0000        30\n",
      "         144     1.0000    1.0000    1.0000        30\n",
      "         145     1.0000    1.0000    1.0000        30\n",
      "         146     1.0000    0.8667    0.9286        30\n",
      "         147     1.0000    1.0000    1.0000        30\n",
      "         148     1.0000    1.0000    1.0000        30\n",
      "         149     0.9259    0.8333    0.8772        30\n",
      "         150     0.0000    0.0000    0.0000         0\n",
      "\n",
      "    accuracy                         0.9618      4500\n",
      "   macro avg     0.9591    0.9554    0.9564      4500\n",
      "weighted avg     0.9655    0.9618    0.9627      4500\n",
      "\n",
      "Out of Scope\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.0000    0.0000    0.0000         0\n",
      "           2     0.0000    0.0000    0.0000         0\n",
      "           3     0.0000    0.0000    0.0000         0\n",
      "           5     0.0000    0.0000    0.0000         0\n",
      "           6     0.0000    0.0000    0.0000         0\n",
      "           7     0.0000    0.0000    0.0000         0\n",
      "           9     0.0000    0.0000    0.0000         0\n",
      "          10     0.0000    0.0000    0.0000         0\n",
      "          11     0.0000    0.0000    0.0000         0\n",
      "          16     0.0000    0.0000    0.0000         0\n",
      "          17     0.0000    0.0000    0.0000         0\n",
      "          18     0.0000    0.0000    0.0000         0\n",
      "          19     0.0000    0.0000    0.0000         0\n",
      "          20     0.0000    0.0000    0.0000         0\n",
      "          22     0.0000    0.0000    0.0000         0\n",
      "          23     0.0000    0.0000    0.0000         0\n",
      "          24     0.0000    0.0000    0.0000         0\n",
      "          25     0.0000    0.0000    0.0000         0\n",
      "          26     0.0000    0.0000    0.0000         0\n",
      "          27     0.0000    0.0000    0.0000         0\n",
      "          28     0.0000    0.0000    0.0000         0\n",
      "          30     0.0000    0.0000    0.0000         0\n",
      "          32     0.0000    0.0000    0.0000         0\n",
      "          33     0.0000    0.0000    0.0000         0\n",
      "          34     0.0000    0.0000    0.0000         0\n",
      "          35     0.0000    0.0000    0.0000         0\n",
      "          36     0.0000    0.0000    0.0000         0\n",
      "          37     0.0000    0.0000    0.0000         0\n",
      "          38     0.0000    0.0000    0.0000         0\n",
      "          41     0.0000    0.0000    0.0000         0\n",
      "          42     0.0000    0.0000    0.0000         0\n",
      "          43     0.0000    0.0000    0.0000         0\n",
      "          44     0.0000    0.0000    0.0000         0\n",
      "          47     0.0000    0.0000    0.0000         0\n",
      "          48     0.0000    0.0000    0.0000         0\n",
      "          49     0.0000    0.0000    0.0000         0\n",
      "          50     0.0000    0.0000    0.0000         0\n",
      "          52     0.0000    0.0000    0.0000         0\n",
      "          53     0.0000    0.0000    0.0000         0\n",
      "          55     0.0000    0.0000    0.0000         0\n",
      "          58     0.0000    0.0000    0.0000         0\n",
      "          59     0.0000    0.0000    0.0000         0\n",
      "          60     0.0000    0.0000    0.0000         0\n",
      "          61     0.0000    0.0000    0.0000         0\n",
      "          63     0.0000    0.0000    0.0000         0\n",
      "          64     0.0000    0.0000    0.0000         0\n",
      "          65     0.0000    0.0000    0.0000         0\n",
      "          67     0.0000    0.0000    0.0000         0\n",
      "          68     0.0000    0.0000    0.0000         0\n",
      "          69     0.0000    0.0000    0.0000         0\n",
      "          71     0.0000    0.0000    0.0000         0\n",
      "          72     0.0000    0.0000    0.0000         0\n",
      "          73     0.0000    0.0000    0.0000         0\n",
      "          75     0.0000    0.0000    0.0000         0\n",
      "          76     0.0000    0.0000    0.0000         0\n",
      "          77     0.0000    0.0000    0.0000         0\n",
      "          78     0.0000    0.0000    0.0000         0\n",
      "          80     0.0000    0.0000    0.0000         0\n",
      "          81     0.0000    0.0000    0.0000         0\n",
      "          82     0.0000    0.0000    0.0000         0\n",
      "          83     0.0000    0.0000    0.0000         0\n",
      "          84     0.0000    0.0000    0.0000         0\n",
      "          87     0.0000    0.0000    0.0000         0\n",
      "          88     0.0000    0.0000    0.0000         0\n",
      "          89     0.0000    0.0000    0.0000         0\n",
      "          92     0.0000    0.0000    0.0000         0\n",
      "          93     0.0000    0.0000    0.0000         0\n",
      "          94     0.0000    0.0000    0.0000         0\n",
      "          96     0.0000    0.0000    0.0000         0\n",
      "          97     0.0000    0.0000    0.0000         0\n",
      "          98     0.0000    0.0000    0.0000         0\n",
      "          99     0.0000    0.0000    0.0000         0\n",
      "         101     0.0000    0.0000    0.0000         0\n",
      "         102     0.0000    0.0000    0.0000         0\n",
      "         104     0.0000    0.0000    0.0000         0\n",
      "         105     0.0000    0.0000    0.0000         0\n",
      "         106     0.0000    0.0000    0.0000         0\n",
      "         107     0.0000    0.0000    0.0000         0\n",
      "         112     0.0000    0.0000    0.0000         0\n",
      "         114     0.0000    0.0000    0.0000         0\n",
      "         117     0.0000    0.0000    0.0000         0\n",
      "         119     0.0000    0.0000    0.0000         0\n",
      "         120     0.0000    0.0000    0.0000         0\n",
      "         123     0.0000    0.0000    0.0000         0\n",
      "         124     0.0000    0.0000    0.0000         0\n",
      "         132     0.0000    0.0000    0.0000         0\n",
      "         133     0.0000    0.0000    0.0000         0\n",
      "         135     0.0000    0.0000    0.0000         0\n",
      "         136     0.0000    0.0000    0.0000         0\n",
      "         137     0.0000    0.0000    0.0000         0\n",
      "         138     0.0000    0.0000    0.0000         0\n",
      "         140     0.0000    0.0000    0.0000         0\n",
      "         141     0.0000    0.0000    0.0000         0\n",
      "         142     0.0000    0.0000    0.0000         0\n",
      "         145     0.0000    0.0000    0.0000         0\n",
      "         148     0.0000    0.0000    0.0000         0\n",
      "         149     0.0000    0.0000    0.0000         0\n",
      "         150     1.0000    0.6930    0.8187      1000\n",
      "\n",
      "    accuracy                         0.6930      1000\n",
      "   macro avg     0.0102    0.0071    0.0084      1000\n",
      "weighted avg     1.0000    0.6930    0.8187      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tayfun.tuna/opt/anaconda3/envs/astra/lib/python3.6/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "40.016282081604004"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time()\n",
    "train_evaulate_model(dirs[2], models[2], batch_sizes[2])\n",
    "time()- start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "5a2e39ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/clinic/data_small/\n",
      "Inscope\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9667    0.9831        30\n",
      "           1     1.0000    0.9000    0.9474        30\n",
      "           2     1.0000    0.9000    0.9474        30\n",
      "           3     0.9667    0.9667    0.9667        30\n",
      "           4     1.0000    1.0000    1.0000        30\n",
      "           5     0.9643    0.9000    0.9310        30\n",
      "           6     0.9667    0.9667    0.9667        30\n",
      "           7     1.0000    0.9667    0.9831        30\n",
      "           8     1.0000    0.9667    0.9831        30\n",
      "           9     1.0000    1.0000    1.0000        30\n",
      "          10     0.8788    0.9667    0.9206        30\n",
      "          11     0.9375    1.0000    0.9677        30\n",
      "          12     1.0000    0.9333    0.9655        30\n",
      "          13     1.0000    1.0000    1.0000        30\n",
      "          14     0.8108    1.0000    0.8955        30\n",
      "          15     0.9375    1.0000    0.9677        30\n",
      "          16     0.9091    1.0000    0.9524        30\n",
      "          17     1.0000    0.9000    0.9474        30\n",
      "          18     0.9000    0.9000    0.9000        30\n",
      "          19     1.0000    1.0000    1.0000        30\n",
      "          20     1.0000    0.9667    0.9831        30\n",
      "          21     0.9677    1.0000    0.9836        30\n",
      "          22     1.0000    0.9000    0.9474        30\n",
      "          23     1.0000    0.9667    0.9831        30\n",
      "          24     1.0000    0.8667    0.9286        30\n",
      "          25     1.0000    0.9667    0.9831        30\n",
      "          26     0.8966    0.8667    0.8814        30\n",
      "          27     0.9375    1.0000    0.9677        30\n",
      "          28     0.9677    1.0000    0.9836        30\n",
      "          29     1.0000    1.0000    1.0000        30\n",
      "          30     0.9677    1.0000    0.9836        30\n",
      "          31     0.9375    1.0000    0.9677        30\n",
      "          32     1.0000    0.9667    0.9831        30\n",
      "          33     1.0000    0.9667    0.9831        30\n",
      "          34     1.0000    1.0000    1.0000        30\n",
      "          35     0.8286    0.9667    0.8923        30\n",
      "          36     1.0000    0.9333    0.9655        30\n",
      "          37     0.9677    1.0000    0.9836        30\n",
      "          38     0.8750    0.9333    0.9032        30\n",
      "          39     1.0000    1.0000    1.0000        30\n",
      "          40     0.9355    0.9667    0.9508        30\n",
      "          41     1.0000    1.0000    1.0000        30\n",
      "          42     0.9677    1.0000    0.9836        30\n",
      "          43     1.0000    1.0000    1.0000        30\n",
      "          44     0.9375    1.0000    0.9677        30\n",
      "          45     0.9667    0.9667    0.9667        30\n",
      "          46     0.9677    1.0000    0.9836        30\n",
      "          47     1.0000    0.9667    0.9831        30\n",
      "          48     1.0000    0.9667    0.9831        30\n",
      "          49     0.7838    0.9667    0.8657        30\n",
      "          50     0.8710    0.9000    0.8852        30\n",
      "          51     1.0000    1.0000    1.0000        30\n",
      "          52     1.0000    1.0000    1.0000        30\n",
      "          53     1.0000    1.0000    1.0000        30\n",
      "          54     1.0000    1.0000    1.0000        30\n",
      "          55     0.9355    0.9667    0.9508        30\n",
      "          56     0.9375    1.0000    0.9677        30\n",
      "          57     0.9667    0.9667    0.9667        30\n",
      "          58     0.9655    0.9333    0.9492        30\n",
      "          59     0.9259    0.8333    0.8772        30\n",
      "          60     0.9655    0.9333    0.9492        30\n",
      "          61     0.9677    1.0000    0.9836        30\n",
      "          62     0.9630    0.8667    0.9123        30\n",
      "          63     0.9375    1.0000    0.9677        30\n",
      "          64     1.0000    1.0000    1.0000        30\n",
      "          65     0.9200    0.7667    0.8364        30\n",
      "          66     1.0000    0.9667    0.9831        30\n",
      "          67     0.9615    0.8333    0.8929        30\n",
      "          68     1.0000    0.9667    0.9831        30\n",
      "          69     0.8824    1.0000    0.9375        30\n",
      "          70     0.8710    0.9000    0.8852        30\n",
      "          71     1.0000    1.0000    1.0000        30\n",
      "          72     0.9615    0.8333    0.8929        30\n",
      "          73     0.9062    0.9667    0.9355        30\n",
      "          74     0.9677    1.0000    0.9836        30\n",
      "          75     0.9375    1.0000    0.9677        30\n",
      "          76     0.8788    0.9667    0.9206        30\n",
      "          77     1.0000    0.9667    0.9831        30\n",
      "          78     0.8750    0.9333    0.9032        30\n",
      "          79     1.0000    1.0000    1.0000        30\n",
      "          80     0.9655    0.9333    0.9492        30\n",
      "          81     1.0000    0.9000    0.9474        30\n",
      "          82     1.0000    1.0000    1.0000        30\n",
      "          83     0.8710    0.9000    0.8852        30\n",
      "          84     1.0000    1.0000    1.0000        30\n",
      "          85     0.8966    0.8667    0.8814        30\n",
      "          86     1.0000    0.8333    0.9091        30\n",
      "          87     1.0000    1.0000    1.0000        30\n",
      "          88     0.9355    0.9667    0.9508        30\n",
      "          89     0.9677    1.0000    0.9836        30\n",
      "          90     1.0000    0.9667    0.9831        30\n",
      "          91     0.9310    0.9000    0.9153        30\n",
      "          92     1.0000    1.0000    1.0000        30\n",
      "          93     0.9630    0.8667    0.9123        30\n",
      "          94     1.0000    1.0000    1.0000        30\n",
      "          95     0.8750    0.9333    0.9032        30\n",
      "          96     0.9375    1.0000    0.9677        30\n",
      "          97     0.9333    0.9333    0.9333        30\n",
      "          98     0.9677    1.0000    0.9836        30\n",
      "          99     0.9655    0.9333    0.9492        30\n",
      "         100     0.8529    0.9667    0.9062        30\n",
      "         101     0.9655    0.9333    0.9492        30\n",
      "         102     0.9355    0.9667    0.9508        30\n",
      "         103     1.0000    1.0000    1.0000        30\n",
      "         104     1.0000    0.9667    0.9831        30\n",
      "         105     1.0000    0.9333    0.9655        30\n",
      "         106     0.9655    0.9333    0.9492        30\n",
      "         107     1.0000    1.0000    1.0000        30\n",
      "         108     0.9677    1.0000    0.9836        30\n",
      "         109     1.0000    0.9667    0.9831        30\n",
      "         110     1.0000    0.8667    0.9286        30\n",
      "         111     0.8750    0.9333    0.9032        30\n",
      "         112     1.0000    0.9000    0.9474        30\n",
      "         113     1.0000    1.0000    1.0000        30\n",
      "         114     0.9310    0.9000    0.9153        30\n",
      "         115     1.0000    1.0000    1.0000        30\n",
      "         116     0.9375    1.0000    0.9677        30\n",
      "         117     1.0000    0.8667    0.9286        30\n",
      "         118     0.9643    0.9000    0.9310        30\n",
      "         119     0.9655    0.9333    0.9492        30\n",
      "         120     1.0000    0.9667    0.9831        30\n",
      "         121     1.0000    1.0000    1.0000        30\n",
      "         122     0.8333    0.8333    0.8333        30\n",
      "         123     1.0000    0.9000    0.9474        30\n",
      "         124     1.0000    0.9667    0.9831        30\n",
      "         125     0.9600    0.8000    0.8727        30\n",
      "         126     0.8824    1.0000    0.9375        30\n",
      "         127     0.9062    0.9667    0.9355        30\n",
      "         128     1.0000    1.0000    1.0000        30\n",
      "         129     0.9355    0.9667    0.9508        30\n",
      "         130     1.0000    1.0000    1.0000        30\n",
      "         131     0.9375    1.0000    0.9677        30\n",
      "         132     1.0000    1.0000    1.0000        30\n",
      "         133     1.0000    0.9333    0.9655        30\n",
      "         134     0.9677    1.0000    0.9836        30\n",
      "         135     0.9375    1.0000    0.9677        30\n",
      "         136     1.0000    0.9667    0.9831        30\n",
      "         137     1.0000    0.9667    0.9831        30\n",
      "         138     1.0000    0.8000    0.8889        30\n",
      "         139     1.0000    1.0000    1.0000        30\n",
      "         140     0.7143    0.6667    0.6897        30\n",
      "         141     0.7714    0.9000    0.8308        30\n",
      "         142     0.9677    1.0000    0.9836        30\n",
      "         143     1.0000    0.9667    0.9831        30\n",
      "         144     1.0000    0.9667    0.9831        30\n",
      "         145     0.9677    1.0000    0.9836        30\n",
      "         146     0.9630    0.8667    0.9123        30\n",
      "         147     1.0000    1.0000    1.0000        30\n",
      "         148     1.0000    1.0000    1.0000        30\n",
      "         149     0.9630    0.8667    0.9123        30\n",
      "         150     0.0000    0.0000    0.0000         0\n",
      "\n",
      "    accuracy                         0.9538      4500\n",
      "   macro avg     0.9540    0.9475    0.9496      4500\n",
      "weighted avg     0.9603    0.9538    0.9559      4500\n",
      "\n",
      "Out of Scope\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.0000    0.0000    0.0000         0\n",
      "           3     0.0000    0.0000    0.0000         0\n",
      "           5     0.0000    0.0000    0.0000         0\n",
      "           6     0.0000    0.0000    0.0000         0\n",
      "           7     0.0000    0.0000    0.0000         0\n",
      "          10     0.0000    0.0000    0.0000         0\n",
      "          11     0.0000    0.0000    0.0000         0\n",
      "          14     0.0000    0.0000    0.0000         0\n",
      "          15     0.0000    0.0000    0.0000         0\n",
      "          16     0.0000    0.0000    0.0000         0\n",
      "          17     0.0000    0.0000    0.0000         0\n",
      "          18     0.0000    0.0000    0.0000         0\n",
      "          19     0.0000    0.0000    0.0000         0\n",
      "          20     0.0000    0.0000    0.0000         0\n",
      "          22     0.0000    0.0000    0.0000         0\n",
      "          23     0.0000    0.0000    0.0000         0\n",
      "          25     0.0000    0.0000    0.0000         0\n",
      "          26     0.0000    0.0000    0.0000         0\n",
      "          27     0.0000    0.0000    0.0000         0\n",
      "          28     0.0000    0.0000    0.0000         0\n",
      "          32     0.0000    0.0000    0.0000         0\n",
      "          33     0.0000    0.0000    0.0000         0\n",
      "          34     0.0000    0.0000    0.0000         0\n",
      "          35     0.0000    0.0000    0.0000         0\n",
      "          36     0.0000    0.0000    0.0000         0\n",
      "          37     0.0000    0.0000    0.0000         0\n",
      "          38     0.0000    0.0000    0.0000         0\n",
      "          40     0.0000    0.0000    0.0000         0\n",
      "          41     0.0000    0.0000    0.0000         0\n",
      "          42     0.0000    0.0000    0.0000         0\n",
      "          43     0.0000    0.0000    0.0000         0\n",
      "          44     0.0000    0.0000    0.0000         0\n",
      "          46     0.0000    0.0000    0.0000         0\n",
      "          47     0.0000    0.0000    0.0000         0\n",
      "          49     0.0000    0.0000    0.0000         0\n",
      "          50     0.0000    0.0000    0.0000         0\n",
      "          51     0.0000    0.0000    0.0000         0\n",
      "          52     0.0000    0.0000    0.0000         0\n",
      "          53     0.0000    0.0000    0.0000         0\n",
      "          55     0.0000    0.0000    0.0000         0\n",
      "          56     0.0000    0.0000    0.0000         0\n",
      "          57     0.0000    0.0000    0.0000         0\n",
      "          58     0.0000    0.0000    0.0000         0\n",
      "          59     0.0000    0.0000    0.0000         0\n",
      "          60     0.0000    0.0000    0.0000         0\n",
      "          61     0.0000    0.0000    0.0000         0\n",
      "          63     0.0000    0.0000    0.0000         0\n",
      "          64     0.0000    0.0000    0.0000         0\n",
      "          65     0.0000    0.0000    0.0000         0\n",
      "          67     0.0000    0.0000    0.0000         0\n",
      "          69     0.0000    0.0000    0.0000         0\n",
      "          70     0.0000    0.0000    0.0000         0\n",
      "          72     0.0000    0.0000    0.0000         0\n",
      "          73     0.0000    0.0000    0.0000         0\n",
      "          75     0.0000    0.0000    0.0000         0\n",
      "          76     0.0000    0.0000    0.0000         0\n",
      "          77     0.0000    0.0000    0.0000         0\n",
      "          78     0.0000    0.0000    0.0000         0\n",
      "          79     0.0000    0.0000    0.0000         0\n",
      "          80     0.0000    0.0000    0.0000         0\n",
      "          81     0.0000    0.0000    0.0000         0\n",
      "          82     0.0000    0.0000    0.0000         0\n",
      "          83     0.0000    0.0000    0.0000         0\n",
      "          84     0.0000    0.0000    0.0000         0\n",
      "          87     0.0000    0.0000    0.0000         0\n",
      "          88     0.0000    0.0000    0.0000         0\n",
      "          89     0.0000    0.0000    0.0000         0\n",
      "          90     0.0000    0.0000    0.0000         0\n",
      "          91     0.0000    0.0000    0.0000         0\n",
      "          92     0.0000    0.0000    0.0000         0\n",
      "          93     0.0000    0.0000    0.0000         0\n",
      "          94     0.0000    0.0000    0.0000         0\n",
      "          96     0.0000    0.0000    0.0000         0\n",
      "          97     0.0000    0.0000    0.0000         0\n",
      "          98     0.0000    0.0000    0.0000         0\n",
      "          99     0.0000    0.0000    0.0000         0\n",
      "         100     0.0000    0.0000    0.0000         0\n",
      "         103     0.0000    0.0000    0.0000         0\n",
      "         104     0.0000    0.0000    0.0000         0\n",
      "         106     0.0000    0.0000    0.0000         0\n",
      "         107     0.0000    0.0000    0.0000         0\n",
      "         111     0.0000    0.0000    0.0000         0\n",
      "         114     0.0000    0.0000    0.0000         0\n",
      "         116     0.0000    0.0000    0.0000         0\n",
      "         119     0.0000    0.0000    0.0000         0\n",
      "         120     0.0000    0.0000    0.0000         0\n",
      "         121     0.0000    0.0000    0.0000         0\n",
      "         122     0.0000    0.0000    0.0000         0\n",
      "         123     0.0000    0.0000    0.0000         0\n",
      "         124     0.0000    0.0000    0.0000         0\n",
      "         127     0.0000    0.0000    0.0000         0\n",
      "         128     0.0000    0.0000    0.0000         0\n",
      "         129     0.0000    0.0000    0.0000         0\n",
      "         130     0.0000    0.0000    0.0000         0\n",
      "         132     0.0000    0.0000    0.0000         0\n",
      "         133     0.0000    0.0000    0.0000         0\n",
      "         136     0.0000    0.0000    0.0000         0\n",
      "         137     0.0000    0.0000    0.0000         0\n",
      "         138     0.0000    0.0000    0.0000         0\n",
      "         140     0.0000    0.0000    0.0000         0\n",
      "         141     0.0000    0.0000    0.0000         0\n",
      "         142     0.0000    0.0000    0.0000         0\n",
      "         143     0.0000    0.0000    0.0000         0\n",
      "         145     0.0000    0.0000    0.0000         0\n",
      "         146     0.0000    0.0000    0.0000         0\n",
      "         148     0.0000    0.0000    0.0000         0\n",
      "         149     0.0000    0.0000    0.0000         0\n",
      "         150     1.0000    0.5830    0.7366      1000\n",
      "\n",
      "    accuracy                         0.5830      1000\n",
      "   macro avg     0.0093    0.0054    0.0068      1000\n",
      "weighted avg     1.0000    0.5830    0.7366      1000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "192.43939208984375"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time()\n",
    "train_evaulate_model(dirs[3], models[3], batch_sizes[3])\n",
    "time()- start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24be1e9",
   "metadata": {},
   "source": [
    "# Summary Results"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d373159d",
   "metadata": {},
   "source": [
    "../data/clinic/data_full/\n",
    "\n",
    "Inscope:        precision    recall  f1-score   support           \n",
    "    accuracy                         0.9531      4500\n",
    "   macro avg     0.9495    0.9468    0.9470      4500\n",
    "weighted avg     0.9559    0.9531    0.9533      4500\n",
    "\n",
    "\n",
    "Outof Scope:   precision    recall  f1-score   support\n",
    "    accuracy                         0.3340      1000\n",
    "   macro avg     0.0085    0.0028    0.0042      1000\n",
    "weighted avg     1.0000    0.3340    0.5007      1000"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6ea39fd8",
   "metadata": {},
   "source": [
    "../data/clinic/data_imbalanced/\n",
    "\n",
    "Inscope:        precision    recall  f1-score   support           \n",
    "    accuracy                         0.9542      4500\n",
    "   macro avg     0.9512    0.9479    0.9483      4500\n",
    "weighted avg     0.9576    0.9542    0.9547      4500\n",
    "\n",
    "\n",
    "    accuracy                         0.5480      1000\n",
    "   macro avg     0.0089    0.0049    0.0063      1000\n",
    "weighted avg     1.0000    0.5480    0.7080      1000"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d2fd4263",
   "metadata": {},
   "source": [
    "../data/clinic/data_oos_plus/\n",
    "\n",
    "Inscope:        precision    recall  f1-score   support           \n",
    "    accuracy                         0.9618      4500\n",
    "   macro avg     0.9591    0.9554    0.9564      4500\n",
    "weighted avg     0.9655    0.9618    0.9627      4500\n",
    "\n",
    "\n",
    "Outof Scope:   precision    recall  f1-score   support\n",
    "    accuracy                         0.6930      1000\n",
    "   macro avg     0.0102    0.0071    0.0084      1000\n",
    "weighted avg     1.0000    0.6930    0.8187      1000"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2ad589d4",
   "metadata": {},
   "source": [
    "../data/clinic/data_small/\n",
    "\n",
    "Inscope:        precision    recall  f1-score   support           \n",
    "    accuracy                         0.9538      4500\n",
    "   macro avg     0.9540    0.9475    0.9496      4500\n",
    "weighted avg     0.9603    0.9538    0.9559      4500\n",
    "\n",
    "\n",
    "Outof Scope:   precision    recall  f1-score   support\n",
    "    accuracy                         0.5830      1000\n",
    "   macro avg     0.0093    0.0054    0.0068      1000\n",
    "weighted avg     1.0000    0.5830    0.7366      1000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
