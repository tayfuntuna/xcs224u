{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0699ba0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd971efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emb_1',\n",
       " 'emb_2',\n",
       " 'emb_3',\n",
       " 'emb_4',\n",
       " 'emb_5',\n",
       " 'emb_6',\n",
       " 'emb_7',\n",
       " 'emb_8',\n",
       " 'emb_9',\n",
       " 'emb_10',\n",
       " 'emb_11',\n",
       " 'emb_12',\n",
       " 'emb_13',\n",
       " 'emb_14',\n",
       " 'emb_15',\n",
       " 'emb_16',\n",
       " 'emb_17',\n",
       " 'emb_18',\n",
       " 'emb_19',\n",
       " 'emb_20',\n",
       " 'emb_21',\n",
       " 'emb_22',\n",
       " 'emb_23',\n",
       " 'emb_24',\n",
       " 'emb_25',\n",
       " 'emb_26',\n",
       " 'emb_27',\n",
       " 'emb_28',\n",
       " 'emb_29',\n",
       " 'emb_30',\n",
       " 'emb_31',\n",
       " 'emb_32',\n",
       " 'emb_33',\n",
       " 'emb_34',\n",
       " 'emb_35',\n",
       " 'emb_36',\n",
       " 'emb_37',\n",
       " 'emb_38',\n",
       " 'emb_39',\n",
       " 'emb_40',\n",
       " 'emb_41',\n",
       " 'emb_42',\n",
       " 'emb_43',\n",
       " 'emb_44',\n",
       " 'emb_45',\n",
       " 'emb_46',\n",
       " 'emb_47',\n",
       " 'emb_48',\n",
       " 'emb_49',\n",
       " 'emb_50',\n",
       " 'emb_51',\n",
       " 'emb_52',\n",
       " 'emb_53',\n",
       " 'emb_54',\n",
       " 'emb_55',\n",
       " 'emb_56',\n",
       " 'emb_57',\n",
       " 'emb_58',\n",
       " 'emb_59',\n",
       " 'emb_60',\n",
       " 'emb_61',\n",
       " 'emb_62',\n",
       " 'emb_63',\n",
       " 'emb_64',\n",
       " 'emb_65',\n",
       " 'emb_66',\n",
       " 'emb_67',\n",
       " 'emb_68',\n",
       " 'emb_69',\n",
       " 'emb_70',\n",
       " 'emb_71',\n",
       " 'emb_72',\n",
       " 'emb_73',\n",
       " 'emb_74',\n",
       " 'emb_75',\n",
       " 'emb_76',\n",
       " 'emb_77',\n",
       " 'emb_78',\n",
       " 'emb_79',\n",
       " 'emb_80',\n",
       " 'emb_81',\n",
       " 'emb_82',\n",
       " 'emb_83',\n",
       " 'emb_84',\n",
       " 'emb_85',\n",
       " 'emb_86',\n",
       " 'emb_87',\n",
       " 'emb_88',\n",
       " 'emb_89',\n",
       " 'emb_90',\n",
       " 'emb_91',\n",
       " 'emb_92',\n",
       " 'emb_93',\n",
       " 'emb_94',\n",
       " 'emb_95',\n",
       " 'emb_96',\n",
       " 'emb_97',\n",
       " 'emb_98',\n",
       " 'emb_99',\n",
       " 'emb_100',\n",
       " 'emb_101',\n",
       " 'emb_102',\n",
       " 'emb_103',\n",
       " 'emb_104',\n",
       " 'emb_105',\n",
       " 'emb_106',\n",
       " 'emb_107',\n",
       " 'emb_108',\n",
       " 'emb_109',\n",
       " 'emb_110',\n",
       " 'emb_111',\n",
       " 'emb_112',\n",
       " 'emb_113',\n",
       " 'emb_114',\n",
       " 'emb_115',\n",
       " 'emb_116',\n",
       " 'emb_117',\n",
       " 'emb_118',\n",
       " 'emb_119',\n",
       " 'emb_120',\n",
       " 'emb_121',\n",
       " 'emb_122',\n",
       " 'emb_123',\n",
       " 'emb_124',\n",
       " 'emb_125',\n",
       " 'emb_126',\n",
       " 'emb_127',\n",
       " 'emb_128',\n",
       " 'emb_129',\n",
       " 'emb_130',\n",
       " 'emb_131',\n",
       " 'emb_132',\n",
       " 'emb_133',\n",
       " 'emb_134',\n",
       " 'emb_135',\n",
       " 'emb_136',\n",
       " 'emb_137',\n",
       " 'emb_138',\n",
       " 'emb_139',\n",
       " 'emb_140',\n",
       " 'emb_141',\n",
       " 'emb_142',\n",
       " 'emb_143',\n",
       " 'emb_144',\n",
       " 'emb_145',\n",
       " 'emb_146',\n",
       " 'emb_147',\n",
       " 'emb_148',\n",
       " 'emb_149',\n",
       " 'emb_150',\n",
       " 'emb_151',\n",
       " 'emb_152',\n",
       " 'emb_153',\n",
       " 'emb_154',\n",
       " 'emb_155',\n",
       " 'emb_156',\n",
       " 'emb_157',\n",
       " 'emb_158',\n",
       " 'emb_159',\n",
       " 'emb_160',\n",
       " 'emb_161',\n",
       " 'emb_162',\n",
       " 'emb_163',\n",
       " 'emb_164',\n",
       " 'emb_165',\n",
       " 'emb_166',\n",
       " 'emb_167',\n",
       " 'emb_168',\n",
       " 'emb_169',\n",
       " 'emb_170',\n",
       " 'emb_171',\n",
       " 'emb_172',\n",
       " 'emb_173',\n",
       " 'emb_174',\n",
       " 'emb_175',\n",
       " 'emb_176',\n",
       " 'emb_177',\n",
       " 'emb_178',\n",
       " 'emb_179',\n",
       " 'emb_180',\n",
       " 'emb_181',\n",
       " 'emb_182',\n",
       " 'emb_183',\n",
       " 'emb_184',\n",
       " 'emb_185',\n",
       " 'emb_186',\n",
       " 'emb_187',\n",
       " 'emb_188',\n",
       " 'emb_189',\n",
       " 'emb_190',\n",
       " 'emb_191',\n",
       " 'emb_192',\n",
       " 'emb_193',\n",
       " 'emb_194',\n",
       " 'emb_195',\n",
       " 'emb_196',\n",
       " 'emb_197',\n",
       " 'emb_198',\n",
       " 'emb_199',\n",
       " 'emb_200',\n",
       " 'emb_201',\n",
       " 'emb_202',\n",
       " 'emb_203',\n",
       " 'emb_204',\n",
       " 'emb_205',\n",
       " 'emb_206',\n",
       " 'emb_207',\n",
       " 'emb_208',\n",
       " 'emb_209',\n",
       " 'emb_210',\n",
       " 'emb_211',\n",
       " 'emb_212',\n",
       " 'emb_213',\n",
       " 'emb_214',\n",
       " 'emb_215',\n",
       " 'emb_216',\n",
       " 'emb_217',\n",
       " 'emb_218',\n",
       " 'emb_219',\n",
       " 'emb_220',\n",
       " 'emb_221',\n",
       " 'emb_222',\n",
       " 'emb_223',\n",
       " 'emb_224',\n",
       " 'emb_225',\n",
       " 'emb_226',\n",
       " 'emb_227',\n",
       " 'emb_228',\n",
       " 'emb_229',\n",
       " 'emb_230',\n",
       " 'emb_231',\n",
       " 'emb_232',\n",
       " 'emb_233',\n",
       " 'emb_234',\n",
       " 'emb_235',\n",
       " 'emb_236',\n",
       " 'emb_237',\n",
       " 'emb_238',\n",
       " 'emb_239',\n",
       " 'emb_240',\n",
       " 'emb_241',\n",
       " 'emb_242',\n",
       " 'emb_243',\n",
       " 'emb_244',\n",
       " 'emb_245',\n",
       " 'emb_246',\n",
       " 'emb_247',\n",
       " 'emb_248',\n",
       " 'emb_249',\n",
       " 'emb_250',\n",
       " 'emb_251',\n",
       " 'emb_252',\n",
       " 'emb_253',\n",
       " 'emb_254',\n",
       " 'emb_255',\n",
       " 'emb_256',\n",
       " 'emb_257',\n",
       " 'emb_258',\n",
       " 'emb_259',\n",
       " 'emb_260',\n",
       " 'emb_261',\n",
       " 'emb_262',\n",
       " 'emb_263',\n",
       " 'emb_264',\n",
       " 'emb_265',\n",
       " 'emb_266',\n",
       " 'emb_267',\n",
       " 'emb_268',\n",
       " 'emb_269',\n",
       " 'emb_270',\n",
       " 'emb_271',\n",
       " 'emb_272',\n",
       " 'emb_273',\n",
       " 'emb_274',\n",
       " 'emb_275',\n",
       " 'emb_276',\n",
       " 'emb_277',\n",
       " 'emb_278',\n",
       " 'emb_279',\n",
       " 'emb_280',\n",
       " 'emb_281',\n",
       " 'emb_282',\n",
       " 'emb_283',\n",
       " 'emb_284',\n",
       " 'emb_285',\n",
       " 'emb_286',\n",
       " 'emb_287',\n",
       " 'emb_288',\n",
       " 'emb_289',\n",
       " 'emb_290',\n",
       " 'emb_291',\n",
       " 'emb_292',\n",
       " 'emb_293',\n",
       " 'emb_294',\n",
       " 'emb_295',\n",
       " 'emb_296',\n",
       " 'emb_297',\n",
       " 'emb_298',\n",
       " 'emb_299',\n",
       " 'emb_300',\n",
       " 'emb_301',\n",
       " 'emb_302',\n",
       " 'emb_303',\n",
       " 'emb_304',\n",
       " 'emb_305',\n",
       " 'emb_306',\n",
       " 'emb_307',\n",
       " 'emb_308',\n",
       " 'emb_309',\n",
       " 'emb_310',\n",
       " 'emb_311',\n",
       " 'emb_312',\n",
       " 'emb_313',\n",
       " 'emb_314',\n",
       " 'emb_315',\n",
       " 'emb_316',\n",
       " 'emb_317',\n",
       " 'emb_318',\n",
       " 'emb_319',\n",
       " 'emb_320',\n",
       " 'emb_321',\n",
       " 'emb_322',\n",
       " 'emb_323',\n",
       " 'emb_324',\n",
       " 'emb_325',\n",
       " 'emb_326',\n",
       " 'emb_327',\n",
       " 'emb_328',\n",
       " 'emb_329',\n",
       " 'emb_330',\n",
       " 'emb_331',\n",
       " 'emb_332',\n",
       " 'emb_333',\n",
       " 'emb_334',\n",
       " 'emb_335',\n",
       " 'emb_336',\n",
       " 'emb_337',\n",
       " 'emb_338',\n",
       " 'emb_339',\n",
       " 'emb_340',\n",
       " 'emb_341',\n",
       " 'emb_342',\n",
       " 'emb_343',\n",
       " 'emb_344',\n",
       " 'emb_345',\n",
       " 'emb_346',\n",
       " 'emb_347',\n",
       " 'emb_348',\n",
       " 'emb_349',\n",
       " 'emb_350',\n",
       " 'emb_351',\n",
       " 'emb_352',\n",
       " 'emb_353',\n",
       " 'emb_354',\n",
       " 'emb_355',\n",
       " 'emb_356',\n",
       " 'emb_357',\n",
       " 'emb_358',\n",
       " 'emb_359',\n",
       " 'emb_360',\n",
       " 'emb_361',\n",
       " 'emb_362',\n",
       " 'emb_363',\n",
       " 'emb_364',\n",
       " 'emb_365',\n",
       " 'emb_366',\n",
       " 'emb_367',\n",
       " 'emb_368',\n",
       " 'emb_369',\n",
       " 'emb_370',\n",
       " 'emb_371',\n",
       " 'emb_372',\n",
       " 'emb_373',\n",
       " 'emb_374',\n",
       " 'emb_375',\n",
       " 'emb_376',\n",
       " 'emb_377',\n",
       " 'emb_378',\n",
       " 'emb_379',\n",
       " 'emb_380',\n",
       " 'emb_381',\n",
       " 'emb_382',\n",
       " 'emb_383',\n",
       " 'emb_384',\n",
       " 'emb_385',\n",
       " 'emb_386',\n",
       " 'emb_387',\n",
       " 'emb_388',\n",
       " 'emb_389',\n",
       " 'emb_390',\n",
       " 'emb_391',\n",
       " 'emb_392',\n",
       " 'emb_393',\n",
       " 'emb_394',\n",
       " 'emb_395',\n",
       " 'emb_396',\n",
       " 'emb_397',\n",
       " 'emb_398',\n",
       " 'emb_399',\n",
       " 'emb_400',\n",
       " 'emb_401',\n",
       " 'emb_402',\n",
       " 'emb_403',\n",
       " 'emb_404',\n",
       " 'emb_405',\n",
       " 'emb_406',\n",
       " 'emb_407',\n",
       " 'emb_408',\n",
       " 'emb_409',\n",
       " 'emb_410',\n",
       " 'emb_411',\n",
       " 'emb_412',\n",
       " 'emb_413',\n",
       " 'emb_414',\n",
       " 'emb_415',\n",
       " 'emb_416',\n",
       " 'emb_417',\n",
       " 'emb_418',\n",
       " 'emb_419',\n",
       " 'emb_420',\n",
       " 'emb_421',\n",
       " 'emb_422',\n",
       " 'emb_423',\n",
       " 'emb_424',\n",
       " 'emb_425',\n",
       " 'emb_426',\n",
       " 'emb_427',\n",
       " 'emb_428',\n",
       " 'emb_429',\n",
       " 'emb_430',\n",
       " 'emb_431',\n",
       " 'emb_432',\n",
       " 'emb_433',\n",
       " 'emb_434',\n",
       " 'emb_435',\n",
       " 'emb_436',\n",
       " 'emb_437',\n",
       " 'emb_438',\n",
       " 'emb_439',\n",
       " 'emb_440',\n",
       " 'emb_441',\n",
       " 'emb_442',\n",
       " 'emb_443',\n",
       " 'emb_444',\n",
       " 'emb_445',\n",
       " 'emb_446',\n",
       " 'emb_447',\n",
       " 'emb_448',\n",
       " 'emb_449',\n",
       " 'emb_450',\n",
       " 'emb_451',\n",
       " 'emb_452',\n",
       " 'emb_453',\n",
       " 'emb_454',\n",
       " 'emb_455',\n",
       " 'emb_456',\n",
       " 'emb_457',\n",
       " 'emb_458',\n",
       " 'emb_459',\n",
       " 'emb_460',\n",
       " 'emb_461',\n",
       " 'emb_462',\n",
       " 'emb_463',\n",
       " 'emb_464',\n",
       " 'emb_465',\n",
       " 'emb_466',\n",
       " 'emb_467',\n",
       " 'emb_468',\n",
       " 'emb_469',\n",
       " 'emb_470',\n",
       " 'emb_471',\n",
       " 'emb_472',\n",
       " 'emb_473',\n",
       " 'emb_474',\n",
       " 'emb_475',\n",
       " 'emb_476',\n",
       " 'emb_477',\n",
       " 'emb_478',\n",
       " 'emb_479',\n",
       " 'emb_480',\n",
       " 'emb_481',\n",
       " 'emb_482',\n",
       " 'emb_483',\n",
       " 'emb_484',\n",
       " 'emb_485',\n",
       " 'emb_486',\n",
       " 'emb_487',\n",
       " 'emb_488',\n",
       " 'emb_489',\n",
       " 'emb_490',\n",
       " 'emb_491',\n",
       " 'emb_492',\n",
       " 'emb_493',\n",
       " 'emb_494',\n",
       " 'emb_495',\n",
       " 'emb_496',\n",
       " 'emb_497',\n",
       " 'emb_498',\n",
       " 'emb_499',\n",
       " 'emb_500',\n",
       " 'emb_501',\n",
       " 'emb_502',\n",
       " 'emb_503',\n",
       " 'emb_504',\n",
       " 'emb_505',\n",
       " 'emb_506',\n",
       " 'emb_507',\n",
       " 'emb_508',\n",
       " 'emb_509',\n",
       " 'emb_510',\n",
       " 'emb_511',\n",
       " 'emb_512']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBED_COLS = ['emb_' + str(i+1) for i in range(512)] \n",
    "EMBED_COLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fad0a1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/awasthiabhijeet/Learning-From-Rules/tree/master/src/hls\n",
    "\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "import os\n",
    "\n",
    "LABEL_DICT = {\"non-critical\": 0, \"critical\": 1}\n",
    "\n",
    "def load_data(mode):\n",
    "    data = pd.read_csv(DATA_PATH + mode + \".csv\", encoding='latin1')\n",
    "    sentence = [x.lower().strip() for x in data[\"text\"].tolist()]\n",
    "    label = [LABEL_DICT[item] for item in data[\"label\"].tolist()]\n",
    "    data = list(zip(sentence, label))\n",
    "    return data\n",
    "\n",
    "def load_rules_from_csv():\n",
    "    rules = []\n",
    "    file_name = 'rule.tsv'\n",
    "    with open(file_name, 'r', encoding='latin1') as f:\n",
    "        for line in f:\n",
    "            list_in = line.strip().split(\"\\t\")\n",
    "            label = LABEL_DICT[list_in[0]]\n",
    "            pattern = list_in[1]\n",
    "            sent = list_in[2].lower()\n",
    "            rules.append((sent, label, pattern))\n",
    "    return rules\n",
    "\n",
    "def load_rules():\n",
    "    rules = []\n",
    "    for i in range(len(dt_rules)):\n",
    "        label = LABEL_DICT[dt_rules['label'][i]]\n",
    "        pattern = '' #dt_rules['pattern'][i]\n",
    "        sent = dt_rules['text'][i].lower()\n",
    "        rules.append((sent, label, pattern))\n",
    "    return rules\n",
    "\n",
    "def load_data_from_csv(file_name):\n",
    "    return pd.read_csv(DATA_PATH + file_name + \".csv\", encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe7fd3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {'calories': 0, 'order': 1, 'travel_notification': 2, 'make_call': 3, 'redeem_rewards': 4, 'credit_limit_change': 5, 'timer': 6, 'flip_coin': 7, 'transactions': 8, 'what_can_i_ask_you': 9, 'what_are_your_hobbies': 10, 'pto_request': 11, 'credit_limit': 12, 'rewards_balance': 13, 'account_blocked': 14, 'insurance_change': 15, 'reminder': 16, 'timezone': 17, 'how_busy': 18, 'min_payment': 19, 'change_volume': 20, 'tell_joke': 21, 'pin_change': 22, 'balance': 23, 'calendar_update': 24, 'meeting_schedule': 25, 'share_location': 26, 'reset_settings': 27, 'interest_rate': 28, 'update_playlist': 29, 'how_old_are_you': 30, 'weather': 31, 'pto_request_status': 32, 'bill_due': 33, 'nutrition_info': 34, 'roll_dice': 35, 'report_lost_card': 36, 'next_song': 37, 'gas': 38, 'pto_used': 39, 'repeat': 40, 'travel_suggestion': 41, 'jump_start': 42, 'thank_you': 43, 'expiration_date': 44, 'taxes': 45, 'oil_change_how': 46, 'credit_score': 47, 'oil_change_when': 48, 'income': 49, 'order_status': 50, 'user_name': 51, 'cancel_reservation': 52, 'change_accent': 53, 'international_fees': 54, 'definition': 55, 'whisper_mode': 56, 'spending_history': 57, 'time': 58, 'tire_pressure': 59, 'payday': 60, 'reminder_update': 61, 'book_hotel': 62, 'find_phone': 63, 'calculator': 64, 'plug_type': 65, 'recipe': 66, 'sync_device': 67, 'text': 68, 'accept_reservations': 69, 'translate': 70, 'card_declined': 71, 'ingredient_substitution': 72, 'who_made_you': 73, 'what_is_your_name': 74, 'current_location': 75, 'vaccines': 76, 'cook_time': 77, 'calendar': 78, 'w2': 79, 'book_flight': 80, 'measurement_conversion': 81, 'restaurant_suggestion': 82, 'where_are_you_from': 83, 'smart_home': 84, 'bill_balance': 85, 'carry_on': 86, 'cancel': 87, 'schedule_meeting': 88, 'direct_deposit': 89, 'maybe': 90, 'goodbye': 91, 'report_fraud': 92, 'lost_luggage': 93, 'mpg': 94, 'change_speed': 95, 'fun_fact': 96, 'confirm_reservation': 97, 'flight_status': 98, 'gas_type': 99, 'order_checks': 100, 'restaurant_reviews': 101, 'freeze_account': 102, 'tire_change': 103, 'shopping_list_update': 104, 'insurance': 105, 'change_ai_name': 106, 'change_language': 107, 'do_you_have_pets': 108, 'apr': 109, 'pay_bill': 110, 'replacement_card_duration': 111, 'meal_suggestion': 112, 'restaurant_reservation': 113, 'pto_balance': 114, 'date': 115, 'uber': 116, 'exchange_rate': 117, 'greeting': 118, 'rollover_401k': 119, 'next_holiday': 120, 'routing': 121, 'traffic': 122, 'meaning_of_life': 123, 'are_you_a_bot': 124, 'schedule_maintenance': 125, 'todo_list': 126, 'who_do_you_work_for': 127, 'application_status': 128, 'directions': 129, 'yes': 130, 'new_card': 131, 'travel_alert': 132, 'what_song': 133, 'international_visa': 134, 'transfer': 135, 'change_user_name': 136, 'play_music': 137, 'alarm': 138, 'food_last': 139, 'shopping_list': 140, 'improve_credit_score': 141, 'todo_list_update': 142, 'car_rental': 143, 'spelling': 144, 'last_maintenance': 145, 'damaged_card': 146, 'no': 147, 'distance': 148, 'ingredients_list': 149, 'oos': 150}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cf0e4898",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generate_data:\n",
    "    def __init__(self, train_data, validation_data, test_data):\n",
    "        self.num_labels = 151\n",
    "        self.train_data = train_data\n",
    "        self.validation_data = validation_data\n",
    "        self.test_data = test_data\n",
    "        self.num_rules = 3\n",
    "\n",
    "    def fire_rules_original(self,sentence):\n",
    "        #returns m and l values for the sentence\n",
    "        m = np.zeros(self.num_rules)\n",
    "        l = self.num_labels + np.zeros(self.num_rules)\n",
    "        for rid,(sent,lab,pat) in enumerate(self.rules):\n",
    "            result = re.findall(re.compile(pat), sentence.strip().lower())                        \n",
    "            if result:\n",
    "                m[rid] = 1\n",
    "                l[rid] = lab\n",
    "        return m,l\n",
    "    \n",
    "    def fire_rules(self,sentence):\n",
    "        #returns m and l values for the sentence\n",
    "        m = np.zeros(self.num_rules)\n",
    "        l = self.num_labels + np.zeros(self.num_rules)\n",
    "        for rid,(sent,lab,pat) in enumerate(self.rules):\n",
    "            result = re.findall(re.compile(pat), sentence.strip().lower())                        \n",
    "            if result:\n",
    "                m[rid] = 1\n",
    "                l[rid] = lab\n",
    "        return m,l\n",
    "\n",
    "    def _geneate_pickles_for_u(self,mode):\n",
    "        if mode == \"U\":\n",
    "            data = self.train_data\n",
    "        elif mode == \"test\":\n",
    "            data = self.test_data\n",
    "        elif mode == \"validation\":\n",
    "            data = self.validation_data\n",
    "        else:\n",
    "            print(\"Error: Wrong mode\")\n",
    "            exit()\n",
    "            \n",
    "        #xx, xl, xm, xL, xd, xr = [], [], [], [], [], []\n",
    "       \n",
    "        xx = data[EMBED_COLS]\n",
    "        xL = [self.num_labels  if mode == \"U\" else l for l in data['label'].tolist()]\n",
    "        xl = data[['l1','l2','l3']].values\n",
    "        xm = data[['l1','l2','l3']].values\n",
    "\n",
    "        xr = []\n",
    "        for i in range(len(xl)):\n",
    "            xl[i] = [self.num_labels if v==-1 else v for v in xl[i]]\n",
    "            xm[i] = [0 if v==-1 else 1 for v in xm[i]]\n",
    "            xr.append(np.zeros(self.num_rules))\n",
    "      \n",
    "        data['xd'] = 0\n",
    "        xd = data['xd'].tolist()\n",
    "           \n",
    "        #print(len(xl))\n",
    "        #print(xl[0])\n",
    "        #print(xl[1])\n",
    "        \n",
    "        with open(TARGET_PATH + \"{}_processed.p\".format(mode),\"wb\") as pkl_f, open(TARGET_PATH + \"{}_sentences.txt\".format(mode),\"w\") as txt_f:\n",
    "                for sentence in data['text']:\n",
    "                    txt_f.write(sentence.strip()+'\\n')\n",
    "        \n",
    "                pickle.dump(np.array(xx),pkl_f)\n",
    "                pickle.dump(np.array(xl),pkl_f)\n",
    "                pickle.dump(np.array(xm),pkl_f)\n",
    "                pickle.dump(np.array(xL),pkl_f)\n",
    "                pickle.dump(np.array(xd),pkl_f)\n",
    "                pickle.dump(np.array(xr),pkl_f)\n",
    "\n",
    "    def _geneate_pickles_for_d(self):\n",
    "        #d_x, d_l, d_m, d_L, d_d, d_r = [], [], [], [], [], []\n",
    "        d_x = self.train_data[EMBED_COLS]\n",
    "        d_L = [l for l in self.train_data['label'].tolist()]\n",
    "        d_l = self.train_data[['l1','l2','l3']].values\n",
    "        d_m = self.train_data[['l1','l2','l3']].values\n",
    "        self.train_data['d_d'] = 1\n",
    "        d_d = self.train_data['d_d'].tolist()\n",
    "        d_r = []\n",
    "        for i in range(len(d_l)):\n",
    "            d_l[i] = [self.num_labels if v==-1 else v for v in d_l[i]]\n",
    "            d_m[i] = [0 if v==-1 else 1 for v in d_m[i]]\n",
    "            d_r.append(np.zeros(self.num_rules))\n",
    "        #d_r = d_m\n",
    "        \n",
    "        #print(len(d_m))\n",
    "        #print(d_m[0])\n",
    "        #print(d_m[1])\n",
    "        \n",
    "        with open(TARGET_PATH +\"d_processed.p\", \"wb\") as pkl_f, open(TARGET_PATH +\"d_sentences.txt\",\"w\") as txt_f:\n",
    "            for sentence in self.train_data['text']:\n",
    "                txt_f.write(sentence.strip()+'\\n')\n",
    "            pickle.dump(np.array(d_x), pkl_f)\n",
    "            pickle.dump(np.array(d_l), pkl_f)\n",
    "            pickle.dump(np.array(d_m), pkl_f)\n",
    "            pickle.dump(np.array(d_L), pkl_f)\n",
    "            pickle.dump(np.array(d_d), pkl_f)\n",
    "            pickle.dump(np.array(d_r), pkl_f)\n",
    "           \n",
    "            \n",
    "    def generate_pickles(self):\n",
    "        self._geneate_pickles_for_d()\n",
    "        self._geneate_pickles_for_u(\"U\")\n",
    "        self._geneate_pickles_for_u(\"validation\")\n",
    "        self._geneate_pickles_for_u(\"test\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d6775acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"../data/clinic/\"\n",
    "TARGET_BASE = \"../data/astra/\"\n",
    "sub_folders = [\"data_small/\", 'data_imbalanced/', 'data_full/', 'data_oos_plus/']\n",
    "for sub_folder in sub_folders:\n",
    "    df_train = pd.read_csv(DATA_FOLDER + sub_folder + 'train_with_use_emb_with_weak_label.csv')\n",
    "    df_valid = pd.read_csv(DATA_FOLDER + sub_folder + 'val_with_use_emb_with_weak_label.csv')\n",
    "    df_test = pd.read_csv(DATA_FOLDER + sub_folder + 'test_with_use_emb_with_weak_label.csv')\n",
    "    TARGET_PATH = TARGET_BASE + sub_folder + \"step1/\"\n",
    "    #print(df_train.head())\n",
    "    obj = Generate_data(df_train,df_valid,df_test)\n",
    "    obj.generate_pickles()\n",
    "\n",
    "    #TARGET_PATH2 = \"/Users/tayfun.tuna/workspace/ASTRA/data/HR/LF_Review/labeled\" +str(train_size) + \"_valid\"+str(valid_size)+\"/step2/seed42/\"\n",
    "\n",
    "    #pdataset = PreprocessedDataset(datapath = TARGET_PATH, dataset='hr',seed=42)\n",
    "    #pdataset.preprocess_fn(TARGET_PATH, TARGET_PATH2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "963c9672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processing code from https://github.com/awasthiabhijeet/Learning-From-Rules\n",
    "import pickle\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "f_d = 'f_d'\n",
    "f_d_U = 'f_d_U'\n",
    "test_w = 'test_w'\n",
    "\n",
    "train_modes = [f_d, f_d_U]\n",
    "\n",
    "F_d_U_Data = collections.namedtuple('GMMDataF_d_U', 'x l m L d r')\n",
    "\n",
    "\n",
    "def discard_r(data):\n",
    "    r = np.zeros(data.r.shape)\n",
    "    return F_d_U_Data(data.x, data.l, data.m, data.L, data.d, r)\n",
    "\n",
    "def concatenate_data(d1, d2):\n",
    "    x = np.vstack([d1.x, d2.x])\n",
    "    l = np.vstack([d1.l, d2.l])\n",
    "    m = np.vstack([d1.m, d2.m])\n",
    "    L = np.vstack([d1.L, d2.L])\n",
    "    d = np.vstack([d1.d, d2.d])\n",
    "    r = np.vstack([d1.r, d2.r])\n",
    "    return F_d_U_Data(x, l, m, L, d, r)\n",
    "\n",
    "def keep_ind(data, inds):\n",
    "    x = data.x[inds]\n",
    "    l = data.l[inds]\n",
    "    m = data.m[inds]\n",
    "    L = data.L[inds]\n",
    "    d = data.d[inds]\n",
    "    r = data.r[inds]\n",
    "    return F_d_U_Data(x, l, m, L, d, r)\n",
    "\n",
    "def load_data(fname, num_load=None):\n",
    "    print('Loading from ', fname)\n",
    "    with open(fname, 'rb') as f:\n",
    "        x = pickle.load(f)\n",
    "        l = pickle.load(f).astype(np.int32)\n",
    "        m = pickle.load(f).astype(np.int32)\n",
    "        L = pickle.load(f).astype(np.int32)\n",
    "        d = pickle.load(f).astype(np.int32)\n",
    "        r = pickle.load(f).astype(np.int32)\n",
    "\n",
    "        len_x = len(x)\n",
    "        print(len(l), len_x)\n",
    "        assert len(l) == len_x\n",
    "        assert len(m) == len_x\n",
    "        assert len(L) == len_x\n",
    "        assert len(d) == len_x\n",
    "        assert len(r) == len_x\n",
    "\n",
    "        L = np.reshape(L, (L.shape[0], 1))\n",
    "        d = np.reshape(d, (d.shape[0], 1))\n",
    "\n",
    "        if num_load is not None and num_load < len_x:\n",
    "            x = x[:num_load]\n",
    "            l = l[:num_load]\n",
    "            m = m[:num_load]\n",
    "            L = L[:num_load]\n",
    "            d = d[:num_load]\n",
    "            r = r[:num_load]\n",
    "\n",
    "        return F_d_U_Data(x, l, m, L, d, r)\n",
    "\n",
    "\n",
    "def dump_data(save_filename, data):\n",
    "    save_file = open(save_filename, 'wb')\n",
    "    pickle.dump(data.x, save_file)\n",
    "    pickle.dump(data.l, save_file)\n",
    "    pickle.dump(data.m, save_file)\n",
    "    pickle.dump(data.L, save_file)\n",
    "    pickle.dump(data.d, save_file)\n",
    "    pickle.dump(data.r, save_file)\n",
    "    save_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f98e8201",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code for self-training with weak supervision.\n",
    "Author: Giannis Karamanolakis (gkaraman@cs.columbia.edu)\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import glob\n",
    "import joblib\n",
    "from copy import deepcopy\n",
    "import shutil\n",
    "\n",
    "# PreprocessedDataset is used for loading exactly the same dataset splits and features as in our experiments\n",
    "\n",
    "class PreprocessedDataset:\n",
    "    # Load pre-processed dataset as used in https://github.com/awasthiabhijeet/Learning-From-Rules\n",
    "    def __init__(self, datapath=\"../../data\", orig_train=True, dataset='trec', seed=42):\n",
    "        self.dataset = dataset\n",
    "        self.seed = seed\n",
    "        self.basedatafolder = os.path.join(datapath, self.dataset.upper())\n",
    "        self.datafolder = os.path.join(self.basedatafolder, 'seed{}/'.format(seed))\n",
    "        self.language = 'english'\n",
    "        self.orig_train = orig_train\n",
    "        self.label2ind = self.get_label2ind()\n",
    "        self.num_labels = len(self.label2ind)\n",
    "        # self.lf_names = [\"rule_{}\".format(i+1) for i in range(15)]\n",
    "        self.lf_names = None\n",
    "\n",
    "    def get_label2ind(self):\n",
    "        if self.dataset == 'hr':\n",
    "            return {\"non-critical\": 0, \"critical\": 1}\n",
    "        else:\n",
    "            raise(BaseException('Pre-trained dataset not supported: {}'.format(self.dataset)))\n",
    "\n",
    "    def load_data(self, method):\n",
    "        if method == 'train' and not self.orig_train:\n",
    "            method = 'unlabeled'\n",
    "        print(self.datafolder)\n",
    "        print(os.path.join(self.datafolder, \"{}_labels.pkl\".format(method)))\n",
    "        texts = joblib.load(os.path.join(self.datafolder, \"{}_x.pkl\".format(method)))\n",
    "        texts = texts.tolist()\n",
    "\n",
    "        labels = joblib.load(os.path.join(self.datafolder, \"{}_labels.pkl\".format(method)))\n",
    "        labels = labels.squeeze().tolist()\n",
    "        rule_preds = joblib.load(os.path.join(self.datafolder, \"{}_rule_preds.pkl\".format(method)))\n",
    "        rule_preds[rule_preds == self.num_labels] = -1\n",
    "        rule_preds = rule_preds.tolist()\n",
    "\n",
    "        if method =='train':\n",
    "            exemplars = joblib.load(os.path.join(self.datafolder, 'train_exemplars.pkl'))\n",
    "            return {'texts': texts, 'labels': labels, 'weak_labels': rule_preds, 'exemplar_labels': exemplars}\n",
    "        else:\n",
    "            return {'texts': texts, 'labels': labels, 'weak_labels': rule_preds}\n",
    "\n",
    "    def preprocess(self):\n",
    "        seed = self.seed\n",
    "        self.train_valid_split()\n",
    "        datafolder = os.path.join(self.basedatafolder, 'original_data'.format(seed))\n",
    "        if not os.path.exists(datafolder):\n",
    "            raise(BaseException('You need to download the original dataset for pre-processing, otherwise use our splits'))\n",
    "        savefolder = os.path.join(self.basedatafolder, 'preprocessed_datasets/seed{}/preprocessed/'.format(seed))\n",
    "        self.preprocess_fn(datafolder=datafolder, savefolder=savefolder)\n",
    "\n",
    "    def preprocess_fn(self, datafolder, savefolder):\n",
    "        # Our function used for pre-processing the original datasets and saving them into different splits (for robustness)\n",
    "        # Original datasets can be found here: https://github.com/awasthiabhijeet/Learning-From-Rules\n",
    "        # You can alternatively download the pre-processed versions used in our experiments\n",
    "        # seed0 contains the original dataset. seed<i> (i=1..5) contains 5 random splits of the unlabeled/train/dev data\n",
    "        # All splits consider the same test set, for a fair comparison to previous approaches.\n",
    "\n",
    "        os.makedirs(savefolder, exist_ok=True)\n",
    "\n",
    "        print(\"\\nunlabeled\")\n",
    "        print(datafolder)\n",
    "        print(os.path.join(datafolder, 'U_processed.p'))\n",
    "        data = load_data(os.path.join(datafolder, 'U_processed.p'))\n",
    "\n",
    "        print('x: {}'.format(data.x.shape))\n",
    "        print('rule_preds: {}'.format(data.l.shape))\n",
    "        joblib.dump(data.x, os.path.join(savefolder, 'unlabeled_x.pkl'))\n",
    "        joblib.dump(data.L, os.path.join(savefolder, 'unlabeled_labels.pkl'))\n",
    "        joblib.dump(data.l, os.path.join(savefolder, 'unlabeled_rule_preds.pkl'))\n",
    "\n",
    "\n",
    "        print(\"\\ntrain\")\n",
    "        data = load_data(os.path.join(datafolder, 'd_processed.p'))\n",
    "        print('x: {}'.format(data.x.shape))\n",
    "        print('rule_preds: {}'.format(data.l.shape))\n",
    "        joblib.dump(data.x, os.path.join(savefolder, 'train_x.pkl'))\n",
    "        joblib.dump(data.L, os.path.join(savefolder, 'train_labels.pkl'))\n",
    "        joblib.dump(data.l, os.path.join(savefolder, 'train_rule_preds.pkl'))\n",
    "        joblib.dump(data.r, os.path.join(savefolder, 'train_exemplars.pkl'))\n",
    "\n",
    "        print(\"\\nvalidation\")\n",
    "        data = load_data(os.path.join(datafolder, 'validation_processed.p'))\n",
    "        print('x: {}'.format(data.x.shape))\n",
    "        print('rule_preds: {}'.format(data.l.shape))\n",
    "        joblib.dump(data.x, os.path.join(savefolder, 'dev_x.pkl'))\n",
    "        joblib.dump(data.L, os.path.join(savefolder, 'dev_labels.pkl'))\n",
    "        joblib.dump(data.l, os.path.join(savefolder, 'dev_rule_preds.pkl'))\n",
    "\n",
    "        print(\"\\ntest\")\n",
    "        data = load_data(os.path.join(datafolder, 'test_processed.p'))\n",
    "        print('x: {}'.format(data.x.shape))\n",
    "        print('rule_preds: {}'.format(data.l.shape))\n",
    "        joblib.dump(data.x, os.path.join(savefolder, 'test_x.pkl'))\n",
    "        joblib.dump(data.L, os.path.join(savefolder, 'test_labels.pkl'))\n",
    "        joblib.dump(data.l, os.path.join(savefolder, 'test_rule_preds.pkl'))\n",
    "        print('\\nsaved files at {}'.format(savefolder))\n",
    "\n",
    "    def train_valid_split(self):\n",
    "        seed = self.seed\n",
    "        np.random.seed(seed)\n",
    "        datafolder = self.basedatafolder\n",
    "        savefolder = os.path.join(self.basedatafolder, 'preprocessed_datasets/seed{}/p'.format(seed))\n",
    "        os.makedirs(savefolder, exist_ok=True)\n",
    "\n",
    "        print(\"\\nunlabeled\")\n",
    "        unlabeled = load_data(os.path.join(datafolder, 'U_processed.p'))\n",
    "        train = load_data(os.path.join(datafolder, 'd_processed.p'))\n",
    "        dev = load_data(os.path.join(datafolder, 'validation_processed.p'))\n",
    "        test = load_data(os.path.join(datafolder, 'test_processed.p'))\n",
    "\n",
    "        # concatenate unlabeled, train, and dev datasets\n",
    "        all = concatenate_data(unlabeled, train)\n",
    "        all = concatenate_data(all, dev)\n",
    "        all_ids = ['unlabeled'] * unlabeled.x.shape[0] + ['train'] * train.x.shape[0] +  ['dev'] * dev.x.shape[0]\n",
    "\n",
    "        # split datasets\n",
    "        df = pd.DataFrame()\n",
    "        df['index'] = np.arange(all.x.shape[0])\n",
    "        df['label'] = all.L\n",
    "        df['method'] = all_ids\n",
    "\n",
    "        train_new_df, df = train_test_split(df, train_size=train.x.shape[0], random_state=seed, shuffle=True, stratify=df['label'])\n",
    "        dev_new_df, unlabeled_new_df = train_test_split(df, train_size=dev.x.shape[0], random_state=seed, shuffle=True, stratify=df['label'])\n",
    "\n",
    "        train_new = keep_ind(all, train_new_df['index'].to_numpy())\n",
    "        dev_new = keep_ind(all, dev_new_df['index'].to_numpy())\n",
    "        unlabeled_new = keep_ind(all, unlabeled_new_df['index'].to_numpy())\n",
    "        unlabeled_new = discard_r(unlabeled_new)\n",
    "\n",
    "        assert train_new.x.shape == train.x.shape\n",
    "        assert dev_new.x.shape == dev.x.shape\n",
    "        assert unlabeled_new.x.shape == unlabeled.x.shape\n",
    "\n",
    "        print('Writing new pre-processed (.p) files to {}'.format(savefolder))\n",
    "        dump_data(os.path.join(savefolder, 'd_processed.p'), train_new)\n",
    "        dump_data(os.path.join(savefolder, 'validation_processed.p'), dev_new)\n",
    "        dump_data(os.path.join(savefolder, 'U_processed.p'), unlabeled_new)\n",
    "        dump_data(os.path.join(savefolder, 'test_processed.p'), test)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bf35dc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "unlabeled\n",
      "../data/astra/data_small/step1/\n",
      "../data/astra/data_small/step1/U_processed.p\n",
      "Loading from  ../data/astra/data_small/step1/U_processed.p\n",
      "7500 7500\n",
      "x: (7500, 512)\n",
      "rule_preds: (7500, 3)\n",
      "\n",
      "train\n",
      "Loading from  ../data/astra/data_small/step1/d_processed.p\n",
      "7500 7500\n",
      "x: (7500, 512)\n",
      "rule_preds: (7500, 3)\n",
      "\n",
      "validation\n",
      "Loading from  ../data/astra/data_small/step1/validation_processed.p\n",
      "3000 3000\n",
      "x: (3000, 512)\n",
      "rule_preds: (3000, 3)\n",
      "\n",
      "test\n",
      "Loading from  ../data/astra/data_small/step1/test_processed.p\n",
      "4500 4500\n",
      "x: (4500, 512)\n",
      "rule_preds: (4500, 3)\n",
      "\n",
      "saved files at ../data/astra/data_small/step2/seed42/\n",
      "\n",
      "unlabeled\n",
      "../data/astra/data_imbalanced/step1/\n",
      "../data/astra/data_imbalanced/step1/U_processed.p\n",
      "Loading from  ../data/astra/data_imbalanced/step1/U_processed.p\n",
      "10525 10525\n",
      "x: (10525, 512)\n",
      "rule_preds: (10525, 3)\n",
      "\n",
      "train\n",
      "Loading from  ../data/astra/data_imbalanced/step1/d_processed.p\n",
      "10525 10525\n",
      "x: (10525, 512)\n",
      "rule_preds: (10525, 3)\n",
      "\n",
      "validation\n",
      "Loading from  ../data/astra/data_imbalanced/step1/validation_processed.p\n",
      "3000 3000\n",
      "x: (3000, 512)\n",
      "rule_preds: (3000, 3)\n",
      "\n",
      "test\n",
      "Loading from  ../data/astra/data_imbalanced/step1/test_processed.p\n",
      "4500 4500\n",
      "x: (4500, 512)\n",
      "rule_preds: (4500, 3)\n",
      "\n",
      "saved files at ../data/astra/data_imbalanced/step2/seed42/\n",
      "\n",
      "unlabeled\n",
      "../data/astra/data_full/step1/\n",
      "../data/astra/data_full/step1/U_processed.p\n",
      "Loading from  ../data/astra/data_full/step1/U_processed.p\n",
      "15000 15000\n",
      "x: (15000, 512)\n",
      "rule_preds: (15000, 3)\n",
      "\n",
      "train\n",
      "Loading from  ../data/astra/data_full/step1/d_processed.p\n",
      "15000 15000\n",
      "x: (15000, 512)\n",
      "rule_preds: (15000, 3)\n",
      "\n",
      "validation\n",
      "Loading from  ../data/astra/data_full/step1/validation_processed.p\n",
      "3000 3000\n",
      "x: (3000, 512)\n",
      "rule_preds: (3000, 3)\n",
      "\n",
      "test\n",
      "Loading from  ../data/astra/data_full/step1/test_processed.p\n",
      "4500 4500\n",
      "x: (4500, 512)\n",
      "rule_preds: (4500, 3)\n",
      "\n",
      "saved files at ../data/astra/data_full/step2/seed42/\n",
      "\n",
      "unlabeled\n",
      "../data/astra/data_oos_plus/step1/\n",
      "../data/astra/data_oos_plus/step1/U_processed.p\n",
      "Loading from  ../data/astra/data_oos_plus/step1/U_processed.p\n",
      "15000 15000\n",
      "x: (15000, 512)\n",
      "rule_preds: (15000, 3)\n",
      "\n",
      "train\n",
      "Loading from  ../data/astra/data_oos_plus/step1/d_processed.p\n",
      "15000 15000\n",
      "x: (15000, 512)\n",
      "rule_preds: (15000, 3)\n",
      "\n",
      "validation\n",
      "Loading from  ../data/astra/data_oos_plus/step1/validation_processed.p\n",
      "3000 3000\n",
      "x: (3000, 512)\n",
      "rule_preds: (3000, 3)\n",
      "\n",
      "test\n",
      "Loading from  ../data/astra/data_oos_plus/step1/test_processed.p\n",
      "4500 4500\n",
      "x: (4500, 512)\n",
      "rule_preds: (4500, 3)\n",
      "\n",
      "saved files at ../data/astra/data_oos_plus/step2/seed42/\n"
     ]
    }
   ],
   "source": [
    "DATA_FOLDER = \"../data/clinic/\"\n",
    "TARGET_BASE = \"../data/astra/\"\n",
    "sub_folders = [\"data_small/\", 'data_imbalanced/', 'data_full/', 'data_oos_plus/']\n",
    "for sub_folder in sub_folders:\n",
    "    '''df_train = pd.read_csv(DATA_FOLDER + sub_folder + 'train_with_use_emb_with_weak_label.csv')\n",
    "    df_valid = pd.read_csv(DATA_FOLDER + sub_folder + 'val_with_use_emb_with_weak_label.csv')\n",
    "    df_test = pd.read_csv(DATA_FOLDER + sub_folder + 'test_with_use_emb_with_weak_label.csv')\n",
    "    TARGET_PATH = TARGET_BASE + sub_folder + \"step1/\"\n",
    "    print(df_train.head())\n",
    "    obj = Generate_data(df_train,df_valid,df_test)\n",
    "    obj.generate_pickles()'''\n",
    "    TARGET_PATH = TARGET_BASE + sub_folder + \"step1/\"\n",
    "    TARGET_PATH2 = TARGET_BASE + sub_folder + \"step2/seed42/\"\n",
    "    pdataset = PreprocessedDataset(datapath = TARGET_PATH, dataset='hr',seed=42)\n",
    "    pdataset.preprocess_fn(TARGET_PATH, TARGET_PATH2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e371b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f376ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f87a16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be4c887",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e0e320",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3753ee7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fb5671",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212f17d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bb901a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6c2329",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../data/astra/step0/\"\n",
    "TARGET_PATH = \"../data/astra/step1/\"\n",
    "\n",
    "if not os.path.exists(TARGET_PATH):os.makedirs(TARGET_PATH);\n",
    "    \n",
    "train = pd.read_csv(DATA_PATH+ \"train.csv\")\n",
    "dt_rules = pd.read_csv(DATA_PATH +\"rules.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ca0308",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = Generate_data()\n",
    "obj.generate_pickles()\n",
    "\n",
    "TARGET_PATH2 = \"/Users/tayfun.tuna/workspace/ASTRA/data/HR/LF_Review/labeled\" +str(train_size) + \"_valid\"+str(valid_size)+\"/step2/seed42/\"\n",
    "\n",
    "pdataset = PreprocessedDataset(datapath = TARGET_PATH, dataset='hr',seed=42)\n",
    "pdataset.preprocess_fn(TARGET_PATH, TARGET_PATH2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dd9047b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity as cos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "12145f7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99146013]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1 = [[1,2,3]]\n",
    "list2  = [[1,2,4]]\n",
    "cos(list1,list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f7579e4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99146013, 1.        ]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1 = [[1,2,3]]\n",
    "list2  = [[1,2,4],[1,2,3]]\n",
    "cos(list1,list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de4db95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
